{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx1DwXpPEWeL"
      },
      "source": [
        "Copyright 2024 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SytPbrqtkUF"
      },
      "source": [
        "![onetwo_logo_horizontal.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ7N9Pmt9kvm"
      },
      "source": [
        "This colab illustrates how to use the [OneTwo](https://github.com/google-deepmind/onetwo) library.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNneQ1X8c_yb"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF_Z92hr2fhG"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWpY81IcArpB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A6jm6BDe8g_"
      },
      "outputs": [],
      "source": [
        "# Uncomment the following line and execute the cell to install OneTwo (if not already installed).\n",
        "# !pip install git+https://github.com/google-deepmind/onetwo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAJGlUJLNwT0"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGi9bnZRNzg5"
      },
      "outputs": [],
      "source": [
        "# At minimum, these are the libraries you will need to import to perform basic\n",
        "# operations with OneTwo.\n",
        "from onetwo import ot\n",
        "from onetwo.builtins import llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsvl1UcXECJb"
      },
      "outputs": [],
      "source": [
        "# These are some additional libraries that we will use in certain sections of\n",
        "# the colab.\n",
        "from collections.abc import Mapping, Sequence\n",
        "import copy\n",
        "import dataclasses\n",
        "import datetime\n",
        "import functools\n",
        "import IPython\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import re\n",
        "import textwrap\n",
        "import typing\n",
        "from typing import Any, Generic, TypeVar\n",
        "\n",
        "from google.colab import data_table\n",
        "\n",
        "from onetwo.agents import python_planning\n",
        "from onetwo.agents import react\n",
        "from onetwo.builtins import composables\n",
        "from onetwo.core import results\n",
        "from onetwo.core import sampling\n",
        "from onetwo.core import tracing\n",
        "from onetwo.core import utils\n",
        "from onetwo.evaluation import agent_evaluation\n",
        "from onetwo.stdlib.code_execution import python_execution_safe_subset\n",
        "from onetwo.stdlib.ensembling import distribution_metrics\n",
        "from onetwo.stdlib.ensembling import self_consistency\n",
        "from onetwo.stdlib.reasoning import chain_of_thought\n",
        "from onetwo.stdlib.tool_use import llm_tool_use\n",
        "from onetwo.stdlib.tool_use import python_tool_use\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvjw75nbdjZz"
      },
      "outputs": [],
      "source": [
        "# Display DataFrames by default as interactive tables.\n",
        "data_table.enable_dataframe_formatter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv6vzl2rZciy"
      },
      "source": [
        "# Overview\n",
        "\n",
        "One of the key principles behind the OneTwo library is to enable the creation of complex flows involving several calls to foundation models and possibly other tools.\n",
        "\n",
        "We facilitate through functionality at two levels:\n",
        "* **OneTwo Core:** Low-level libraries for connecting to models, prompting them, and constructing programs that chain multiple calls to such models, while leaving you full flexibility in the structuring of your program (just like if you were programming in pure Python).\n",
        "* **OneTwo Standard Library:** A set of reusable components built on top of the OneTwo Core, providing generic implementations of a wide range of SOTA prompting strategies from Google DeepMind, Google Research, and published research from outside of Google.\n",
        "\n",
        "In this overview, we provide a quick introduction to both the OneTwo core and the OneTwo standard library, with the intention that with the overview alone, you should already be up and running with OneTwo in less than an hour. Feel free to then continue on with the advanced sections, or come back to them later!\n",
        "\n",
        "**OneTwo Core**\n",
        "\n",
        "For ease of experimentation, it is important to easily change the backends or their configuration and run the same flow on two backends/configurations, e.g. when doing comparisons.\n",
        "\n",
        "The bottleneck is often the multiple RPC requests that need to happen. This makes fast iterations or experimenting on many examples slow and tedious. In order to reduce this bottleneck, there are two strategies that are implemented in the OneTwo library:\n",
        "1. **Caching**: The result of the calls to the models are cached, which enables one to very quickly replay a flow or an experiment which may have partially executed (e.g. failed in the middle of execution). For example, if you have a complex flow and want to add just one extra step, rerunning the whole thing amounts to reading everything from cache and only executing for real that one last step.\n",
        "1. **Asynchronous Execution**: While some of the model calls might need to be chained serially, there are many situations when you may want to execute some calls in parallel (e.g. talking to different backends, running an experiment on many examples, or having a step in your flow where several independent tasks are performed). A natural way to do that is to use asynchronous programming, or multi-threading.\n",
        "\n",
        "In the first several sub-sections of the overview, we provide an introduction that illustrates how to connect to and prompt a model using the basic OneTwo functionality.\n",
        "\n",
        "**OneTwo Standard Library:**\n",
        "\n",
        "In the later sub-sections of the overview, we introduce some of the most widely used components from the OneTwo standard library, including bread-and-butter techniques such as chain-of-thought and self-consistency, as well as more complex agent and tool use strategies.\n",
        "\n",
        "More details on the OneTwo standard library and other advanced features are provided in additional sections after the overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6soYEHIjZci_"
      },
      "source": [
        "## Connecting to a Model\n",
        "\n",
        "The Gemini, and OpenAI APIs provide an easy way to connect to a model. Select your preferred API/model in the drop-down and enter the corresponding API key (where relevant) in the text box below.\n",
        "\n",
        "OneTwo also supports connecting to other models such as Gemma models running locally or on a server. See Appendix A for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0HS107hVx3-"
      },
      "source": [
        "### Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akiM6TkoZu5p"
      },
      "outputs": [],
      "source": [
        "# @title {run:\"auto\"}\n",
        "\n",
        "# Select one of the LLM backend options here\n",
        "model_selection = 'Gemini API' # @param [ 'Gemini API', 'OpenAI API']\n",
        "\n",
        "# Boilerplate for conditional cell execution\n",
        "@IPython.core.magic.register_cell_magic('run_if')\n",
        "def run_if(line, cell):\n",
        "  if eval(line):\n",
        "     IPython.get_ipython().run_cell(cell)\n",
        "  else:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQL3iCpBEU7v"
      },
      "source": [
        "### Caching Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHrn17vNxuzp"
      },
      "outputs": [],
      "source": [
        "# Here we define a location in which to store a cache of requests/replies for\n",
        "# each backend of interest, which we can use for speeding up running of the\n",
        "# colab if we re-run it (or make iterative modifications to it) in the future.\n",
        "# We will use a separate cache file for each backend.\n",
        "OWN_CACHE_DIRECTORY = '/tmp/onetwo_colab_backend_caches/tutorial'\n",
        "\n",
        "# If you would like to share cache files with others in your working group, you\n",
        "# can optionally specify another shared cache directory here. If you specify\n",
        "# this, then we will read from shared cache directory and give precedence to its\n",
        "# contents, while merging in any additional content from OWN_CACHE_DIRECTORY.\n",
        "# When saving the cache, however, we will by default write only to\n",
        "# OWN_CACHE_DIRECTORY to reduce the chance of people clobbering each other's\n",
        "# changes.\n",
        "SHARED_CACHE_DIRECTORY = None\n",
        "\n",
        "# If you want to automatically merge in content from any of your teammates'\n",
        "# cache directories or from a cache that was output by a batch eval run,\n",
        "# you can list the additional directories here.\n",
        "ADDITIONAL_CACHE_DIRECTORIES = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUKMriyEpX9n"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AN_zoMUrXs6"
      },
      "source": [
        "### Caching Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9qpS98lwlYw"
      },
      "source": [
        "Here we define some helper functions for loading/saving backend caches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLJ9Oc3DrZWM"
      },
      "outputs": [],
      "source": [
        "def get_model_name_for_cache(model_name: Any, temperature: float = 0.0) -\u003e str:\n",
        "  \"\"\"Returns a name to identify the model/temperature for caching.\"\"\"\n",
        "  if not isinstance(model_name, str):\n",
        "    model_name = str(model_name)\n",
        "  if match := re.search(r'\\.(.+)', model_name):\n",
        "    model_name = match[1]\n",
        "  if temperature \u003e 0:\n",
        "    temperature_str = str(temperature).replace('.', '_')\n",
        "    model_name = f'{model_name}_{temperature_str}'\n",
        "  return model_name\n",
        "\n",
        "def get_cache_path(model_name: Any, temperature: float = 0.0) -\u003e str:\n",
        "  \"\"\"Returns the path for caching a given model (may be an enum or a string).\"\"\"\n",
        "  model_name_for_cache = get_model_name_for_cache(model_name, temperature)\n",
        "  return os.path.join(OWN_CACHE_DIRECTORY, f'{model_name_for_cache}.json')\n",
        "\n",
        "def load_backend_cache(backend: Any, *, overwrite: bool = True):\n",
        "  \"\"\"Checks if the cache file(s) already exist, in which case we load them.\n",
        "\n",
        "  Args:\n",
        "    backend: The backend for which to load the cache.\n",
        "    overwrite: If True, then completely replaces the current in-memory cache\n",
        "      with the contents of the cache file(s). If False, then preserves the\n",
        "      current in-memory cache contents, while merging in any additional content\n",
        "      from the file(s).\n",
        "  \"\"\"\n",
        "  if not backend.cache_filename:\n",
        "    print('No cache filename specified for {backend.model_name}. Not loading.')\n",
        "    return\n",
        "  cache_file_basename = os.path.basename(backend.cache_filename)\n",
        "\n",
        "  cache_filenames = []\n",
        "  if SHARED_CACHE_DIRECTORY:\n",
        "    shared_cache_filename = os.path.join(\n",
        "        SHARED_CACHE_DIRECTORY, cache_file_basename)\n",
        "    cache_filenames.append(shared_cache_filename)\n",
        "  if backend.cache_filename and backend.cache_filename not in cache_filenames:\n",
        "    cache_filenames.append(backend.cache_filename)\n",
        "  if ADDITIONAL_CACHE_DIRECTORIES:\n",
        "    for cache_directory in ADDITIONAL_CACHE_DIRECTORIES:\n",
        "      cache_filename = os.path.join(cache_directory, cache_file_basename)\n",
        "      if cache_filename not in cache_filenames:\n",
        "        cache_filenames.append(cache_filename)\n",
        "\n",
        "  backend_name = cache_file_basename.split('.')[0]\n",
        "  if cache_filenames:\n",
        "    print(f'Loading {len(cache_filenames)} cache file(s) for {backend_name}.')\n",
        "  else:\n",
        "    print(f'No cache files specified for {backend_name}.')\n",
        "\n",
        "  for cache_filename in cache_filenames:\n",
        "    if os.path.exists(cache_filename):\n",
        "      print(f'Loading cache from {cache_filename} ({overwrite=}).')\n",
        "      cache_size_before = len(backend._cache_handler._cache_data.values_by_key)\n",
        "      backend.load_cache(overwrite=overwrite, cache_filename=cache_filename)\n",
        "      cache_size_after = len(backend._cache_handler._cache_data.values_by_key)\n",
        "      print(f'Loaded {cache_size_after - cache_size_before} items: '\n",
        "            f'{cache_size_before} =\u003e {cache_size_after}.')\n",
        "      overwrite = False\n",
        "    else:\n",
        "      print(f'Cache file does not exist: {cache_filename}')\n",
        "\n",
        "def load_backend_caches(backends: Sequence[Any]):\n",
        "  \"\"\"Loads the caches of all the given backends.\"\"\"\n",
        "  for backend in backends:\n",
        "    load_backend_cache(backend)\n",
        "\n",
        "def save_backend_caches(\n",
        "    backends: Sequence[Any],\n",
        "    *,\n",
        "    cache_directory: str | None = None):\n",
        "  \"\"\"Saves the caches of all the given backends.\n",
        "\n",
        "  Args:\n",
        "    backends: The backends for which to save the caches.\n",
        "    cache_directory: If specified, then will save the caches to this directory.\n",
        "      Otherwise, will save the caches to the location configured when the\n",
        "      backend was created.\n",
        "  \"\"\"\n",
        "  for backend in backends:\n",
        "    cache_filename = backend.cache_filename\n",
        "    if cache_directory:\n",
        "      cache_filename = os.path.join(\n",
        "          cache_directory, os.path.basename(cache_filename))\n",
        "    time_string = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f'{time_string}: Saving cache to {cache_filename}.')\n",
        "    backend.save_cache(cache_filename=cache_filename, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVQ-lzf5j34Y"
      },
      "outputs": [],
      "source": [
        "# We will keep track of all the backends that we construct in this colab (as a\n",
        "# mapping of name to backend instance), to make it easy to load/save the caches.\n",
        "# Having a mapping of backends like this can also be convenient when doing\n",
        "# things constructing an experiment run that does a sweep over multiple LLMs.\n",
        "backends = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-uL_yLBZf2t"
      },
      "source": [
        "### Gemini API\n",
        "\n",
        "OneTwo can connect to publicly-hosted Gemini models via the Gemini API. If you have not used the Gemini API before, you will need to first create an account and API key following the instructions on https://ai.google.dev/. Then either copy-paste your API key into the text box, or store it in the 'GOOGLE_API_KEY' environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbNLcbPvZci_"
      },
      "outputs": [],
      "source": [
        "%%run_if model_selection == 'Gemini API'  # Execute cell only for 'Gemini API'\n",
        "\n",
        "from onetwo.backends import gemini_api\n",
        "\n",
        "# You can specify your API key either here or as an environment variable.\n",
        "api_key = \"\"  # @param {type: 'string'}\n",
        "\n",
        "if not api_key and 'GOOGLE_API_KEY' not in os.environ:\n",
        "  raise ValueError(\n",
        "      'The api key must be specified either here or in the environment.')\n",
        "\n",
        "# Create and register a connection to the default Gemini backend (Gemini Pro).\n",
        "backend = gemini_api.GeminiAPI(\n",
        "    api_key=api_key,\n",
        "    temperature=0.0,\n",
        "    cache_filename=get_cache_path('GEMINI_PRO', temperature=0.0),\n",
        ")\n",
        "backends['GEMINI_PRO'] = backend\n",
        "backend.register()\n",
        "print('Gemini API backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrpCvVxJZi9k"
      },
      "source": [
        "### OpenAI API\n",
        "\n",
        "OneTwo can connect to OpenAI models via the OpenAI API. If you have not used the OpenAI API before, you will need to first create an account and API key at https://platform.openai.com/signup. Then copy-paste your API key into the text box below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6o2jzmjHXIA"
      },
      "outputs": [],
      "source": [
        "%%run_if model_selection == 'OpenAI API'  # Execute cell only for 'OpenAI API'\n",
        "\n",
        "from onetwo.backends import openai_api\n",
        "\n",
        "# You can specify your API key either here or as an environment variable.\n",
        "api_key = \"\"  # @param {type: 'string'}\n",
        "\n",
        "if not api_key and 'OPENAI_API_KEY' not in os.environ:\n",
        "  raise ValueError(\n",
        "      'The api key must be specified either here or in the environment.')\n",
        "\n",
        "model_name = 'gpt-3.5-turbo'  # Specify the model of your choice.\n",
        "\n",
        "# Create and register a connection to the OpenAI backend.\n",
        "backend = openai_api.OpenAIAPI(\n",
        "    api_key=api_key,\n",
        "    model_name=model_name,\n",
        "    temperature=0.0,\n",
        "    cache_filename=get_cache_path(model_name, temperature=0.0),\n",
        ")\n",
        "backends[model_name] = backend\n",
        "backend.register()\n",
        "print('OpenAI API backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNGj_MPN4emn"
      },
      "source": [
        "### Saving and Loading Caches\n",
        "\n",
        "When creating a backend, one can associate a cache file to it, which allows one to store and retrieve previously executed requests. Note that when we created the LLM backend connections above, we already specified a location for loading/saving the cache, using the parameter `cache_filename`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzkK93qG3zPb"
      },
      "source": [
        "**Loading cache:**\n",
        "\n",
        "The following initializes the cache if the file exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd6DOLFS3zPm"
      },
      "outputs": [],
      "source": [
        "# This is a simple helper function that we defined earlier in the colab.\n",
        "load_backend_caches(backends.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_m9azV_37lF"
      },
      "source": [
        "**Saving own cache:**\n",
        "\n",
        "As you perform operations on the backend in colab (including eval sweeps, etc.), the in-memory version of the cache will be continually updated. To write the in-memory caches back to file, you can do so at any time by temporarily uncommenting the following line and then manually running it.\n",
        "\n",
        "Since we included your username as part of the cache directory, you can save to your own cache directory at any time without fear of clobbering any one else's work. (If you have multiple instances of this colab open simultaneously, though, then you will still need to be careful to avoid clobbering yourself, as any time you save the cache from one instance of the colab, it will overwrite whatever was there before.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1fjZ9jf37lG"
      },
      "outputs": [],
      "source": [
        "# save_backend_caches(backends.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyyWN3yo4EyO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsofAL74Zci_"
      },
      "source": [
        "## LLM Built-ins and Executables\n",
        "\n",
        "OneTwo provides a number of built-in functions representing the basic operations one may want to perform on an LLM.\n",
        "\n",
        "- `llm.generate_text()` - Generate raw text.\n",
        "- `llm.instruct()` - Generate answer to instructions.\n",
        "- `llm.chat()` - Generate text in a multi-turn dialogue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nnzGJ8fT9AG"
      },
      "source": [
        "### `llm.generate_text()`\n",
        "\n",
        "The function `llm.generate_text()` asks the model to continue the given text. This works for both base models and instruction-tuned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VXQvK2YZci_"
      },
      "outputs": [],
      "source": [
        "# Ask the model to continue the given text.\n",
        "e = llm.generate_text(\n",
        "    'Three not so well known cities in France are',\n",
        "    max_tokens=20,\n",
        ")\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tleOIzMZci_"
      },
      "source": [
        "Note that the LLM built-ins do not directly issue a request to the model. Instead, they return an `Executable` (the variable `e` in the above example), which can then be executed to produce the final result. The benefit of this two-step process is that one can define possibly complex execution flows in a natural pythonic way, and decouple the definition of the flow from the actual backends that are used to execute it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwYEKWRUhEJ5"
      },
      "source": [
        "### `llm.chat()`\n",
        "\n",
        "The function `llm.chat()` asks the model to continue a sequence of messages that are a back-and-forth between a user and the model. As illustrated in the example below, each `Message` consists of a role and some content. The exact behavior of `llm.chat()` is model-specific. By default, OneTwo creates a prompt by merging the messages and adding model-specific control tokens and role indicators. The prompt is then sent to `llm.generate.text()`. However, if the model supports `chat` API natively (e.g., like most of the OpenAI and Gemini models), `llm.chat` uses it directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSV0tkXwv39u"
      },
      "outputs": [],
      "source": [
        "from onetwo.core import content\n",
        "\n",
        "messages = [\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.USER,\n",
        "        content=(\n",
        "            'Pretend that you are Albert Einstein in 1911.\\n'\n",
        "            'Hi, my name is Peter. Who are you?'\n",
        "        ),\n",
        "    ),\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.MODEL,\n",
        "        content='Nice to meet you, Peter. My name is Albert.',\n",
        "    ),\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.USER,\n",
        "        content='Tell me more about yourself. Do you have a family and what is your job like?',\n",
        "    ),\n",
        "]\n",
        "e = llm.chat(messages, max_tokens=20)\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9g9iZp1iLjS"
      },
      "source": [
        "### `llm.instruct()`\n",
        "\n",
        "The function `llm.instruct()` asks the model to follow a certain instruction. This is different from `llm.generate_text()` in that the request is formatted such that it is clear that we want the instruction to be followed. The exact behavior of `llm.instruct()` is model-specific. By default OneTwo converts it into an `llm.chat()` request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uACg3J1ZUJsu"
      },
      "outputs": [],
      "source": [
        "# Issue a generate_text() request.\n",
        "e = llm.instruct('Write a 4 line poem about the Swiss Alps.', max_tokens=30)\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_iAv6afOH3E"
      },
      "source": [
        "### Printing actual formatted prompts sent to the model\n",
        "\n",
        "Often when using builtins like `llm.instruct` and `llm.chat` and debugging your code it may be important to print the arguments used in actual calls to the underlying model API (e.g., final formatted prompts or lists of messages). One simple way to do it is to swap (mock) the `llm.generate_text` (which often happens to be the very last step for many of the builtins) with a fake implementation that returns its input.\n",
        "\n",
        "Below we connect to the `gpt-3.5-turbo-instruct` OpenAI model. This model does not natively support the `chat` API. When we register this backend, `llm.chat` gets configured with the default implementation that formats the prompt and calls `llm.generate_text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33xFxpD1OOgc"
      },
      "outputs": [],
      "source": [
        "%%run_if model_selection == 'OpenAI API'  # Execute cell only for 'OpenAI API'.\n",
        "\n",
        "new_backend = openai_api.OpenAIAPI(\n",
        "    api_key=api_key,\n",
        "    model_name='gpt-3.5-turbo-instruct',  # Does not support `chat` natively.\n",
        "    temperature=0.0,\n",
        ")\n",
        "new_backend.register()\n",
        "\n",
        "messages = [\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.USER,\n",
        "        content=(\n",
        "            'Hello! Who is your favourite writer?'\n",
        "        ),\n",
        "    ),\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.MODEL,\n",
        "        content='I guess one of the American novelists from the 1940s, e.g.',\n",
        "    ),\n",
        "]\n",
        "e = llm.chat(messages, max_tokens=20)\n",
        "\n",
        "# `llm.chat` formats the prompt and calls `llm.generate_text`.\n",
        "print(\n",
        "    f'llm.chat returned:\\n--\u003e|{ot.run(e)}|\u003c--',\n",
        "    end='\\n\\n',\n",
        ")\n",
        "\n",
        "def fake_generate_text(prompt: str | content.ChunkList, **kwargs):\n",
        "    return prompt\n",
        "\n",
        "llm.generate_text.configure(fake_generate_text)\n",
        "\n",
        "# Now `llm.generate_text` simply returns its input.\n",
        "print(f'llm.generate_text was called with prompt:\\n--\u003e|{ot.run(e)}|\u003c--')\n",
        "\n",
        "# Reset the builtins to their original state.\n",
        "backend.register()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWYnC670Zci_"
      },
      "source": [
        "## Composing Executables\n",
        "\n",
        "If we want to chain two successive calls, we can perform one after the other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuef_kwvZci_"
      },
      "outputs": [],
      "source": [
        "result1 = ot.run(\n",
        "    llm.generate_text(\n",
        "        'Q: What is the southernmost city in France? A:', max_tokens=20\n",
        "    )\n",
        ")\n",
        "result2 = ot.run(\n",
        "    llm.generate_text(f'Q: Who is the mayor of {result1}? A:', max_tokens=20)\n",
        ")\n",
        "print(result1)\n",
        "print(result2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_kWQvNhZci_"
      },
      "source": [
        "But a better way is to create a new Executable that performs the all the desired operations and can then be executed on arbitrary backends. We do this by writing a function with the decorator `@ot.make_executable`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptzNx8XmZcjA"
      },
      "outputs": [],
      "source": [
        "@ot.make_executable\n",
        "async def f() -\u003e str:\n",
        "  result1 = await llm.generate_text(\n",
        "      'Q: What is the southernmost city in France? A:',\n",
        "      max_tokens=20,\n",
        "  )\n",
        "  print('Intermediate result:', result1)\n",
        "  result2 = await llm.generate_text(\n",
        "      f'Q: Who is the mayor of {result1}? A:',\n",
        "      max_tokens=20,\n",
        "  )\n",
        "  return result2\n",
        "\n",
        "result = ot.run(f())\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_sYw3yaZcjA"
      },
      "source": [
        "If we want to execute in parallel instead of executing serially, we can compose executables with `onetwo.parallel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P78BDUeZcjA"
      },
      "outputs": [],
      "source": [
        "e1 = llm.generate_text(\n",
        "    'Q: What is the southernmost city in France? A:', max_tokens=20\n",
        ")\n",
        "e2 = llm.generate_text(\n",
        "    'Q: What is the southernmost city in Spain? A:', max_tokens=20\n",
        ")\n",
        "e = ot.parallel(e1, e2)\n",
        "result_list = ot.run(e)\n",
        "print(result_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWSlVHY1ZcjA"
      },
      "source": [
        "## Templates and Composables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BKl_h2lZcjA"
      },
      "source": [
        "The advantage of the LLM built-ins is that arbitrarily complex execution flows can be defined directly in the Python language. At the same time, this approach may not always be the best way to visualize the textual structure of the prompt. Therefore, we provide alternative techniques for defining and composing prompts, which tend to be more visual (\"what you see is what you get\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVndXBBZcjA"
      },
      "source": [
        "One technique is to define and execute prompt templates using the Jinja2 syntax. We can create a Junja2 template using the built-in `composables.j()`. Note that this function again does not directly issue a call to a backend. Instead, it returns an executable that can be run on a certain model or can be composed with other executables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTP5uo-ZZcjA"
      },
      "outputs": [],
      "source": [
        "template = composables.j(\"\"\"\\\n",
        "What is the southernmost city in France? {{ generate_text(max_tokens=20) }}\n",
        "Who is its mayor? {{ generate_text(max_tokens=20) }}\n",
        "\"\"\")\n",
        "result = ot.run(template)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDu_vKvjZcjA"
      },
      "source": [
        "Another technique is to use `Composables`, which are variants of the LLM built-ins that can be concatenated into the prompt string using `+`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXjoSHb2ZcjA"
      },
      "outputs": [],
      "source": [
        "e = (\n",
        "    'What is the southernmost city in France? ' + composables.generate_text(max_tokens=20) +\n",
        "    '\\nWho is its mayor? ' + composables.generate_text(max_tokens=20)\n",
        ")\n",
        "result = ot.run(e)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JZM8UDFZcjA"
      },
      "source": [
        "## Prompt Variables\n",
        "\n",
        "Another useful technique is to use variables as part of the prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmEp1IdcZcjB"
      },
      "source": [
        "One way to use variables is via Python f-strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37CM70NjZcjB"
      },
      "outputs": [],
      "source": [
        "question = 'France'\n",
        "prompt1 = f'Q: What is the capital of {question}?\\nA:'\n",
        "res1 = ot.run(llm.generate_text(prompt1, max_tokens=10, stop=['Q:', '\\n\\n']))\n",
        "print(res1)\n",
        "prompt2 = f'Q: Who is the mayor of {res1}?\\nA:'\n",
        "res2 = ot.run(llm.generate_text(prompt2, max_tokens=10, stop=['Q:', '\\n\\n']))\n",
        "print(res2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLaedoVlZcjB"
      },
      "source": [
        "To make this code more reusable, we can compose the two executables into a single function that defines a new executable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf0kMwq9ZcjB"
      },
      "outputs": [],
      "source": [
        "@ot.make_executable\n",
        "async def capital_mayor(country: str) -\u003e str:\n",
        "  prompt = f'Q: What is the capital of {country}?\\nA:'\n",
        "  res = await llm.generate_text(prompt, max_tokens=10, stop=['Q:', '\\n\\n'])\n",
        "  prompt2 = f'Q: Who is the mayor of {res}?\\nA:'\n",
        "  return await llm.generate_text(prompt2, max_tokens=10, stop=['Q:', '\\n\\n'])\n",
        "\n",
        "print(ot.run(capital_mayor('France')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJJUTmLNZcjB"
      },
      "source": [
        "Another variant is to use variables in the Jinja 2 syntax. The following Jinja2 template is parameterized by the input variable `question`. In addition, we issue multiple requests and store their results in variables that can be referenced from within the prompt. The variables can also be retrieved from the resulting executable (`prompt`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgZava8qZcjB"
      },
      "outputs": [],
      "source": [
        "prompt = composables.j(\n",
        "    'Q: What is the capital of {{ question }}?\\n'\n",
        "    'A:{{ store(\"city\", generate_text(max_tokens=10, stop=[\"Q:\", \"\\n\\n\"])) }}\\n'\n",
        "    'Q: Who is the mayor of {{ __vars__.city }}?\\n'\n",
        "    'A:{{ store(\"mayor\", generate_text(max_tokens=10, stop=[\"Q:\", \"\\n\\n\"])) }}'\n",
        ")\n",
        "res = ot.run(prompt(question='France'))\n",
        "print(res)\n",
        "print(prompt['city'], prompt['mayor'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtpW_iuhZcjB"
      },
      "source": [
        "This above Jinja2 behavior can also be emulated using `composables`. We use `composables.f()` to format a string containing variables and we use `composables.store()` to store the results of LLM requests. As for Jinja2 variables, these variables can be retrieved from the resulting executable (`e`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ6ksMRLZcjB"
      },
      "outputs": [],
      "source": [
        "e = (\n",
        "    composables.f('Q: What is the capital of {question}?\\nA:') +\n",
        "    composables.store('city', composables.generate_text(max_tokens=10, stop=['Q:', '\\n\\n'])) +\n",
        "    composables.f('\\nQ: Who is the mayor of {city}?\\nA:') +\n",
        "    composables.store('mayor', composables.generate_text(max_tokens=10, stop=['Q:', '\\n\\n']))\n",
        ")\n",
        "res = ot.run(e(question='France'))\n",
        "print(res)\n",
        "print(e['city'], e['mayor'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-trZgnodt02P"
      },
      "source": [
        "## Sampling\n",
        "\n",
        "By default, the result for a given prompt execution is cached. This means that if you execute the same prompt more than once, a backend request is issued only the first time and the cached result is returned for all subsquent executions, even if temperature \u003e 0.\n",
        "\n",
        "While this is useful, there are cases where we actually want to obtain different samples. OneTwo provides the function `repeat()` to achieve this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEM8zNMZlq2b"
      },
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    'Here are three no-so-well-known cities in Switzerland:'\n",
        "    '1. Kilchberg, 2. Aesch, 3.'\n",
        ")\n",
        "\n",
        "# Make sure we set a non-zero temperature to avoid that the model returns\n",
        "# the same sample for each call.\n",
        "executable = llm.generate_text(prompt=prompt, max_tokens=20, temperature=0.5)\n",
        "\n",
        "# Create a list of containing 3 instances of the above executable.\n",
        "repeated_executable = ot.repeat(executable, 3)\n",
        "\n",
        "# Create an executable that executes the executables in the list in parallel.\n",
        "parallel_executable = ot.parallel(*repeated_executable)\n",
        "\n",
        "print(ot.run(parallel_executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nrba0fFzY8k"
      },
      "source": [
        "Note that `repeat()` takes as an argument the `executable` and returns a list (`repeated_executable`) containing 3 instances of it. We then use `parallel` to turn this list into the single executable `parallel_executable`, which executes all elements in parallel.\n",
        "\n",
        "Also note that we set temperature to 0.5 in `generate_text()` to avoid that the model returns the same sample for each call.\n",
        "\n",
        "If we now ask for 6 samples, the first 3 will be drawn from the cache while the next 3 will be generated by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4tkwZuT1q8s"
      },
      "outputs": [],
      "source": [
        "repeated_executable = ot.repeat(executable, 6)\n",
        "parallel_executable = ot.parallel(*repeated_executable)\n",
        "\n",
        "print(ot.run(parallel_executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgz2cId2MpO"
      },
      "source": [
        "If we want to get 2 more samples, we can do this by providing the `start_index` as an additional argument to `repeat()`. In the code below, we use `start_index=6`, which means that we do not want to retrieve the first 6 results of the executable from the cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sCq71bZ2q_Q"
      },
      "outputs": [],
      "source": [
        "repeated_executable = ot.repeat(executable, 2, start_index=6)\n",
        "parallel_executable = ot.parallel(*repeated_executable)\n",
        "\n",
        "print(ot.run(parallel_executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ct5-ISOS00L"
      },
      "source": [
        "## Switching Backends\n",
        "\n",
        "While up till now we just registered a single LLM backend globally so as to send all of the LLM requests to it, it is also possible to specify a backend to use for one specific code branch without affecting the choice of backend used elsewhere.\n",
        "\n",
        "We can do that by wrapping a code block in a `with ot.with_registry(...):` construct and then registering the LLM to use within just that block of code. The basic pattern looks like the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LduEUIPJTupK"
      },
      "outputs": [],
      "source": [
        "# other_backend = ...\n",
        "# with ot.RegistryContext():\n",
        "#   other_backend.register()\n",
        "#   llm.generate_text(prompt=prompt)  # Would use `other_backend`.\n",
        "\n",
        "# llm.generate_text(prompt=prompt)  # Would use the original backend again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUKIrRixWdrO"
      },
      "source": [
        "You can also use a similar pattern for swapping between different variants of the same LLM backend -- e.g., to apply a different default value for a parameter like `temperature`. We show how to do that below, using `ot.RegistryContext()` together with application of `update` to a built-in function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ueb7RTkXZkk"
      },
      "outputs": [],
      "source": [
        "prompt = 'Ten not so well-known cities in Switzerland are Kilchberg,'\n",
        "with ot.RegistryContext():\n",
        "  # The 'update' method can be used to set default values for arbitrary\n",
        "  # parameters of any built-in function.\n",
        "  llm.generate_text.update(temperature=0.5, max_tokens=5)\n",
        "\n",
        "  # From here on, any calls to `llm.generate_text` will default to the\n",
        "  # temperature and max_tokens values we set above.\n",
        "  print('Prompting LLM with temperature 0.5 and max_tokens 5:')\n",
        "  print(ot.run(llm.generate_text(prompt=prompt, stop=['\\n'])))\n",
        "\n",
        "# When we exit the `ot.RegistryContext()` block, the registered backends and\n",
        "# their default values all revert to the values they had previously.\n",
        "print('\\nPrompting LLM with the default temperature and max_tokens:')\n",
        "print(ot.run(llm.generate_text(prompt=prompt, stop=['\\n'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJaJLqJ7Tl6O"
      },
      "source": [
        "It is also possible to make multiple copies of the registry ahead of time and swap between them. We demonstrate this approach below in the `register_and_get_wrapper` convenience function, which causes the wrapped executable to be executed with the specified backend and with the specified default parameter values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOPI6caITUdr"
      },
      "outputs": [],
      "source": [
        "def register_and_get_wrapper(\n",
        "    backend: Any,\n",
        "    generate_text_kwargs: dict | None = None):\n",
        "  \"\"\"Returns a wrapper function to run an executable with the specified backend.\n",
        "\n",
        "  Args:\n",
        "    backend: The backend to use for the wrapped executable.\n",
        "    generate_text_kwargs: Default parameter values to use in calls to\n",
        "      `llm.generate_text` within the wrapped executable.\n",
        "  \"\"\"\n",
        "  with ot.RegistryContext():\n",
        "    backend.register()\n",
        "    if generate_text_kwargs:\n",
        "      llm.generate_text.update(**generate_text_kwargs)\n",
        "    registry = ot.copy_registry()\n",
        "  return lambda e: ot.with_registry(e, registry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5MwIhw1c8P_"
      },
      "outputs": [],
      "source": [
        "# Now we can use this to create a wrapper function for running an executable\n",
        "# with our customized `temperature` and `max_tokens` values.\n",
        "on_backend_temp_0_5_max_tokens_5 = register_and_get_wrapper(\n",
        "    backend, generate_text_kwargs={'temperature': 0.5, 'max_tokens': 5})\n",
        "\n",
        "# Suppose this is the executable that we want to be able to run with the\n",
        "# different backends or parameter defaults.\n",
        "executable = llm.generate_text(prompt=prompt, stop=['\\n'])\n",
        "\n",
        "# Now we can call that same executable as many times as we want, swapping\n",
        "# between backends or parameter defaults using the wrapper function.\n",
        "print('Prompting LLM with temperature 0.5 and max_tokens 5:')\n",
        "print(ot.run(on_backend_temp_0_5_max_tokens_5(executable)))\n",
        "\n",
        "print('\\nPrompting LLM with the default temperature and max_tokens:')\n",
        "print(ot.run(executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0cpbigTZcjC"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "As you tweak your prompt or prompting strategy, it is important to use an evaluation process based on hard numbers and not just anecdotal evidence.\n",
        "OneTwo comes with several example evaluation functions to facilitate this process, which you can either reuse as-is or clone to create your own custom script. We present the most basic and general-purpose script here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQum4Gmkq-Kg"
      },
      "source": [
        "### Programmatic Comparison\n",
        "\n",
        "The `ot.evaluate` function can be used to evaluate a prompting strategy by running it over a dataset and then automatically comparing the answers to the golden data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxYTdfbm2GwJ"
      },
      "outputs": [],
      "source": [
        "# Golden dataset to evaluate our prompting strategy.\n",
        "dataset = [\n",
        "    {'question': 'There are 100 people in a room. 55 are women and 70 are married. If 30 of the women are married, how many unmarried men are there?', 'answer': '5'},\n",
        "    {'question': 'A farmer has 12 sheep and 6 cows. How many more sheep than cows does he have?', 'answer': '6'},\n",
        "    {'question': 'A train travels 240 miles in 3 hours. What is its average speed in miles per hour?', 'answer': '80'},\n",
        "    {'question': 'A rectangular garden is 12 meters long and 8 meters wide. What is its perimeter?', 'answer': '40'},\n",
        "    {'question': 'If x + y = 10 and x - y = 2, what is the value of x?', 'answer': '6'},\n",
        "    {'question': 'A store sells apples for $0.50 each and oranges for $0.75 each. If I buy 5 apples and 3 oranges, how much will I spend?', 'answer': '4.75'},\n",
        "    {'question': 'A circle has a radius of 5 cm. What is its circumference in centimeters?', 'answer': '10*pi'},\n",
        "    {'question': 'A cube has a volume of 27 cubic feet. What is the length of one side of the cube in feet?', 'answer': '3'},\n",
        "    {'question': 'If 2^x = 16, what is the value of x?', 'answer': '4'},\n",
        "    {'question': 'What is the sum of the first 10 positive odd numbers?', 'answer': '100'}\n",
        "]\n",
        "\n",
        "# Define a simple strategy that passes the question directly to the model.\n",
        "@ot.make_executable\n",
        "async def strategy(question, **_):\n",
        "  answer = await llm.generate_text(\n",
        "      prompt=f'Question: {question}\\nFinal answer (formula or number):',\n",
        "      stop=['Question:', '\\n'],\n",
        "  )\n",
        "  return answer.strip()\n",
        "\n",
        "# Define a simple metric function that checks whether the correct answer is part\n",
        "# of the model output.\n",
        "def metric_fn(answer, example):\n",
        "  correct = str(example['answer']) in answer\n",
        "  extra_info = {}\n",
        "  if not correct:\n",
        "    index = hash(example['question'])\n",
        "    extra_info = {index: {\n",
        "        'question': example['question'][0:30] + '...',\n",
        "        'golden': example['answer'],\n",
        "        'answer': answer,\n",
        "    }}\n",
        "  return float(correct), extra_info\n",
        "\n",
        "# We run the evaluation on the dataset.\n",
        "time_elapsed, avg_metric, aggr_info = ot.evaluate(\n",
        "    strategy=strategy,\n",
        "    examples=dataset,\n",
        "    critic=metric_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiL6Z_CrNHxk"
      },
      "outputs": [],
      "source": [
        "# We can look at the cases where the model got a wrong answer.\n",
        "for v in aggr_info.values():\n",
        "  print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hYBlp1VIAJ7"
      },
      "source": [
        "### Using an LLM Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XEF_qrEeEP0"
      },
      "source": [
        "In some cases, programmatic comparison to the Golden is difficult. For instance, consider the example above where the golden answer is `10*pi` while the model may provide an answer such as `31.4 cm`.\n",
        "\n",
        "The same holds for the dataset below, where the answer to the questions is not a fixed number or string. In such cases, the evaluation script allows one to use an LLM to judge whether or not the provided answer is equivalent to the golden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4A12d1OMz_"
      },
      "outputs": [],
      "source": [
        "# We create another dataset of questions that do not necessarily have a fixed\n",
        "# definite answer.\n",
        "dataset = [\n",
        "    {\n",
        "        'question': 'Who developed the TCP/IP protocol?',\n",
        "        'golden_answer': 'Bob Kahn and Vint Cerf',\n",
        "    },\n",
        "    {\n",
        "        'question': 'What date was the declaration of independence signed (not written)?',\n",
        "        'golden_answer': '8/2/1776'\n",
        "    },\n",
        "    {\n",
        "        'question': 'How big is the area of a triangle with base a and height h',\n",
        "        'golden_answer': '1/2 * ah',\n",
        "    },\n",
        "    {\n",
        "        'question': 'Which countried border Guatemala?',\n",
        "        'golden_answer': 'Mexico, Belize, Honduras, El Salvador',\n",
        "    },\n",
        "    {\n",
        "        'question': 'How do vaccines work?',\n",
        "        'golden_answer': (\n",
        "            'Vaccines contain weakened or inactive pathogens that stimulate the'\n",
        "            ' immune system to produce antibodies, which then protect against'\n",
        "            ' future infections from the same pathogen.'\n",
        "        ),\n",
        "    },\n",
        "]\n",
        "\n",
        "# Define a simple strategy that passes the question directly to the model.\n",
        "@ot.make_executable\n",
        "async def strategy(question, **_):\n",
        "  answer = await llm.generate_text(\n",
        "      prompt=f'Question: {question} ?\\nAnswer (concise):',\n",
        "      stop=['Question:'],\n",
        "      max_tokens=500,\n",
        "  )\n",
        "  return answer.strip()\n",
        "\n",
        "  # We run the evaluation on the dataset using an LLM critic.\n",
        "time_elapsed, total_votes, aggr_info = ot.evaluate(\n",
        "    strategy=strategy,\n",
        "    critic=ot.naive_evaluation_critic,\n",
        "    examples=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nESW4fQ0hnhu"
      },
      "outputs": [],
      "source": [
        "# This prints the critic prompt that was used.\n",
        "print(list(aggr_info.values())[0]['critic_prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBN4nc8eIfMt"
      },
      "outputs": [],
      "source": [
        "# This prints some debug information about the critic's judgments.\n",
        "for k, v in aggr_info.items():\n",
        "  print('----------')\n",
        "  print(f'Question: {k}')\n",
        "  print(f'   - Golden: {v[\"golden_answer\"]}')\n",
        "  print(f'   - Answer: {v[\"candidate_answer\"]}')\n",
        "  print(f'   - Correct: {v[\"answer_is_correct\"]}')\n",
        "  print(f'   - Reason: {v[\"reason\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_d8cxyA4Ovn"
      },
      "source": [
        "## Chain-of-Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVv17afNUIFZ"
      },
      "source": [
        "Although in the QA examples up till now, we mostly prompted the model to directly output an answer, for problems that could benefit from reasoning or from some sort of multi-step solution, one can often achieve best results by using \"chain-of-thought prompting\" [[Wei, et al., 2023]](https://arxiv.org/pdf/2201.11903).\n",
        "\n",
        "In its most basic form, chain-of-thought is simply a style of how to\n",
        "write a prompt, where one prompts the LLM to output a reasoning chain followed\n",
        "by a final answer, rather than just the final answer, with the improved accuracy coming from the fact that the individual steps are easier to do \"off the top of one's head\" than the full problem, and the model is able to attend to its solutions to the intermediate steps when decoding the final answer.\n",
        "\n",
        "Chain-of-thought is easy to incorporate into your prompting strategies and, in its simplest form, does not require any dedicated components beyond what have already been introduced above. For example, as shown in [Kojima, et al., 2022](https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf), chain-of-thought reasoning can be elicited in many models in a zero-shot manner, by simply prefixing the answer with a phrase like \"Let's think step by step.\".\n",
        "\n",
        "As you can see below, implementing a zero-shot chain-of-thought strategy is just as easy as implementing a zero-shot direct answer strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjLRi-2-wNfl"
      },
      "outputs": [],
      "source": [
        "# Simple direct-answer strategy (similar to the one illustrated earlier).\n",
        "@ot.make_executable\n",
        "async def direct_answer_strategy(question, **_):\n",
        "  answer = await llm.generate_text(\n",
        "      prompt=f\"Q: {question}\\nA:\",\n",
        "      stop=['Q:', '\\n'],\n",
        "  )\n",
        "  return answer.strip()\n",
        "\n",
        "# Equally simple zero-shot chain-of-thought strategy.\n",
        "@ot.make_executable\n",
        "async def zero_shot_cot_strategy(question, **_):\n",
        "  answer = await llm.generate_text(\n",
        "      prompt=f\"Q: {question}\\nA: Let's think step by step.\",\n",
        "      stop=['Q:', '\\n\\n'],\n",
        "  )\n",
        "  return answer.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID_9YkF_8CFK"
      },
      "source": [
        "Now let's try evaluating each of these strategies on one of the questions from our earlier dataset that many models get wrong when prompted for the answer directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXN0kgAL5KhK"
      },
      "outputs": [],
      "source": [
        "# The correct answer is '5'.\n",
        "question = 'There are 100 people in a room. 55 are women and 70 are married. If 30 of the women are married, how many unmarried men are there?'\n",
        "direct_answer = ot.run(direct_answer_strategy(question))\n",
        "print(direct_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qezoj9XCwizS"
      },
      "outputs": [],
      "source": [
        "cot_answer = ot.run(zero_shot_cot_strategy(question))\n",
        "print(cot_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ-putxb8TTL"
      },
      "source": [
        "As you can see, the chain-of-thought answer now contains a detailed reasoning chain leading up to a final answer.\n",
        "\n",
        "When using chain-of-thought in practice, however, a number of additional pieces typically come into play, in particular:\n",
        "* **Answer extraction:** Even if the long-form chain-of-thought response contains the correct final answer, to make use of the answer programmatically, you will typically need to extract the final answer as a stand-alone string, for comparison to the golden answer or for consumption by the caller.\n",
        "* **Few-shot prompting:** Providing few-shot exemplars illustrating a particular style of chain-of-thought output can further improve accuracy on many tasks and/or make it easier to parse the final answer from the LLM reply.\n",
        "\n",
        "OneTwo's `chain_of_thought` library provides a number of off-the-shelf components that illustrate typical approaches to solving the above problems, which you can either re-use as-is or imitate in your own solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEtGrheAMsKz"
      },
      "source": [
        "Specifically, one approach to extracting the final answer is to prompt the LLM in two steps, first for the reasoning, and then for the final answer. This approach is illustrated in `QACoTPromptJ2`, which encapsulates the two-step prompt in a Jinja2 prompt template. The result is returned as a `CoTReply` data structure, which contains separate fields for the `reasoning` and the `answer`.\n",
        "\n",
        "This approach of using a follow-up prompt to elicit a final answer is essentially equivalent to the one described in [Zhou, et al., 2023](https://arxiv.org/pdf/2205.10625), who used it when applying a more structured chain-of-thought variant called \"least-to-most prompting\" to math reasoning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7sbJhxOy9Oa"
      },
      "outputs": [],
      "source": [
        "two_step_cot_strategy = chain_of_thought.QACoTPromptJ2()\n",
        "cot_reply, trace_cot = ot.run(two_step_cot_strategy(question), enable_tracing=True)\n",
        "print(f'Reasoning: {cot_reply.reasoning}')\n",
        "print(f'Answer: {cot_reply.answer}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1VMxeIgNZXp"
      },
      "source": [
        "You can see the exact prompts that were sent to the LLM by inspecting the trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIFkIAyS6z2k"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace_cot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70HPQeHyNf-_"
      },
      "source": [
        "The other approach to answer extraction is to apply few-shot prompting to guide the LLM to output an answer in a specific format (e.g., by separating the reasoning chain from the final answer with some fixed phrase like \"The answer is\") and then programmatically parse the LLM reply to separate the reasoning from the answer. This approach is illustrated in `QACoTPromptWithAnswerParserJ2`, which makes just a single call to the LLM, followed by a call to an answer parser. The result is returned as a `CoTReply` data structure, the same as above.\n",
        "\n",
        "The below example emulates the same prompt and roughly the same parsing algorithm used in [Wei, et al., 2023](https://arxiv.org/pdf/2201.11903) in their original evaluation of chain-of-thought on math reasoning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3ML97kpG_Sa"
      },
      "outputs": [],
      "source": [
        "few_shot_cot_strategy = chain_of_thought.QACoTPromptWithAnswerParserJ2(\n",
        "    exemplars=chain_of_thought.QA_COT_EXEMPLARS_ORIGINAL_MATH_WORD_PROBLEMS\n",
        ")\n",
        "cot_reply, trace_cot = ot.run(few_shot_cot_strategy(question), enable_tracing=True)\n",
        "print(f'Reasoning: {cot_reply.reasoning}')\n",
        "print(f'Answer: {cot_reply.answer}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1cTmRbmRnf0"
      },
      "source": [
        "You can again see the expanded prompt and raw LLM reply by inspecting the trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXpFdceVB6JY"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace_cot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB4Edjwv94A3"
      },
      "source": [
        "## Self-Consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75xgMIrx94A3"
      },
      "source": [
        "One way of thinking about chain-of-thought is as a general-purpose strategy for trading higher computational cost (in the form of a longer decoded output) for higher accuracy on a complex, multi-step task.\n",
        "\n",
        "Another widely applicable strategy that trades higher computational cost for higher accuracy on complex tasks is that of \"self-consistency\" [[Wang, et al., 2023]](https://arxiv.org/pdf/2203.11171). Rather than higher computational cost through computation of longer sequences, however, self-consistency invests additional computational cost in the form of parallel processing.\n",
        "\n",
        "Specifically, the basic idea of self-consistency is to generate multiple samples using chain-of-thought or some other strategy involving diverse reasoning paths; extract a final answer from each sample; and then do majority voting over the answers. This has the effect of estimating a probability distribution over final answers, while marginalizing over the many possible reasoning paths that could potentially lead to the same answer. Empirically, this has been found to lead to double-digit improvements in accuracy on many tasks.\n",
        "\n",
        "Using techniques like those illustrated in the \"Sampling\" section above, you already have all the building blocks you need to implement your own versions of self-consistency, by combining things like `ot.repeat` and `ot.parallel` with some underlying strategy and appropriate post-processing. As a convenience, however, OneTwo also provides a generic implementation of self-consistency in the form of the `SelfConsistency` strategy, which can be used to wrap an arbitrary underlying strategy. Below we show how use this to wrap an underlying chain-of-thought strategy, like in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfY8xNwnnO-j"
      },
      "outputs": [],
      "source": [
        "# For this example we will use a model with temperature \u003e 0, so that we can\n",
        "# sample multiple distinct candidates.\n",
        "on_backend_temp_0_7 = register_and_get_wrapper(\n",
        "    backend, generate_text_kwargs={'temperature': 0.7})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRj4EDLtnO-8"
      },
      "outputs": [],
      "source": [
        "# Wrapping a few-shot chain-of-thought strategy in self-consistency is as\n",
        "# simple as this.\n",
        "cot_sc = self_consistency.SelfConsistency(\n",
        "    sampler=sampling.Repeated(few_shot_cot_strategy),\n",
        "    bucketizer=lambda x: x.answer,  # Use the final answer as the bucket.\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkAqSQPunO-9"
      },
      "outputs": [],
      "source": [
        "# Now let's run it on the same question as before and inspect the results.\n",
        "answer_distribution_cot_sc, trace_cot_sc = ot.run(on_backend_temp_0_7(\n",
        "    cot_sc(question)), enable_tracing=True)\n",
        "\n",
        "pprint.pprint(answer_distribution_cot_sc, width=160)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdDk501_XrPd"
      },
      "source": [
        "As you can see, rather than returning just a single `CoTReply`, the strategy now returns a probability distribution over possible `CoTReply` candidates, with votes aggregated by bucketizing all of the candidate replies that contained the same final answer.\n",
        "\n",
        "This distribution shows the highest probability assigned to the correct answer ('5'), which some probability scattered over several other incorrect candidates. For each of the candidate answers, we also receive one representative reasoning path that led to that answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sORLpQsxZAmz"
      },
      "source": [
        "If all we care about is extracting the consensus answer, we can do so by wrapping the `SelfConsistency` strategy with `ExtractConsensus`. This gives us a strategy with exactly the same function signature as the original underlying strategy (in this case, of `few_shot_cot_strategy`), so that it can be used as a drop-in replacement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7RwFS5SZMml"
      },
      "outputs": [],
      "source": [
        "cot_sc_concensus = self_consistency.ExtractConsensus(cot_sc)\n",
        "cot_sc_consensus_reply, trace_cot_sc_consensus = ot.run(\n",
        "    on_backend_temp_0_7(cot_sc_concensus(question)), enable_tracing=True)\n",
        "cot_sc_consensus_reply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u2T_C3_ZrCD"
      },
      "source": [
        "If we inspect the trace, we can see the full series of parallel calls to the LLM, along with the post-processing step that led to the consensus answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_5Hc69W94A4"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace_cot_sc_consensus))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J71bgSVypamm"
      },
      "source": [
        "## Agents and Tool Use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rqq_U9bfk2"
      },
      "source": [
        "The same low-level primitives illustrated above can also be used to encapsulate generic higher-level strategies into reusable building blocks, which can in turn be composed to build more complex custom solutions.\n",
        "\n",
        "In this section we will illustrate two higher-level strategies that are available as off-the-shelf components in OneTwo, both targeting multi-step tool use:\n",
        "\n",
        "\n",
        "1.   `ReActAgent`\n",
        "2.   `PythonPlanningAgent`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuF-dxzpgZi1"
      },
      "source": [
        "Both of these strategies take the form of an \"agent\", which we define as a strategy that converts inputs to outputs by way of a series of repeated updates to an internal state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3pW2gYu2zIU"
      },
      "source": [
        "### ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdCZIRuNeymM"
      },
      "source": [
        "The `ReActAgent` is based on the \"ReAct\" strategy presented in https://arxiv.org/abs/2210.03629.\n",
        "\n",
        "In this strategy, we present the LLM with a list of tool descriptions with invocation examples, and then iteratively prompt the LLM to output a sequence of steps, each of which consists of a \"thought\", an \"action\" and an \"observation\". The \"thought\" and \"action\" are output by the LLM directly. At each step, we programmatically parse the LLM-generated \"action\" string, perform the corresponding tool call, and then use the result of that tool call as the \"observation\" that is included in the LLM prompt in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgH--0of3J2n"
      },
      "source": [
        "#### Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss57Hu7NgPgP"
      },
      "source": [
        "As a first step, we will prepare a list of tools that we want to make available to the LLM.\n",
        "\n",
        "In this case, we will use two tools:\n",
        "\n",
        "*   **Python:** Tool for executing a program in a Python sandbox (e.g., for performing calculations).\n",
        "*   **Search:** Tool for retrieving snippets from Google Search.\n",
        "\n",
        "In addition, we provide one more \"tool\", which is actually just a means for the LLM to indicate when it is ready to return the final answer:\n",
        "\n",
        "*   **Finish:** Simple \"tool\" that the LLM uses to indicate when it is ready to return the final answer.\n",
        "\n",
        "For each tool, we provide a tool name, description, and usage example. These will all be included in the prompt that is shown to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j9Hwvs_TFO_"
      },
      "outputs": [],
      "source": [
        "python_sandbox_class = python_execution_safe_subset.PythonSandboxSafeSubset\n",
        "python_sandbox_factory = python_execution_safe_subset.PythonSandboxSafeSubsetFactory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JczX5xa3nm7"
      },
      "outputs": [],
      "source": [
        "# Python tool for executing a program in a Python sandbox.\n",
        "PYTHON_EXAMPLE = textwrap.dedent(\"\"\"\\\n",
        "  tool_code(\"1 + 1\") returns \"2\".\n",
        "  We can also run multiple lines of code like this:\n",
        "  ```tool_code\n",
        "  a = []\n",
        "  a.append(1)\n",
        "  a.append(2)\n",
        "  a\n",
        "  ```\n",
        "  returns [1, 2].\"\"\")\n",
        "\n",
        "# Here we show the simplest case of a stateless Python tool. If we don't need\n",
        "# to carry variable state over from one call to another, we can just create a\n",
        "# fresh sandbox on each invocation of the Python tool.\n",
        "async def run_stateless_python(request: str) -\u003e str:\n",
        "  temporary_sandbox = python_sandbox_class()\n",
        "  async with temporary_sandbox.start() as temporary_sandbox:\n",
        "    result = await temporary_sandbox.run(request)\n",
        "    return str(result)\n",
        "\n",
        "python_tool = llm_tool_use.Tool(\n",
        "    name='tool_code',\n",
        "    function=run_stateless_python,\n",
        "    description='Python interpreter. Can be used as a calculator or to execute any Python code. Returns the result of execution.',\n",
        "    example=PYTHON_EXAMPLE,\n",
        "    color='plum',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zlnAB9I3oe_"
      },
      "outputs": [],
      "source": [
        "# In the open source environment, there are a number of commercial services\n",
        "# available for accessing web search. If you do not have web search connectivity\n",
        "# set up yet, you can start exploring the OneTwo agent strategies using the\n",
        "# following simple mock Search tool that returns hard-coded responses.\n",
        "# When using the agent for real, you can replace this with a function that calls\n",
        "# a real search engine, or that retrieves relevant passages from an indexed\n",
        "# corpus.\n",
        "def mock_search(query: str) -\u003e str:\n",
        "  response_by_query = {\n",
        "      'capital of France': 'Paris',\n",
        "      'population of Tuebingen': 'Tubingen 91,877 Population [2021]',\n",
        "      'population of Tubingen': 'Tubingen 91,877 Population [2021]',\n",
        "      'population Tuebingen': 'Tbingen 91,877 Population [2021]',\n",
        "      'population Tubingen': 'Tbingen 91,877 Population [2021]',\n",
        "      'population of Zuerich': '402,762 (2017)',\n",
        "      'population of Zurich': '402,762 (2017)',\n",
        "      'population of Zrich': '402,762 (2017)',\n",
        "      'population Zuerich': '402,762 (2017)',\n",
        "      'population Zurich': '402,762 (2017)',\n",
        "      'population Zrich': '402,762 (2017)',\n",
        "      'first president of the United States': 'George Washington',\n",
        "      'who was the first president of the United States?': 'George Washington',\n",
        "      'wife of George Washington': 'Martha Washington',\n",
        "      'who was the wife of George Washington?': 'Martha Washington',\n",
        "      'Frozen box office': '$1.280 billion',\n",
        "      'Frozen box office earnings': '$1.280 billion',\n",
        "      'Frozen movie box office earnings': '$1.280 billion',\n",
        "      'box office Frozen': '$1.280 billion',\n",
        "      'box office for Frozen': '$1.280 billion',\n",
        "      'box office of Frozen': '$1.280 billion',\n",
        "      'box office earnings of Frozen': '$1.280 billion',\n",
        "      'box office revenue Frozen': '$1.280 billion',\n",
        "      'how much did Frozen make at the box office?': '$1.280 billion',\n",
        "      'Lion King box office': '1.663 billion USD',\n",
        "      'Lion King box office earnings': '1.663 billion USD',\n",
        "      'Lion King movie box office earnings': '1.663 billion USD',\n",
        "      'box office Lion King': '1.663 billion USD',\n",
        "      'box office for Lion King': '1.663 billion USD',\n",
        "      'box office of Lion King': '1.663 billion USD',\n",
        "      'box office earnings of Lion King': '1.663 billion USD',\n",
        "      'box office revenue Lion King': '1.663 billion USD',\n",
        "      'how much did Lion King make at the box office?': '1.663 billion USD',\n",
        "      'Titanic box office': 'worldwide theatrical total = $2.264 billion',\n",
        "      'Titanic box office earnings': 'worldwide theatrical total = $2.264 billion',\n",
        "      'Titanic movie box office earnings': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office for Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office of Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office earnings of Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office revenue Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'how much did Titanic make at the box office?': 'worldwide theatrical total = $2.264 billion',\n",
        "  }\n",
        "  # Normalize capitalization.\n",
        "  response_by_query = {k.lower(): v for k, v in response_by_query.items()}\n",
        "  query = query.lower()\n",
        "  return response_by_query.get(query, 'No results.')\n",
        "\n",
        "search_tool = llm_tool_use.Tool(\n",
        "    name='search',\n",
        "    function=mock_search,\n",
        "    description='Search engine. Returns a relevant snippet or answer to query.',\n",
        "    example=textwrap.dedent(\"search('capital of France')  # returns 'Paris'\"),\n",
        "    color='darkseagreen',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QWTDoRa5q0X"
      },
      "outputs": [],
      "source": [
        "# The \"Finish\" function provides the LLM with a way of indicating when it is\n",
        "# ready to return a final answer. E.g., \"Finish('USA')\" returns 'USA'.\n",
        "finish_tool = llm_tool_use.Tool(\n",
        "    name='finish',\n",
        "    function=lambda x: x,\n",
        "    description='Function for returning the final answer.',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD03FWpki1dT"
      },
      "outputs": [],
      "source": [
        "tools = [python_tool, search_tool, finish_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjaoKmFdRRof"
      },
      "source": [
        "Finally, we create a couple of few-shot examples to demonstrate the use of these tools:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXxBnraASacp"
      },
      "outputs": [],
      "source": [
        "react_fewshots = react.default_react_exemplars(\n",
        "    python_tool_name='tool_code',\n",
        "    search_tool_name='search',\n",
        "    finish_tool_name='finish',\n",
        ")\n",
        "pprint.pprint(react_fewshots)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa-Jv6Sh3Gpm"
      },
      "source": [
        "#### Invoke ReActAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNioHChyireA"
      },
      "source": [
        "Once we've set up the tool handler and the few-shot examples, constructing a `ReActAgent` and executing it on a question is just a matter of a few lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9HlNTj3YkEz"
      },
      "outputs": [],
      "source": [
        "react_agent = react.ReActAgent(\n",
        "    exemplars=react_fewshots,\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        tools=tools,\n",
        "    ),\n",
        "    max_steps=10,\n",
        "    stop_prefix='',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "852PwsNyjSGE"
      },
      "source": [
        "In the simplest usage, we can treat the `ReActAgent` as a black box -- i.e., as just a function that takes a question as input and then returns the answer. We can do this quite literally, as the `Agent` class qualifies as a `Callable`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZZ3iBXpYkE0"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer = ot.run(react_agent(inputs=question))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um2O6qsY3RJX"
      },
      "source": [
        "#### Inspect ReActAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkoM9zRn6Y6Q"
      },
      "source": [
        "##### Inspect steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6o9rkU_jVL-"
      },
      "source": [
        "If we want to see what is going on under the hood, there are multiple ways to do this.\n",
        "\n",
        "One simple way is to specify `return_final_state=True` when calling the agent. When we do this, we now receive a `final_state` object as return value, alongside the `answer` itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BemcJB9r581N"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, final_state = ot.run(react_agent(inputs=question, return_final_state=True))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZqWwYatkMJc"
      },
      "source": [
        "If we print the final state, we can see the series of steps that the agent took in determining the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEQRNAMe6EMr"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx-iD6oC6dPs"
      },
      "source": [
        "##### Inspect detailed trace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj4Qvz0XkYrL"
      },
      "source": [
        "For even more details, we can specify `enable_tracing=True` in the call to `ot.run` to receive a detailed execution trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf3TIhce6lYR"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, trace = ot.run(react_agent(inputs=question), enable_tracing=True)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBNY3LVtk1p8"
      },
      "source": [
        "If we print the execution trace, we can see the exact series of requests that were sent to the LLM, along with the LLM's replies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPVy9Ccc6zmw"
      },
      "outputs": [],
      "source": [
        "print(results.format_result(trace, color=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9xtKbVBL9tG"
      },
      "source": [
        "We can also render the trace as interactive HTML block, where we can explore the full hierarchy of the prompting strategy, from the top-level agent down to the LLM and tool calls. Try clicking on the stage names to expand/collapse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soi784rGL9tH"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIfWTPWR20ov"
      },
      "source": [
        "### Python Planning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUjphg0rpFth"
      },
      "source": [
        "The `PythonPlanningAgent` is inspired by various various research in Python-based tool orchestration, such as ViperGPT (https://arxiv.org/pdf/2303.08128.pdf) and AdaPlanner (https://arxiv.org/pdf/2305.16653.pdf).\n",
        "\n",
        "In this strategy, we present the LLM with a list of tool descriptions with invocation examples, and then iteratively prompt the LLM to output a sequence of steps, each of which consists of a Python code block, with \"thoughts\", where relevant, in the form of code comments. At each step, we execute the LLM-generated code in a Python sandbox that provides access to the relevant tools via predefined functions. We then take everything that the code writes to stdout, and we include that in the LLM prompt in the next step, similarly to how we did with the \"observation\" in the ReAct strategy.\n",
        "\n",
        "While the `ReActAgent` performs exactly one tool call in each step, the `PythonPlanningAgent` can potentially make multiple tool calls from a single code block, and can include other control structures like loops and if-statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poM2x5ZZ3X0u"
      },
      "source": [
        "#### Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xspmdmg_4ook"
      },
      "source": [
        "Similarly to what we did for ReAct, we will again start by configuring the tools that we want to make available to the `PythonPlanningAgent`.\n",
        "\n",
        "The way we register the tools is very similar to before. One thing you might have noticed in the syntax for the `ReActAgent` was that we specified the list of tools as part of a `PythonToolUseEnvironmentConfig`. While for `ReActAgent`, we were treating the `PythonToolUseEnvironment` as basically just a class that manages a set of tools and provides a uniform way to call them, the `PythonToolUseEnvironment` actually contains quite a bit more functionality than that, including functionality to allow the tools to be called from within a Python sandbox, and to create and manage Python sandboxes on demand. In the `PythonPlanningAgent`, we will use this full range of functionality, as we orchestrate the tool use via execution of blocks of Python code.\n",
        "\n",
        "Note that for security reasons, it is important to always use a well-protected sandbox when automatically executing code that was generated by an LLM, similarly to how you would avoid any unprotected automatic execution of code that was provided by an untrusted user. The main idea of the sandbox is that we don't want to allow the LLM-generated program to directly read/write files or directly perform RPCs or import arbitrary libraries. Instead, we will provide an explicit allow-list of libraries to be imported and functions that can be called, which will include all of the tools from the tool handler.\n",
        "\n",
        "In this case, we will register two tools:\n",
        "\n",
        "*   **search:** Tool for retrieving snippets from Google Search (same as in the ReAct example).\n",
        "*   **firstnumber:** Simple function for extracting the first number from a block of text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m96l9fj4Hu_"
      },
      "outputs": [],
      "source": [
        "# Register a first tool called 'search' similar to the one used in ReActAgent.\n",
        "# For the purposes of this colab, it just returns hard-coded responses. (When\n",
        "# using the agent for real, you can replace this with a function that calls\n",
        "# a real search engine, or that retrieves relevant passages from an indexed\n",
        "# corpus.)\n",
        "search_tool = llm_tool_use.Tool(\n",
        "    name='search',\n",
        "    function=mock_search,\n",
        "    description='Search engine. Returns a relevant snippet or answer to query.',\n",
        "    example=textwrap.dedent(\"search('capital of France')  # returns 'Paris'\"),\n",
        "    color='darkseagreen',\n",
        ")\n",
        "\n",
        "# Register a second tool called 'firstnumber' which is just a simple Python\n",
        "# function that we define here, for extracting the first number from a block of\n",
        "# text.\n",
        "def firstnumber(x):\n",
        "  matches = re.match(r'[^\\d]*([\\d\\.,]+).*', str(x).replace(',', ''))\n",
        "  if matches:\n",
        "    try:\n",
        "      return float(matches.group(1))\n",
        "    except Exception as e:\n",
        "      return f'Error: could not parse {x} as a number ({e})'\n",
        "  else:\n",
        "    return f'Error: could not parse {x} as a number'\n",
        "\n",
        "first_number_tool = llm_tool_use.Tool(\n",
        "    name='firstnumber',\n",
        "    function=firstnumber,\n",
        "    description='Extracts the first number in a string.',\n",
        "    example=\"firstnumber('it is 1,203m high')  # return 1203.0 as a float\",\n",
        ")\n",
        "\n",
        "tools = [search_tool, first_number_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh5LmoZq3cgT"
      },
      "source": [
        "#### Invoke PythonPlanningAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4hB6ztC8acK"
      },
      "source": [
        "Once we've set up the tool handler, constructing a `PythonPlanningAgent` and executing it on a question is again just a matter of a few lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSZlzLw2yFE5"
      },
      "outputs": [],
      "source": [
        "python_agent = python_planning.PythonPlanningAgent(\n",
        "    exemplars=python_planning.DEFAULT_PYTHON_PLANNING_EXEMPLARS,\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        sandbox_factory=python_sandbox_factory,\n",
        "        tools=tools,\n",
        "    ),\n",
        "    max_steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Nscop98kIB"
      },
      "source": [
        "In the simplest usage, we can again simply treat the `PythonPlanningAgent` as just a function that takes a question as input and then returns the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P_ilWpByFE6"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Titanic?'\n",
        "answer = ot.run(python_agent(inputs=question))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7tW0FNH3gGo"
      },
      "source": [
        "#### Inspect PythonPlanningAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kOqBQib8XIN"
      },
      "source": [
        "##### Inspect steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRjw-w2S8vvU"
      },
      "source": [
        "The same options that we saw for inspecting the intermediate steps of a `ReActAgent` are available for `PythonPlanningAgent` as well.\n",
        "\n",
        "In particular, if we  specify `return_final_state=True` when calling the agent, we receive a `final_state` object as return value, alongside the `answer` itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX6e8sgZ8fGP"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Titanic?'\n",
        "answer, final_state = ot.run(python_agent(inputs=question, return_final_state=True))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMlIWt7d9JWr"
      },
      "source": [
        "If we print the final state, we can see the series of steps that the agent took in determining the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GsjxWX1861z"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3ABClO78ZNE"
      },
      "source": [
        "##### Inspect detailed trace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHYyWcIQ9S-o"
      },
      "source": [
        "For even more details, we can again specify `enable_tracing=True` in the call to `ot.run` to receive a detailed execution trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iONYreoFDvF"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Titanic?'\n",
        "answer, trace = ot.run(python_agent(inputs=question), enable_tracing=True)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yk9oaT_9dbZ"
      },
      "source": [
        "If we print the execution trace, we can again see the exact series of requests that were sent to the LLM, along with the LLM's replies. Each of the tool calls also appears in the execution trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uzkzWpVFDvF"
      },
      "outputs": [],
      "source": [
        "print(results.format_result(trace, color=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_O2RT8NK3Cp"
      },
      "source": [
        "We can also again render the trace as interactive HTML block to view the full hierarchy of the prompting strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrRktccEBxlX"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J4sKi0COHm0"
      },
      "source": [
        "# Additional Agent Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFiLfkuYRzhb"
      },
      "source": [
        "One thing you may have noticed in the above examples is how similar the syntax is for interacting with `ReActAgent` and `PythonPlanningAgent`. That actually is not a coincidence! Both of these strategies have been implemented as subclasses of a generic `Agent` interface.\n",
        "\n",
        "Strategies that are implemented in this way support a number of additional operations out-of-the-box, in addition to what you saw above.\n",
        "\n",
        "We will illustrate some of these operations using the `ReActAgent` example from above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nISq_RQOKZH"
      },
      "source": [
        "## Stream states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3ISgVqrRFZt"
      },
      "source": [
        "For a long-running agent, rather than running monolothically, we may alternatively choose to stream the sequence of agent states, so that we can potentially save progress as we go along or apply our own dynamic criteria for stopping.\n",
        "\n",
        "We can do this using `Agent.stream_states`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlRBDVYsOedL"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "\n",
        "state_iterator = react_agent.start_environment_and_stream_states(\n",
        "    initial_state=state0)\n",
        "\n",
        "state_trajectory = []\n",
        "with ot.safe_stream(state_iterator) as state_stream:\n",
        "  for state in state_stream:\n",
        "    print('State arrived!')\n",
        "    state_trajectory.append(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN5z58UMSrWK"
      },
      "source": [
        "We end up receiving one state for each step that the agent performed -- in this case, 4 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tOLl8LnO095"
      },
      "outputs": [],
      "source": [
        "len(state_trajectory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIZU-gO9S71B"
      },
      "source": [
        "If we inspect the first state, we can see that it is of the same form as the final state that we saw earlier, except that the `updates` list contains only the first step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haJgNI-sOwPD"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(state_trajectory[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjnmoRwNTH4U"
      },
      "source": [
        "If we inspect the second state, we can see that the `updates` list now contains two steps. And so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h_g_8OSO7q_"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(state_trajectory[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxbPwShLQTSk"
      },
      "source": [
        "## Invoke prompt template directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_0UNyYETRHn"
      },
      "source": [
        "If you ever want to debug the behavior of the prompt template that is used by the agent internally (e.g., if you customize the prompt and are iterating on debugging), you can also execute the prompt template standalone, which can be done as follows, using any of the intermediate states from the state trajectory retrieved above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvWPnl_3yoF9"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(react_agent.environment_config.tools, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRzsyVM8xbUA"
      },
      "outputs": [],
      "source": [
        "react_agent.prompt = react.ReActPromptJ2(text=react.DEFAULT_REACT_PROMPT_TEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CIzMzcQYT0"
      },
      "outputs": [],
      "source": [
        "prompt_outputs, trace = ot.run(react_agent.prompt(\n",
        "        # These arguments are just imitating what is done in the `ReActAgent`\n",
        "        # implementation (more or less copy-pasted from `react.py`).\n",
        "        tools=react_agent.environment_config.tools,\n",
        "        exemplars=react_agent.exemplars,\n",
        "        stop_prefix=react_agent.stop_prefix,\n",
        "        stop_sequences=react_agent._get_stop_sequences(),\n",
        "        force_finish=False,\n",
        "        # Here we can manually specify any of the intermediate states from the\n",
        "        # state trajectory above to reproduce the behavior of the prompt\n",
        "        # template at that step.\n",
        "        #state=state_trajectory[1],\n",
        "        state=state0,\n",
        "    ),\n",
        "    enable_tracing=True)\n",
        "pprint.pprint(prompt_outputs, width=160)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS1UfsT1UqJQ"
      },
      "source": [
        "If we want to see the precise prompt that was sent to the LLM, we can print the detailed execution trace of the prompt template, in the same way we did earlier for the agent strategy as a whole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ3NRnzwQ1F2"
      },
      "outputs": [],
      "source": [
        "print(results.format_result(trace, color=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzNqGKcxOOF8"
      },
      "source": [
        "## Stream updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBmv7bMcXpWD"
      },
      "source": [
        "Similarly to how we produced a stream of agent states using `Agent.stream_states`, we can alternatively produce a stream of state updates using `Agent.stream_updates`. The idea is very similar, except that each agent update contains just the new information that needs to be added to the previous state to create the new state. In the case of `ReActAgent`, the state update is represented as a `ReActStep` (the same data structure that we saw inside of the `final_state` earlier, for representing the individual steps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yj5CPggWRFl"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "\n",
        "# Request a stream of updates.\n",
        "update_iterator = react_agent.start_environment_and_stream_updates(\n",
        "    initial_state=state0)\n",
        "\n",
        "# Two things we could do with these updates:\n",
        "# (A) Gather them in a list / work with them directly.\n",
        "# (B) Add them to a previous state to create the next state.\n",
        "updates = []\n",
        "current_state = copy.deepcopy(state0)\n",
        "with ot.safe_stream(update_iterator) as state_stream:\n",
        "  for update in state_stream:\n",
        "    print('Update arrived!')\n",
        "    updates.append(update)\n",
        "    # Agent states can be updated using `+=` with a state update.\n",
        "    current_state += update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE44WxG2Yl8P"
      },
      "source": [
        "If we look at the list of updates, we can see that each update is one `ReActStep`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_khaEA8WwPz"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(updates, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txPC-CrsYu3j"
      },
      "source": [
        "By incrementally updating an initial state with each of the updates using `+=`, we can also reproduce the current state at any given step. Now that we have processed the full update stream, we can see that `current_state` is exactly the same as the `final_state` that we observed earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiOrlNS_W7go"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(current_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcCA2iTOOQ6a"
      },
      "source": [
        "## Stop / edit / resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNE0AmYFZS-C"
      },
      "source": [
        "Since the behavior of the agent at each step is fully determined by the contents of the agent state, we are free to directly manipulate any of these state objects and pass them back into the agent to see what the agent would have done in that scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJqUuGdRZ-go"
      },
      "source": [
        "As an example, let's use `stream_states` for just 2 steps and temporarily stop execution there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTQ6Ey6GOlOy"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "state_iterator = react_agent.start_environment_and_stream_states(\n",
        "    initial_state=state0,\n",
        "    max_steps=2)\n",
        "with ot.safe_stream(state_iterator) as state_stream:\n",
        "  state_trajectory = list(state_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sr151AxaKsF"
      },
      "source": [
        "At this point, we can see that the agent had just finish calling the `Search` tool to get the `population of Zuerich`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbT57_VNU6q9"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(state_trajectory[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM70KMHHaY5Z"
      },
      "source": [
        "Let's try modifying the `observation` from the last step to see what would have happened if Google Search had returned a different snippet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwMfNI90VCoE"
      },
      "outputs": [],
      "source": [
        "state_trajectory[1].updates[-1].observation = 'Population of Zurich: 5 million people and growing!'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68xAaVaJas6M"
      },
      "source": [
        "Now let's resume execution from this modified state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvEVO925VhC2"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, final_state = ot.run(react_agent(\n",
        "    inputs=question,\n",
        "    initial_state=state_trajectory[1],\n",
        "    return_final_state=True))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJgXKk8Vaybz"
      },
      "source": [
        "If we look at the final trajectory, we can see the modified internal state, as well as the new sequence of steps the agent would have taken after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm3G7wlkV9vT"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na0I3zpcrnlQ"
      },
      "source": [
        "## Customize exemplars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo5JIVYJrqLh"
      },
      "source": [
        "One thing you might have noticed when we were instantiating the `ReActAgent` and `PythonPlanningAgent` was that we needed to provide a list of exemplars.\n",
        "\n",
        "```python\n",
        "react_agent = react.ReActAgent(\n",
        "    exemplars=react_fewshots,          # \u003c== Exemplars\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        tools=tools,\n",
        "    ),\n",
        "    max_steps=10,\n",
        "    stop_prefix='')\n",
        "\n",
        "```\n",
        "\n",
        "```python\n",
        "python_agent = python_planning.PythonPlanningAgent(\n",
        "    exemplars=python_planning.DEFAULT_PYTHON_PLANNING_EXEMPLARS,         # \u003c== Exemplars\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        sandbox_factory=python_sandbox_factory,\n",
        "        tools=tools,\n",
        "    ),\n",
        "    max_steps=10)\n",
        "```\n",
        "\n",
        "So far we have just used a predefined default list of exemplars, which were compatible with the set of tools that we had configured. In practice, though, you may want to customize the list of exemplars, or even select them dynamically from some kind of larger exemplar pool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZp7JTfhs4EH"
      },
      "source": [
        "Let's take a look at the exemplars that we have been using so far for `ReActAgent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbxaLlb5s-Oo"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(react_agent.exemplars, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtgyUcNutMnW"
      },
      "source": [
        "One thing you might have noticed is that the format of these exemplars looks very similar to the `final_state` that is output when we execute the agent with `return_final_state = True`.\n",
        "\n",
        "This is again not a coincidence, and you can indeed directly reuse any state object output from a past run of the agent strategy as an exemplar in future runs, or you can construct them fully manually, or semi-automatically via the \"stop / edit / resume\" workflow shown earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36klZfYouRPq"
      },
      "source": [
        "Let's try harvesting a couple of trajectories output by our current `react_agent` and use those as exemplars in a new agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrtMoeL4ucdb"
      },
      "outputs": [],
      "source": [
        "q1 = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, final_state_q1 = ot.run(react_agent(inputs=q1, return_final_state=True))\n",
        "answer\n",
        "# pprint.pprint(final_state_q1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAlRgUvLw5MS"
      },
      "outputs": [],
      "source": [
        "q2 = 'Who was the wife of the first president of the United States?'\n",
        "answer, final_state_q2 = ot.run(react_agent(inputs=q2, return_final_state=True))\n",
        "answer\n",
        "# pprint.pprint(final_state_q2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLJXbk-OwU4t"
      },
      "outputs": [],
      "source": [
        "react_agent2 = react.ReActAgent(\n",
        "    exemplars=[final_state_q1, final_state_q2],\n",
        "    environment_config=react_agent.environment_config,\n",
        "    max_steps=10,\n",
        "    stop_prefix='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEJ_2H8BzAPr"
      },
      "source": [
        "If we look at the exemplars of the new agent, we can see that these indeed correspond to the trajectories from the two questions that we presented to the first agent above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veSmLJtnuxU5"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(react_agent2.exemplars, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUAW8TYhzOso"
      },
      "source": [
        "Now let's try the new agent on a new question and see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQEF42dLudJ6"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Lion King?'\n",
        "(answer, final_state), trace = ot.run(react_agent2(inputs=question, return_final_state=True), enable_tracing=True)\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdmFmd1DxnCj"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq_3bIt-zSgb"
      },
      "source": [
        "If we look at the prompt that `react_agent2` sent to the LLM, we can see that the exemplars were now the ones that we specified, which were bootstrapped from the original `react_agent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXt64NrRvQM0"
      },
      "outputs": [],
      "source": [
        "print(results.format_result(trace.get_leaf_results()[0], color=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAqxD4A7dWT8"
      },
      "source": [
        "## Run step-by-step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WB-yqzWm0L6"
      },
      "source": [
        "As an alternative to iterating through a stream of updates, we can also run individual steps of the agent interactively using `Agent.sample_next_step`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx6arXo8diEU"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "env = python_tool_use.PythonToolUseEnvironment(config=config)\n",
        "\n",
        "# Since we will perform multiple operations on the same environment\n",
        "# actively, we start the environment manually here, rather than wrapping\n",
        "# everything in a context manager. (We will call `stop` manually later.)\n",
        "env = ot.run(env.start_unsafe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s4qStvGdsGb"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "\n",
        "# Sample a single candidate for step 1).\n",
        "next_step_candidates = ot.run(\n",
        "    react_agent.sample_next_step(\n",
        "        state=state0, num_candidates=1, environment=env\n",
        "    )\n",
        ")\n",
        "pprint.pprint(next_step_candidates, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2e_XUHdfd24"
      },
      "outputs": [],
      "source": [
        "update1 = next_step_candidates[0]\n",
        "state1 = state0 + next_step_candidates[0]\n",
        "\n",
        "# Sample a single candidate for step 2.\n",
        "next_step_candidates = ot.run(\n",
        "    react_agent.sample_next_step(\n",
        "        state=state1, num_candidates=1, environment=env\n",
        "    )\n",
        ")\n",
        "pprint.pprint(next_step_candidates, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWA_aVVMwNHA"
      },
      "outputs": [],
      "source": [
        "# We need to stop the environment manually, since we started it manually\n",
        "# earlier.\n",
        "env.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dhXcl0GiM23"
      },
      "source": [
        "## Sample multiple candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAG7K28Iv_ll"
      },
      "source": [
        "You may have noticed that when we called `sample_next_step` above, rather than returning just a single state update, it returned a list of state updates (in this case, a list of length 1).\n",
        "\n",
        "If we specify a value of `num_candidates` greater than 1, we can generate multiple candidate next steps.\n",
        "\n",
        "This is how you would interact with the agent, for example, if you wanted to implement a higher-level strategy on top of it such as beam search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCWhBBlZiUNf"
      },
      "outputs": [],
      "source": [
        "ot.run(env.start_unsafe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz7KXabKiWVB"
      },
      "outputs": [],
      "source": [
        "question = 'What is the population of the third largest city in Switzerland?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdEnbZO83MZ1"
      },
      "outputs": [],
      "source": [
        "# Since `register_and_get_wrapper` makes a full copy of the current\n",
        "# function registry, we make a new wrapper now for `m_it_temp_0_7` to include\n",
        "# the `GSearchEngine` that was registered in the previous section.\n",
        "on_backend_temp_0_7_with_search = register_and_get_wrapper(\n",
        "    backend, generate_text_kwargs={'temperature': 0.7})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z08Z_URYhTtX"
      },
      "outputs": [],
      "source": [
        "# Sample three candidates for step 1.\n",
        "# Note that we use a model with temperature \u003e 0, so that we can sample multiple\n",
        "# distinct candidates.\n",
        "next_step_candidates = ot.run(\n",
        "    on_backend_temp_0_7_with_search(\n",
        "        react_agent.sample_next_step(\n",
        "            state=state0, num_candidates=3, environment=env\n",
        "        )\n",
        "    )\n",
        ")\n",
        "pprint.pprint(next_step_candidates, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJWWtFtvdnKL"
      },
      "outputs": [],
      "source": [
        "env.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bo3VABFxCtD"
      },
      "source": [
        "Notice that `next_step_candidates` this time was a list containing three different candidate `ReActStep` objects, all being candidates for the same step (Step 1 in this case)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtfBYHvf02dc"
      },
      "source": [
        "# PythonToolUseEnvironment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx5SCBFB1JBX"
      },
      "source": [
        "Although agents in OneTwo don't necessarily need to involve tool use, the two agents shown so far (`ReActAgent` and `PythonPlanningAgent`) both did take advantage of tool use and/or code execution capabilities, which were provided by a shared `PythonToolUseEnvironment` class.\n",
        "\n",
        "In this section, we will illustrate some additional operations that can be performed using the `PythonToolUseEnvironment`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz6t09Z0ri4v"
      },
      "source": [
        "## Invoke tools directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90u8RwzlrnH0"
      },
      "source": [
        "If you ever want to debug the behavior of the tools that are called by the agent internally, you can execute the tools standalone as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03moJ_f3tgan"
      },
      "source": [
        "First let's try executing the `Search` tool that we registered earlier with the `ReActAgent`. We can do this using `PythonToolUseEnvironment.run_tool`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3p2FjuOsS_N"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_tool(tool_name='Search', tool_args=['capital of France'], tool_kwargs={})\n",
        "  )\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3nCdvRctnk9"
      },
      "source": [
        "Now let's try executing the `Python` tool from the same `ReActAgent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTgvg459s6iR"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_tool(tool_name='tool_code', tool_args=['8849 - 8611'], tool_kwargs={})\n",
        "  )\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO_oNVzw0Guc"
      },
      "source": [
        "## Run code directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHoecBVVxF6N"
      },
      "source": [
        "To spawn a Python sandbox and run a block of code in it (potentially including calls to tools) the way that is done in `PythonPlanningAgent`, we can use `PythonToolUseEnvironment.run_code`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYP2hmTNu-7P"
      },
      "outputs": [],
      "source": [
        "config = python_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_code(\n",
        "          sandbox_state=tuple(),\n",
        "          code=textwrap.dedent(\"\"\"\\\n",
        "          search_result = search('population of Tubingen')\n",
        "          print('Search result: %s' % search_result)\n",
        "          population = firstnumber(search_result)\n",
        "          population\n",
        "          \"\"\"),\n",
        "      )\n",
        "  )\n",
        "pprint.pprint(result, width=160)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42gDq_8CxpB9"
      },
      "source": [
        "Note that `PythonToolUseEnvironment.run_code` is somewhat similar to running the \"Python tool\" from `ReActAgent`, but is more powerful, since the executed Python code can include calls to any of the tools registered in the `PythonToolUseEnvironment` (for example, the \"search\" and \"firstnumber\" tools invoked in the code snippet above). Note also that while the \"Python tool\" returns just a single string as its output, `run_code` returns a much more detailed `RunCodeResult` object, which contains, among other things, both the value of the final expression (in this case, the value of `population`) and all content that was written to `stdout`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc800ur92Dz2"
      },
      "source": [
        "One thing you may have noticed above is the `sandbox_state` parameter, which we set simply to an empty `tuple()`. The tuple provided in `sandbox_state` represents the sequence of code blocks that we expect to have been executed in the Python sandbox so far. In this case, we requested that the code be run in a fresh sandbox (i.e., one in which no code has been executed yet). For achieving the effect of a stateful sandbox, however, we can also specify a non-empty sequence of code blocks as the `sandbox_state`, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEizEIft0Uqa"
      },
      "outputs": [],
      "source": [
        "config = python_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_code(\n",
        "          sandbox_state=(\n",
        "              \"search_result = 'Tubingen 91,877'\",\n",
        "          ),\n",
        "          code='firstnumber(search_result)',\n",
        "      )\n",
        "  )\n",
        "result.sandbox_result.final_expression_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcNohQjG3Vph"
      },
      "source": [
        "Notice that the code succeeded in running even though it included a reference to a variable `search_result` that was not directly defined in the given code block, since it was defined in one of the code blocks from the provided `sandbox_state`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrgHBKs64fTg"
      },
      "source": [
        "Notice also that the above `run_code` request succeeded even though we had never actually previously run the precise code block `\"search_result = 'Tubingen 91,877'\"` on this `PythonToolUseEnvironment` instance up till now. In cases like this, `PythonToolUseEnvironment` will automatically reconstruct Python sandbox instances on demand that match the requested state. This is convenient, for example, if you have a long-running `PythonPlanningAgent` strategy for which you would like to serialize the intermediate state, and then later deserialize the state and continue execution in a separate process, or if you were to apply a branching strategy like beam search over top of the `PythonPlanningAgent` trajectories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlY6w7Vf7GN_"
      },
      "source": [
        "In the simple case where you want to just execute a series of code blocks in a single stateful Python sandbox in a single process, you can simply pass in the list of code blocks executed so far in each call to `PythonToolUseEnvironment.run_code`, and the environment will automatically reuse the same sandbox for the whole series of calls, to avoid any unnecessary duplicate code execution.\n",
        "\n",
        "Here is an example of what that could look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjhmJUyY7JFK"
      },
      "outputs": [],
      "source": [
        "config = python_agent.environment_config\n",
        "code_blocks_to_execute = [\n",
        "    \"search_result = 'Tbingen 91,877'\",\n",
        "    'firstnumber(search_result)',\n",
        "]\n",
        "code_blocks_executed_so_far = []\n",
        "result_list = []\n",
        "\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  for code in code_blocks_to_execute:\n",
        "    result = ot.run(\n",
        "        env.run_code(\n",
        "            sandbox_state=tuple(code_blocks_executed_so_far),\n",
        "            code=code,\n",
        "        )\n",
        "    )\n",
        "    result_list.append(result)\n",
        "    code_blocks_executed_so_far.append(code)\n",
        "\n",
        "pprint.pprint(result_list, width=160)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTaLMNXSp0Nw"
      },
      "source": [
        "## Start environment manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IEDAPslshrj"
      },
      "source": [
        "When using a `PythonToolUseEnvironment`, we need to always \"start\" it first and then \"stop\" it onces we are done, so as to ensure that any background threads or other resources get cleaned up.\n",
        "\n",
        "As you may have noticed in the examples above, there are two different syntaxes available for starting and stopping the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVZozTtEtFG5"
      },
      "source": [
        "When implementing a prompting strategy for real, the recommended approach is to start the environment via a context manager, as shown below. In this approach, the environment is started when it is enters the context and then is automatically stopped when exiting the context. This is the \"safest\" syntax to use, as it ensures that the environment is always cleaned up at the end, even in cases where execution is interrupted due to an exception."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCVavsOqrEd0"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "# The environment will start when entering the context and stop when exiting.\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(env.run_code(sandbox_state=tuple(), code='x = 1'))\n",
        "  result = ot.run(env.run_code(sandbox_state=('x = 1',), code='x * 2'))\n",
        "result.sandbox_result.final_expression_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brtFxXM9nbMJ"
      },
      "source": [
        "In cases where we will be performing a series of interactive operations on the same `PythonToolUseEnvironment` instance, though, it can be convenient to simply start the environment once at the beginning, and then just leaving it running indefinitely until we choose to manually stop it later, as shown below.\n",
        "\n",
        "Note that this approach is \"unsafe\" in the sense that if we forget to call `env.stop()` (or if that line fails to run due to an exception raised in an earlier part of the code), we could be left with some threads left running in the background, which would be undesirable in a long-running process. For the purposes of experimenting manually in colab, though, this approach is fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoRf89zxsJX6"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "env = python_tool_use.PythonToolUseEnvironment(config=config)\n",
        "\n",
        "# Here we start the environment manually.\n",
        "ot.run(env.start_unsafe())\n",
        "\n",
        "result = ot.run(env.run_code(sandbox_state=tuple(), code='x = 1'))\n",
        "result = ot.run(env.run_code(sandbox_state=('x = 1',), code='x * 2'))\n",
        "\n",
        "# Here we stop the environment manually.\n",
        "# (This could also be done in a separate cell.)\n",
        "env.stop()\n",
        "\n",
        "result.sandbox_result.final_expression_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-VJtIUNS0PC"
      },
      "source": [
        "# Agent Evals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk7w_Qzikw2v"
      },
      "source": [
        "For evaluating an agent strategy over a dataset, one option is to follow the same approach illustrated earlier in the \"Evals\" section, where the general-purpose `evaluation.evaluate` script provides a loose structure for the eval run, but where you define for yourself the format in which to output the evaluation results.\n",
        "\n",
        "As an alternative, though, we also provide an `agent_evaluation.evaluate` script, which is designed to be easy to use with agent strategies, and which automatically generates various detailed debug information out-of-the-box in a standardized format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCCWbuanm6He"
      },
      "source": [
        "In both cases, we would start by constructing a dataset consisting of a list or stream of examples, where each example is represented as a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtjACM5kTX_g"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    {'question': 'What is the total population of Tuebingen and Zuerich?',\n",
        "     'answer': 494639},\n",
        "    {'question': 'Which movie had the larger box office: Frozen or Titanic?',\n",
        "     'answer': 'Titanic'},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om74PtcSnUw0"
      },
      "source": [
        "When using the `agent_evaluation` script, we define our metric functions following the function signature shown below, where the function takes as input a target and prediction and returns a float value.\n",
        "\n",
        "In this example, we are providing just a simple, ordinary function, but it could also be an `async` function, e.g., for a metric based on an AI rater."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrS6UW9ZmIG"
      },
      "outputs": [],
      "source": [
        "def number_aware_accuracy(target: str | float | int, prediction: str) -\u003e float:\n",
        "  if isinstance(target, float) or isinstance(target, int):\n",
        "    # Numerical comparison: Ignore thousands separators, handle rounding error.\n",
        "    try:\n",
        "      prediction_as_float = float(prediction.replace(',', ''))\n",
        "      correct = (round(target - prediction_as_float, 8) == 0.0)\n",
        "    except ValueError:\n",
        "      correct = False\n",
        "  else:\n",
        "    # String comparison.\n",
        "    correct = (target == prediction)\n",
        "  return float(correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rah0K6TRodf-"
      },
      "source": [
        "You can then run the eval as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4swk9FMUJNF"
      },
      "outputs": [],
      "source": [
        "react_summary = agent_evaluation.evaluate(\n",
        "    strategy=react_agent,\n",
        "    examples=dataset,\n",
        "    metric_functions={'accuracy': number_aware_accuracy},\n",
        "    output_results=True,\n",
        "    output_results_debug=True,\n",
        "    output_final_states=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga51ECD0piD6"
      },
      "outputs": [],
      "source": [
        "print(f'\\nTime elapsed (seconds): {react_summary.timing.time_elapsed.total_seconds()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXImio46ptFe"
      },
      "source": [
        "The returned object is an `EvaluationSummary` object, which summarizes the aggregated eval results (metrics, counters, timing), along with various optional details for debugging purposes:\n",
        "* **results:** Brief summary of each example: Inputs, outputs, metric, etc.\n",
        "* **results_debug:** Detailed trace of each example.\n",
        "* **final_states:** Final state of the agent for each example (only relevant when the strategy is an agent).\n",
        "\n",
        "Try clicking on the various elements in the hierarchies to expand/collapse!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_JccsYZUUTW"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(react_summary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "081MOFNp3cpZ"
      },
      "source": [
        "# Self-Consistency Variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYyYOF753jZU"
      },
      "source": [
        "Here we illustrate additional variants of the self-consistency strategy that appeared in the original self-consistency paper ([Wang, et al., 2023](https://arxiv.org/pdf/2203.11171)) or in follow-up research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW_sog79pDaG"
      },
      "outputs": [],
      "source": [
        "# For purposes of comparison, we will evaluate each of the variants on the\n",
        "# following question (same one used in the earlier ReAct section).\n",
        "question = 'What is the total population of Tuebingen and Zuerich?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mOnz3qT5Hj9"
      },
      "source": [
        "## SC with CoT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3bBUkanKu8A"
      },
      "source": [
        "Self-consistency with chain-of-thought is the most basic/traditional usage of self-consistency, as presented in [Wang, et al., 2023](https://arxiv.org/pdf/2203.11171) and in the presentation of self-consistency in the Overview section.\n",
        "\n",
        "As a reminder, this is what it looks like to initialize this strategy, using the zero-shot two-step variants of chain-of-thought."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq8vhJGxrerO"
      },
      "outputs": [],
      "source": [
        "cot_sc = self_consistency.SelfConsistency(\n",
        "    sampler=sampling.Repeated(chain_of_thought.QACoTPromptJ2()),\n",
        "    bucketizer=lambda x: x.answer,  # Use the final answer as the bucket.\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjH6g60KLd53"
      },
      "source": [
        "And here is the output of running this strategy on our question of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43px78K19wjB"
      },
      "outputs": [],
      "source": [
        "answer_distribution_cot_sc, trace_cot_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_sc(question)), enable_tracing=True)\n",
        "\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_cot_sc):\n",
        "  cot_reply = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': probability,\n",
        "      'Answer': cot_reply.answer,\n",
        "      'Representative Reasoning': cot_reply.reasoning,\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN-Q5yIc9wjC"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer().render(trace_cot_sc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJwk1Gba5UFJ"
      },
      "source": [
        "## SC with ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQp2vwYiMIho"
      },
      "source": [
        "Aside from chain-of-thought, we can just as easily apply self-consistency to arbitrarily complex prompting strategies, such as `ReActAgent`, as long as there is some notion of \"intermediate steps\" to marginalize over, and some notion of \"final answer\" to vote on, and where many different sequences of intermediate steps could potentially lead to the same final answer. In the case of `ReActAgent`, the natural choice is to use the agent's final state (i.e., the trajectory of thoughts/actions that were taken by the agent) as the intermediate steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BouhMwsngKSL"
      },
      "outputs": [],
      "source": [
        "react_agent = react.ReActAgent(\n",
        "    exemplars=react_fewshots,\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        tools=[python_tool, search_tool, finish_tool],\n",
        "    ),\n",
        "    max_steps=10,\n",
        "    stop_prefix='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J30Bt4sr5-J3"
      },
      "outputs": [],
      "source": [
        "react_sc = self_consistency.SelfConsistency(\n",
        "    # We use `return_final_state=True` to make it easy to inspect/evaluate a\n",
        "    # representative reasoning trajectory along with the final answer, the same\n",
        "    # way we did for chain-of-thought.\n",
        "    sampler=sampling.Repeated(\n",
        "        functools.partial(react_agent, return_final_state=True)),\n",
        "    bucketizer=lambda x: x[0],  # Use the final answer as the bucket.\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF6LDOZB6USI"
      },
      "outputs": [],
      "source": [
        "answer_distribution_react_sc, trace_react_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(react_sc(question)), enable_tracing=True)\n",
        "\n",
        "# Let's summarize the results in a table in the same format as before.\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_react_sc):\n",
        "  answer, final_state = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': probability,\n",
        "      'Answer': answer,\n",
        "      'Representative Reasoning': final_state,\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKNqAHt01vSp"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer(levels_to_expand=2).render(trace_react_sc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypeo2BX55atr"
      },
      "source": [
        "## SC over diverse exemplars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPLVJc_zOU6H"
      },
      "source": [
        "Self-consistency is typically thought of as a type of \"rationale-augmented ensemble\" [[Wang, et al., 2022]](https://arxiv.org/pdf/2207.00747). As an ensembling method, its effectiveness depends crucially on the diversity and independence of the samples that are being aggregated over. The less diversity in the samples, the more quickly the consensus accuracy will tend to plateau.\n",
        "\n",
        "One approach that is sometimes taken for increasing the diversity of the samples is to draw the samples from multiple different variations of the underlying strategy. In particular, one approach that was adopted in [[Li, et al., 2023]](https://arxiv.org/pdf/2206.02336) is to use the same basic underlying prompting strategy (e.g., the same basic prompt template), but with different choices of few-shot exemplars.\n",
        "\n",
        "We can do this easily using the `RoundRobin` sampler, which takes a list of other samplers as input and then does round-robin over them.\n",
        "\n",
        "The resulting answer distribution is in the same format as before, but with more diversity of reasoning paths (and in this case, of answers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzQ_P2EWCppX"
      },
      "outputs": [],
      "source": [
        "cot_exemplars = chain_of_thought.QA_COT_EXEMPLARS_ORIGINAL_MATH_WORD_PROBLEMS\n",
        "cot_over_diverse_exemplars_sampler = sampling.RoundRobin([\n",
        "    sampling.Repeated(chain_of_thought.QACoTPromptJ2(\n",
        "        exemplars=cot_exemplars[0:4])),\n",
        "    sampling.Repeated(chain_of_thought.QACoTPromptJ2(\n",
        "        exemplars=cot_exemplars[4:8])),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7RD9wKPCtXH"
      },
      "outputs": [],
      "source": [
        "cot_diverse_exemplars_sc = self_consistency.SelfConsistency(\n",
        "    sampler=cot_over_diverse_exemplars_sampler,\n",
        "    bucketizer=lambda x: x.answer,  # Use the final answer as the bucket.\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui0japsKCtXQ"
      },
      "outputs": [],
      "source": [
        "answer_distribution_cot_div_ex_sc, trace_cot_div_ex_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_diverse_exemplars_sc(question)),\n",
        "    enable_tracing=True)\n",
        "\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_cot_div_ex_sc):\n",
        "  cot_reply = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': probability,\n",
        "      'Answer': cot_reply.answer,\n",
        "      'Representative Reasoning': cot_reply.reasoning,\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6IZP8q6CtXQ"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer().render(trace_cot_div_ex_sc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVyyAgrE5m8-"
      },
      "source": [
        "## SC over diverse strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6jQVGFHRHJF"
      },
      "source": [
        "The same principle can be applied even more radically by sampling round-robin over fundamentally different prompting strategies. E.g., in the example below, we generate some of the samples using chain-of-thought and others using ReAct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuc9o4OVDT7E"
      },
      "outputs": [],
      "source": [
        "cot_over_diverse_strategies_sampler = sampling.RoundRobin([\n",
        "    sampling.Repeated(chain_of_thought.QACoTPromptJ2()),\n",
        "    sampling.Repeated(functools.partial(react_agent, return_final_state=True)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0L3m2LODySk"
      },
      "outputs": [],
      "source": [
        "# Since the outputs of the different strategies are in slightly different\n",
        "# formats, we define a couple of helper functions here for extracting the\n",
        "# \"answer\" and \"reasoning\" in a way that would work with either format.\n",
        "\n",
        "def extract_answer_cot_or_react(\n",
        "    x: chain_of_thought.CoTReply | tuple[str, str]) -\u003e str:\n",
        "  if isinstance(x, chain_of_thought.CoTReply):\n",
        "    # Chain-of-Thought\n",
        "    return x.answer\n",
        "  elif isinstance(x, tuple):\n",
        "    # ReAct: tuple(answer, reasoning)\n",
        "    return x[0]\n",
        "  else:\n",
        "    raise ValueError(f'Unexpected type: {type(x)}')\n",
        "\n",
        "def extract_reasoning_cot_or_react(\n",
        "    x: chain_of_thought.CoTReply | tuple[str, str]) -\u003e str:\n",
        "  if isinstance(x, chain_of_thought.CoTReply):\n",
        "    # Chain-of-Thought\n",
        "    return x.reasoning\n",
        "  elif isinstance(x, tuple):\n",
        "    # ReAct: tuple(answer, reasoning)\n",
        "    return x[1]\n",
        "  else:\n",
        "    raise ValueError(f'Unexpected type: {type(x)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb8telOsDT7P"
      },
      "outputs": [],
      "source": [
        "cot_diverse_strategies_sc = self_consistency.SelfConsistency(\n",
        "    sampler=cot_over_diverse_strategies_sampler,\n",
        "    bucketizer=extract_answer_cot_or_react,\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA0Cw4HADT7P"
      },
      "outputs": [],
      "source": [
        "answer_distribution_div_strat_sc, trace_div_strat_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_diverse_strategies_sc(question)),\n",
        "    enable_tracing=True)\n",
        "\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_div_strat_sc):\n",
        "  reply = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': probability,\n",
        "      'Answer': extract_answer_cot_or_react(reply),\n",
        "      'Representative Reasoning': extract_reasoning_cot_or_react(reply),\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b2ukx2UFksw"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer().render(trace_div_strat_sc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs9_E12dwvUo"
      },
      "source": [
        "## SC with normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pj15IkYR4IE"
      },
      "source": [
        "One of the things you might have noticed in the above answer distributions is that we sometimes have multiple different competing answers that actually mean the same thing, but are just formatted differently (e.g., \"490,000\" vs. \"490000\"). If we want to ensure that these are placed in the same bucket for voting purposes, we can do so by providing a custom `bucketizer` function that performs some appropriate normalization of the answer format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZeemtt1B5sl"
      },
      "outputs": [],
      "source": [
        "def get_numeric_answer(prediction: str) -\u003e str:\n",
        "  \"\"\"Returns a normalized number string, where possible, otherwise the original.\n",
        "\n",
        "  Args:\n",
        "    prediction: The LLM reply to normalize.\n",
        "  \"\"\"\n",
        "  # Remove any trailing periods.\n",
        "  normalized = prediction\n",
        "  while normalized and normalized.endswith('.'):\n",
        "    normalized = normalized[:-1]\n",
        "\n",
        "  # If the prediction is in the form of an equation (e.g., '2 + 3 = 5'), then\n",
        "  # we take just the answer ('5').\n",
        "  normalized = normalized.split('=')[-1]\n",
        "\n",
        "  # Ignore any thousands separators.\n",
        "  normalized = normalized.replace(',', '')\n",
        "\n",
        "  try:\n",
        "    _ = float(normalized)\n",
        "    return normalized\n",
        "  except ValueError:\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BZwVdtIwvU-"
      },
      "outputs": [],
      "source": [
        "get_numeric_answer('490,000.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp6slg8TElq3"
      },
      "outputs": [],
      "source": [
        "cot_sc_normalized = self_consistency.SelfConsistency(\n",
        "    sampler=sampling.Repeated(chain_of_thought.QACoTPromptJ2()),\n",
        "    bucketizer=lambda x: get_numeric_answer(x.answer),\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogquWZWyElq3"
      },
      "outputs": [],
      "source": [
        "answer_distribution_cot_norm_sc, trace_cot_norm_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_sc_normalized(question)),\n",
        "    enable_tracing=True)\n",
        "\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_cot_norm_sc):\n",
        "  cot_reply = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': probability,\n",
        "      'Answer': cot_reply.answer,\n",
        "      'Representative Reasoning': cot_reply.reasoning,\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pnt5mnj5ttX"
      },
      "source": [
        "## SC with weighted votes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dXzAH55StGr"
      },
      "source": [
        "In cases where we have access to some additional signal for judging the plausibility of an answer, we can pass this to the `SelfConsistency` strategy in the form of a custom `scorer` function, so as to weight each vote by its score.\n",
        "\n",
        "This approach is described in [[Li, et al., 2023]](https://arxiv.org/pdf/2206.02336) as a \"voting verifier\".\n",
        "\n",
        "Here we show an example of this approach, where the \"verifier\" is implemented via prompting of the same LLM that was used to generate the response (while eliciting different behavior, and thus hopefully additional signal, by using a materially different prompt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-ksdpxOdZzf"
      },
      "outputs": [],
      "source": [
        "# Type representing a strategy's input.\n",
        "_I = TypeVar('_I')\n",
        "\n",
        "# Type representing a strategy's output.\n",
        "_O = TypeVar('_O')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLAOxpH8GVkP"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class VerifierReply:\n",
        "  \"\"\"Verifier reply.\"\"\"\n",
        "  rating: float | None = None\n",
        "  rating_string: str = ''\n",
        "  reasoning: str = ''\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class VerifierExemplar(Generic[_I, _O]):\n",
        "  \"\"\"Verifier inputs+outputs, suitable for use as a few-shot exemplar.\n",
        "\n",
        "  Attributes:\n",
        "    inputs: The question to answer.\n",
        "    prediction: The predicted answer.\n",
        "    rating: Numeric rating of the plausibility of the predicted answer being\n",
        "      correct, where 0.0 means clearly wrong and 1.0 means very plausible.\n",
        "      Rating of None means not defined (e.g., because the rating_string output\n",
        "      by the LLM could not be parsed).\n",
        "    rating_string: Optional textual representation of the rating\n",
        "      (e.g., '0', '1'). In the case of a critic that is implemented via LLM\n",
        "      prompting, this would represent the rating string that was generated by\n",
        "      the LLM, prior to parsing.\n",
        "    reasoning: The reasoning/justification for the rating.\n",
        "  \"\"\"\n",
        "  inputs: _I\n",
        "  prediction: _O\n",
        "  rating: float | None = None\n",
        "  rating_string: str = ''\n",
        "  reasoning: str = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqXNCAaRGVko"
      },
      "outputs": [],
      "source": [
        "VERIFIER_PROMPT_TEXT = \"\"\"\\\n",
        "{#- Preamble: Instructions -#}\n",
        "Give a numerical rating for plausibility of the proposed answer, ranging from 0.0 (clearly wrong) to 1.0 (very plausible), along with the justification for the rating. When rating the plausibility of the answer, be humble (i.e., don't simply assume that you yourself know the correct answer), but consider factors such as the following:\n",
        "* whether the answer is of the correct type (e.g., the question asked for a place, but they answered with a time)\n",
        "* whether the answer cites its sources, includes the detailed reasoning that led up to the final answer\n",
        "* whether the reasoning is sound\n",
        "* whether there was any sign of system error (e.g., the answer was blank, or included an error message, or repeated the same sentence many times, or got cut off)\n",
        "* whether the answer is of a plausible order of magnitude (in case of a numeric answer)\n",
        "\n",
        "{#- Preamble: Few-shots exemplars -#}\n",
        "{%- for exemplar in exemplars -%}\n",
        "{{ '\\n' }}\n",
        "Question: {{exemplar.inputs}}\n",
        "Proposed answer: {{exemplar.prediction}}\n",
        "Justification for rating: {{exemplar.reasoning}}\n",
        "Rating (0.0 to 1.0): {{exemplar.rating}}\n",
        "{%- endfor -%}\n",
        "\n",
        "{#- Start of the processing of the actual inputs. -#}\n",
        "{{ '\\n' }}\n",
        "Question: {{inputs}}\n",
        "Proposed answer: {{prediction}}\n",
        "Justification for rating: {{store(\"reasoning\", generate_text(stop=[\"\\\\nRating\", \"\\\\n\\\\n\"])) | trim }}\n",
        "Rating (0.0 to 1.0): {{store(\"rating_string\", generate_text(stop=[\"\\\\nQuestion:\", \"\\\\n\\\\n\"], max_tokens=10)) | trim }}\n",
        "\"\"\"\n",
        "\n",
        "VERIFIER_EXEMPLARS = [\n",
        "    VerifierExemplar(\n",
        "        inputs='Circumference of a circle with radius 1cm?',\n",
        "        prediction='6.28 meters',\n",
        "        rating=0.1,\n",
        "        rating_string='0.1',\n",
        "        reasoning='The circumference of a circle should be a just few times bigger than its radius (same order of magnitude). So if the radius is 1cm, the circumference should similarly be just a few centimeters. 6.28 meters means 628 centimeters, which is 2 orders of magnitude too big.',\n",
        "    ),\n",
        "    VerifierExemplar(\n",
        "        inputs='Circumference of a circle with radius 1cm?',\n",
        "        prediction='6 centimeters',\n",
        "        rating=0.9,\n",
        "        rating_string='0.9',\n",
        "        reasoning='The units of the answer are plausible (circumference is a unit of length, and so are centimeters). The order of magnitude also looks about right (the circumference of a circle should be a few times bigger than its radius, so if the radius is 1cm, a circumference of 6cm sounds like the right ballpark).',\n",
        "    ),\n",
        "    VerifierExemplar(\n",
        "        inputs='What is the capital of the France?',\n",
        "        prediction='Paris',\n",
        "        rating=1.0,\n",
        "        rating_string='1.0',\n",
        "        reasoning='The question is asking for city in France, and Paris is a well-known city in France and indeed commonly known to be the capital of France.',\n",
        "    ),\n",
        "    VerifierExemplar(\n",
        "        inputs='What major league baseball player scored the most home runs in the year 2010?',\n",
        "        prediction='Mickey Mouse',\n",
        "        rating=0.5,\n",
        "        rating_string='0.5',\n",
        "        reasoning=\"The question asked for a real person, but the answer Mickey Mouse sounds like it is referring to a cartoon character, which would be wrong. There is a chance, however, that there could be a person named Mickey Mouse that I don't know about, as I am not an expert on major league baseball players.\",\n",
        "    ),\n",
        "    VerifierExemplar(\n",
        "        inputs='Spell first 5 digits of pi.',\n",
        "        prediction='SyntaxError',\n",
        "        rating=0.0,\n",
        "        rating_string='0.0',\n",
        "        reasoning='The question asked for a series of digits, but the answer suggests that some kind of error occurred while processing the question.',\n",
        "    ),\n",
        "]\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class VerifierPromptJ2(Generic[_O]):\n",
        "  \"\"\"Basic verifier based on a single Jinja2 prompt template.\n",
        "\n",
        "  Attributes:\n",
        "    prompt_text: Text of prompt template in Jinja2 format. Available input\n",
        "      variables are `args`, `kwargs`, `target`, `prediction`, and `exemplars`.\n",
        "      Should populate output variables `rating_yes_no` and `reasoning`.\n",
        "    exemplars: Default exemplars to use, if question-specific exemplars are not\n",
        "      provided.\n",
        "  \"\"\"\n",
        "  prompt_text: str = VERIFIER_PROMPT_TEXT\n",
        "  exemplars: Sequence[VerifierExemplar[_I, _O]] = tuple()\n",
        "\n",
        "  @ot.make_executable(copy_self=False)\n",
        "  @tracing.trace(name=utils.FROM_INSTANCE_CLASS_NAME)\n",
        "  async def __call__(\n",
        "      self,\n",
        "      inputs: _I,\n",
        "      prediction: _O,\n",
        "      exemplars: Sequence[VerifierExemplar[_I, _O]] = tuple(),\n",
        "  ) -\u003e VerifierReply:\n",
        "    \"\"\"Returns a rating of the plausibility of the prediction, with reasoning.\n",
        "\n",
        "    Args:\n",
        "      inputs: The question.\n",
        "      prediction: The predicted answer.\n",
        "      exemplars: Optional few-shot exemplars to be used for this specific\n",
        "        question.\n",
        "    \"\"\"\n",
        "    if not exemplars:\n",
        "      exemplars = self.exemplars\n",
        "    prompt_template = composables.j(self.prompt_text)\n",
        "    _ = await prompt_template(\n",
        "        inputs=inputs, prediction=prediction, exemplars=exemplars)\n",
        "    rating_string=prompt_template['rating_string']\n",
        "    try:\n",
        "      rating = float(rating_string)\n",
        "    except ValueError:\n",
        "      rating = None\n",
        "    return VerifierReply(\n",
        "        rating=rating,\n",
        "        rating_string=rating_string,\n",
        "        reasoning=prompt_template['reasoning'],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmXmJH-pDzYs"
      },
      "outputs": [],
      "source": [
        "@tracing.trace\n",
        "async def cot_verifier(\n",
        "    args: Sequence[Any],\n",
        "    kwargs: Mapping[str, Any],\n",
        "    output: chain_of_thought.CoTReply) -\u003e float:\n",
        "  \"\"\"Returns a rating of the plausibility of the CoT reply for the question.\"\"\"\n",
        "  if len(args) == 1 and not kwargs:\n",
        "    # Question provided as positional arg.\n",
        "    question = args[0]\n",
        "  elif not args and len(kwargs) == 1:\n",
        "    # Question provided as keyword arg.\n",
        "    question = list(kwargs.values())[0]\n",
        "  else:\n",
        "    # Multiple args/kwargs provided -- squeeze them into a string to use as the\n",
        "    # \"question\".\n",
        "    question = repr({'args': args, 'kwargs': kwargs})\n",
        "  critic = VerifierPromptJ2(exemplars=VERIFIER_EXEMPLARS)\n",
        "  prediction = (\n",
        "      f'{output.reasoning.strip()} The answer is {output.answer}.')\n",
        "  verifier_reply = await critic(inputs=question, prediction=prediction)\n",
        "  return verifier_reply.rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na4jixuE7Wws"
      },
      "outputs": [],
      "source": [
        "# Question provided as positional arg.\n",
        "verifier_result, verifier_trace = ot.run(cot_verifier(\n",
        "    args=('What is the total population of Tuebingen and Zuerich?',),\n",
        "    kwargs={},\n",
        "    output=chain_of_thought.CoTReply(\n",
        "        answer='490,000',\n",
        "        reasoning= '\\nPopulation of Tuebingen = 90,000\\nPopulation of Zuerich = 400,000\\nTotal population = 90,000 + 400,000 = 490,000')\n",
        "), enable_tracing=True)\n",
        "verifier_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePZSVdoUX0N7"
      },
      "outputs": [],
      "source": [
        "# Question provided as keyword arg.\n",
        "verifier_result, verifier_trace = ot.run(cot_verifier(\n",
        "    args=tuple(),\n",
        "    kwargs={'question': 'What is the total population of Tuebingen and Zuerich?'},\n",
        "    output=chain_of_thought.CoTReply(\n",
        "        answer='490,000',\n",
        "        reasoning= '\\nPopulation of Tuebingen = 90,000\\nPopulation of Zuerich = 400,000\\nTotal population = 90,000 + 400,000 = 490,000')\n",
        "), enable_tracing=True)\n",
        "verifier_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7ebXDvb79q9"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(verifier_trace))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5TtWkyhGcTJ"
      },
      "outputs": [],
      "source": [
        "cot_sc_score_weighted = self_consistency.SelfConsistency(\n",
        "    sampler=sampling.Repeated(chain_of_thought.QACoTPromptJ2()),\n",
        "    bucketizer=lambda x: get_numeric_answer(x.answer),\n",
        "    scorer=cot_verifier,\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUk9dy2e5ttY"
      },
      "outputs": [],
      "source": [
        "answer_distribution_cot_score_sc, trace_cot_score_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_sc_score_weighted(question)),\n",
        "    enable_tracing=True)\n",
        "\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_cot_score_sc):\n",
        "  cot_reply = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': probability,\n",
        "      'Answer': cot_reply.answer,\n",
        "      'Representative Reasoning': cot_reply.reasoning,\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5va7d6vQIJQw"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer().render(trace_cot_score_sc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT13bumQXDav"
      },
      "source": [
        "## SC with consensus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_aJAXzxUq5q"
      },
      "source": [
        "As described in the overview section, if we simply we want to use a self-consistency-enhanced strategy as a drop-in replacement for its underlying strategy, we can do so by wrapping it in `ExtractConsensus`, so as to return just the single consensus answer (i.e., highest probability answer), rather than the full answer distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWNQCOdjVpeM"
      },
      "outputs": [],
      "source": [
        "cot_sc_concensus = self_consistency.ExtractConsensus(cot_sc)\n",
        "cot_sc_consensus_reply, trace_cot_sc_consensus = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_sc_concensus(question)),\n",
        "    enable_tracing=True)\n",
        "cot_sc_consensus_reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKVR0glaWm2K"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer().render(trace_cot_sc_consensus))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj0l4UOf8sGb"
      },
      "source": [
        "## Eval with distribution metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjdGv_5UVQez"
      },
      "source": [
        "Another approach to evaluation, however, is to use the `SelfConsistency` strategy as-is, while providing an evaluation metric that takes an answer distribution as its \"prediction\".\n",
        "\n",
        "An advantage of this approach is that it allows outputting of metrics like \"accuracy@k\" (i.e., the fraction of time that the correct answer is among the top K candidates), which would not be possible to calculate based on the consensus answer alone.\n",
        "\n",
        "Note that \"accuracy@k\" fully subsumes its underlying ordinary accuracy metric, which simply corresponds to the case of `k=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VONGeBBS8YgS"
      },
      "outputs": [],
      "source": [
        "cot_sc = self_consistency.SelfConsistency(\n",
        "    sampler=sampling.Repeated(chain_of_thought.QACoTPromptJ2()),\n",
        "    bucketizer=lambda x: x.answer,  # Use the final answer as the bucket.\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UBp7IMX8Rv-"
      },
      "outputs": [],
      "source": [
        "# We define a custom accuracy metric, since CoT predictions are represented as\n",
        "# ChainOfThoughtReply objects (containing 'answer' and 'reasoning'), whereas the\n",
        "# target is simply the final answer.\n",
        "cot_exact_match = lambda t, p: 1.0 * (str(t) == p.answer)\n",
        "cot_answer_included = lambda t, p: 1.0 * (str(t) in p.answer)\n",
        "\n",
        "AccuracyAtK = distribution_metrics.AccuracyAtK\n",
        "\n",
        "with ot.RegistryContext():\n",
        "  llm.generate_text.update(temperature=0.7)\n",
        "  cot_sc_summary = agent_evaluation.evaluate(\n",
        "      strategy=cot_sc,\n",
        "      examples=dataset,\n",
        "      metric_functions={\n",
        "          'exact_match': AccuracyAtK(k=1, base_metric=cot_exact_match),\n",
        "          'exact_match@3': AccuracyAtK(k=3, base_metric=cot_exact_match),\n",
        "          'answer_included': AccuracyAtK(k=1, base_metric=cot_answer_included),\n",
        "          'answer_included@3': AccuracyAtK(k=3, base_metric=cot_answer_included),\n",
        "      },\n",
        "      output_results=True,\n",
        "      output_results_debug=True,\n",
        "      output_final_states=True,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPa4WphCANCS"
      },
      "outputs": [],
      "source": [
        "# Here are the metric values aggregated over the whole dataset.\n",
        "for metric, value in cot_sc_summary.metrics.items():\n",
        "  print(f'{metric}: {value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyWagmbNBu08"
      },
      "outputs": [],
      "source": [
        "# Here is a table showing the results broken down by question.\n",
        "eval_summary_data = []\n",
        "for i, result in cot_sc_summary.results.items():\n",
        "  predicted_distribution = result.outputs['output']\n",
        "  eval_summary_data.append({\n",
        "      'Question': result.inputs['args'][0],\n",
        "      'Target': result.targets['target'],\n",
        "      'EM@1': result.metrics['exact_match'],\n",
        "      'EM@3': result.metrics['exact_match@3'],\n",
        "      'Answer Included@1': result.metrics['answer_included'],\n",
        "      'Answer Included@3': result.metrics['answer_included@3'],\n",
        "      'Answer 1': predicted_distribution[0][0].answer if len(predicted_distribution) \u003e= 1 else '--',\n",
        "      'Answer 2': predicted_distribution[1][0].answer if len(predicted_distribution) \u003e= 2 else '--',\n",
        "      'Answer 3': predicted_distribution[2][0].answer if len(predicted_distribution) \u003e= 3 else '--',\n",
        "  })\n",
        "eval_summary_data_df = pd.DataFrame(eval_summary_data)\n",
        "eval_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2Yd6fc3w1yE"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(cot_sc_summary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HWP6DhTw7Rc"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT052IG0ckbI"
      },
      "source": [
        "## Appendix A: Other Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFbbBl9Pcs3k"
      },
      "source": [
        "As of the initial release, we support connections to the following models (more will be added soon):\n",
        "- Gemini API\n",
        "- OpenAI API\n",
        "- Gemma running locally\n",
        "- Gemma running on a OneTwo model server\n",
        "\n",
        "In the Overview section, we illustrated how to connect to the Gemini and OpenAI APIs. Below, we illustrate how to setup and connect to Gemma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sesaZZtsglUb"
      },
      "source": [
        "### Gemma (running locally)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOxjsEVxx8FR"
      },
      "source": [
        "The [Gemma](https://ai.google.dev/gemma) family of open weights models ([GitHub](https://github.com/google-deepmind/gemma)) can be obtained from the following repositories:\n",
        "- Vertex Model Garden: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335\n",
        "- Kaggle: https://www.kaggle.com/models/google/gemma/\n",
        "- HuggingFace: https://huggingface.co/docs/transformers/en/model_doc/gemma\n",
        "\n",
        "As an example, below are the instructions for downloading a Gemma model from Kaggle. If you have not used Kaggle before, you will need to first create a Kaggle account and API key (a.k.a. API token) following the instructions on https://www.kaggle.com/docs/api. Then copy-paste your username and API key into the placeholders below.\n",
        "\n",
        "If you have not used Gemma on Kaggle before, you will also need to go to https://www.kaggle.com/models/google/gemma/ and click \"Request access\" to complete the consent form before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptIrpyV3_t7R"
      },
      "outputs": [],
      "source": [
        "# If you want to run this section, just change the below to `True`.\n",
        "# (We keep it disabled by default due to the kaggle dependency.)\n",
        "enable_gemma_local = False  # @param [\"True\", \"False\"] {type:\"raw\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLfR3pFzhDMN"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "# Install kaggle\n",
        "! pip install kaggle\n",
        "! pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWepgri9gptd"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "import kagglehub\n",
        "from kagglehub import auth\n",
        "\n",
        "auth.set_kaggle_credentials(\n",
        "    username='YOUR_KAGGLE_USERNAME', api_key='YOUR_KAGGLE_API_KEY'\n",
        ")\n",
        "kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHylR7QshOHB"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "VARIANT = '2b-it'  # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\n",
        "weights_dir = kagglehub.model_download(f'google/gemma/Flax/{VARIANT}')\n",
        "\n",
        "checkpoint_path = os.path.join(weights_dir, VARIANT)\n",
        "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD7WNFp_yzFb"
      },
      "source": [
        "Once you have downloaded a copy of the model, you can create a OneTwo backend using this model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vwlxrmDfNc8"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "from onetwo.backends import gemma_local\n",
        "\n",
        "backend = gemma_local.Gemma(\n",
        "    checkpoint_path=checkpoint_path, vocab_path=vocab_path\n",
        ")\n",
        "backend.register()\n",
        "print('Gemma local backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv8giCScg8k5"
      },
      "source": [
        "### Gemma (running on a separate server)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QvJLDTIzAcJ"
      },
      "source": [
        "While experimenting, you may not want to have to reload the model every time you change something in your code, so it can be convenient to set up the Gemma model to run in a separate process or even on a separate machine.\n",
        "\n",
        "We provide a model server script for that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzw2TkcIhja9"
      },
      "source": [
        "#### Server setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLYm4DaItG1J"
      },
      "source": [
        "In order to set up a server, you can use the `run_model_server.py` script. For example you can create a server that serves a Gemma model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3w18fFWhxSy"
      },
      "source": [
        "```shell\n",
        "CHECKPOINT_PATH=\"PATH_TO_THE_CHECKPOINT_DIR\"\n",
        "VOCAB_PATH=\"PATH_TO_THE_VOCAB_FILE\"\n",
        "\n",
        "python run_model_server.py \\\n",
        "  --backend_module=\"onetwo.backends.gemma_local\" \\\n",
        "  --backend_class=\"Gemma\" \\\n",
        "  --backend_params=\"{\\\"checkpoint_path\\\": \\\"$CHECKPOINT_PATH\\\", \\\"vocab_path\\\": \\\"$VOCAB_PATH\\\"}\" \\\n",
        "  --port=8888\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12trBlN7tUIb"
      },
      "source": [
        "You can also simply create your own python code to load a model locally or connect to a remote model and run as a simple web server using code that looks like:\n",
        "\n",
        "```python\n",
        "import uvicorn\n",
        "\n",
        "def main(args):\n",
        "  # Code to create the backend connection (local or remote).\n",
        "  backend = ...\n",
        "  backend.register()\n",
        "\n",
        "  # Starting a simple server piping requests to the registered backend.\n",
        "  uvicorn.run(\n",
        "      'onetwo.backends.model_server:ModelServer',\n",
        "      host='0.0.0.0',\n",
        "      port=8888,\n",
        "      factory=True,\n",
        "  )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m26cftOxhlYs"
      },
      "source": [
        "#### Client connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9EOqVN6uGiC"
      },
      "source": [
        "Once your server is set up, on the client side, you can connect to it using the onetwo_api module.\n",
        "\n",
        "If you have set up a server as described above, you can uncomment the code below and copy-paste your server hostname into the placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tedsV2vmhnzP"
      },
      "outputs": [],
      "source": [
        "# from onetwo.backends import onetwo_api\n",
        "\n",
        "# backend = onetwo_api.OneTwoAPI(endpoint='http://SERVER_HOST_NAME:8888')\n",
        "# backend.register()\n",
        "# print('Gemma server backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-a8NwgirOKy"
      },
      "source": [
        "## Appendix B: Diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMZEdM1I0YwA"
      },
      "outputs": [],
      "source": [
        "def print_cache_summary():\n",
        "  print('Cache summary:')\n",
        "  for model_name, backend in backends.items():\n",
        "    print(f'* {model_name}: {backend.cache_filename}')\n",
        "    cache_length = len(backend._cache_handler._cache_data.values_by_key)\n",
        "    print(f'  * Cache contains {cache_length} items.')\n",
        "    print(f'  * Counters: {backend._cache_handler._cache_data.counters}')\n",
        "    num_calls_in_progress = len(backend._cache_handler._calls_in_progress)\n",
        "    print(f'  * Calls in progress: {num_calls_in_progress}')\n",
        "\n",
        "def clear_all_calls_in_progress():\n",
        "  for model_name, backend in backends.items():\n",
        "    if backend._cache_handler._calls_in_progress:\n",
        "      print(f'Clearing calls in progress for {model_name}.')\n",
        "      print(f'BEFORE: {len(backend._cache_handler._calls_in_progress)}')\n",
        "      backend._cache_handler._calls_in_progress.clear()\n",
        "      print(f'AFTER: {len(backend._cache_handler._calls_in_progress)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZbBK-TQDhlN"
      },
      "outputs": [],
      "source": [
        "print_cache_summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xVyyAgrE5m8-"
      ],
      "last_runtime": {
        "build_target": "//learning/brain/research/system12/onetwo/colabs:onetwo_colab",
        "kind": "private"
      },
      "name": "OneTwo - Tutorial",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
