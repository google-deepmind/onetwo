{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvx4Xxkk5nIH"
      },
      "source": [
        "Copyright 2024 DeepMind Technologies Limited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sx1DwXpPEWeL"
      },
      "outputs": [],
      "source": [
        "#@title SPDX-License-Identifier: Apache-2.0\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use\n",
        "# this file except in compliance with the License. You may obtain a copy of the\n",
        "# License at http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software distributed\n",
        "# under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\n",
        "# CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
        "# specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SytPbrqtkUF"
      },
      "source": [
        "![onetwo_logo_horizontal.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ7N9Pmt9kvm"
      },
      "source": [
        "This colab illustrates how to use the [OneTwo](https://github.com/google-deepmind/onetwo) library.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNneQ1X8c_yb"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF_Z92hr2fhG"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWpY81IcArpB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A6jm6BDe8g_"
      },
      "outputs": [],
      "source": [
        "# Uncomment the following line and execute the cell to install OneTwo (if not already installed).\n",
        "# !pip install git+https://github.com/google-deepmind/onetwo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAJGlUJLNwT0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGi9bnZRNzg5"
      },
      "outputs": [],
      "source": [
        "# At minimum, these are the libraries you will need to import to perform basic\n",
        "# operations with OneTwo.\n",
        "from onetwo import ot\n",
        "from onetwo.builtins import llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsvl1UcXECJb"
      },
      "outputs": [],
      "source": [
        "# These are some additional libraries that we will use in certain sections of\n",
        "# the colab.\n",
        "from collections.abc import Mapping, Sequence\n",
        "import copy\n",
        "import dataclasses\n",
        "import datetime\n",
        "import io\n",
        "import functools\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import re\n",
        "import requests\n",
        "import textwrap\n",
        "import typing\n",
        "from typing import Any, Generic, TypeVar\n",
        "\n",
        "from onetwo.agents import python_planning\n",
        "from onetwo.agents import react\n",
        "from onetwo.builtins import composables\n",
        "from onetwo.builtins import formatting\n",
        "from onetwo.core import content as content_lib\n",
        "from onetwo.core import sampling\n",
        "from onetwo.core import tracing\n",
        "from onetwo.core import utils\n",
        "from onetwo.evaluation import agent_evaluation\n",
        "from onetwo.stdlib.code_execution import python_execution_safe_subset\n",
        "from onetwo.stdlib.ensembling import distribution_metrics\n",
        "from onetwo.stdlib.ensembling import self_consistency\n",
        "from onetwo.stdlib.reasoning import chain_of_thought\n",
        "from onetwo.stdlib.tool_use import llm_tool_use\n",
        "from onetwo.stdlib.tool_use import python_tool_use\n",
        "from onetwo.utils import colab_utils\n",
        "\n",
        "from PIL import Image\n",
        "import pydantic\n",
        "\n",
        "\n",
        "\n",
        "BaseModel = pydantic.BaseModel\n",
        "pydantic_dataclass = pydantic.dataclasses.dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAD4cH_68pvD"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google.colab import userdata\n",
        "  running_in_colab = True\n",
        "  print(\"Running in Google Colab.\")\n",
        "except ImportError:\n",
        "  running_in_colab = False\n",
        "  print(\"Running in Jupyter notebook.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh9KmZ30ZlvG"
      },
      "outputs": [],
      "source": [
        "# Confirm the currently installed OneTwo version.\n",
        "print(f'Running OneTwo v{ot.__version__}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv6vzl2rZciy"
      },
      "source": [
        "# Overview\n",
        "\n",
        "One of the key principles behind the OneTwo library is to enable the creation of complex flows involving several calls to foundation models and possibly other tools.\n",
        "\n",
        "We facilitate through functionality at two levels:\n",
        "* **OneTwo Core:** Low-level libraries for connecting to models, prompting them, and constructing programs that chain multiple calls to such models, while leaving you full flexibility in the structuring of your program (just like if you were programming in pure Python).\n",
        "* **OneTwo Standard Library:** A set of reusable components built on top of the OneTwo Core, providing generic implementations of a wide range of SOTA prompting strategies from Google DeepMind, Google Research, and published research from outside of Google.\n",
        "\n",
        "In this overview, we provide a quick introduction to both the OneTwo core and the OneTwo standard library, with the intention that with the overview alone, you should already be up and running with OneTwo in less than an hour. Feel free to then continue on with the advanced sections, or come back to them later!\n",
        "\n",
        "**OneTwo Core**\n",
        "\n",
        "For ease of experimentation, it is important to easily change the backends or their configuration and run the same flow on two backends/configurations, e.g. when doing comparisons.\n",
        "\n",
        "The bottleneck is often the multiple RPC requests that need to happen. This makes fast iterations or experimenting on many examples slow and tedious. In order to reduce this bottleneck, there are two strategies that are implemented in the OneTwo library:\n",
        "1. **Caching**: The result of the calls to the models are cached, which enables one to very quickly replay a flow or an experiment which may have partially executed (e.g. failed in the middle of execution). For example, if you have a complex flow and want to add just one extra step, rerunning the whole thing amounts to reading everything from cache and only executing for real that one last step.\n",
        "1. **Asynchronous Execution**: While some of the model calls might need to be chained serially, there are many situations when you may want to execute some calls in parallel (e.g. talking to different backends, running an experiment on many examples, or having a step in your flow where several independent tasks are performed). A natural way to do that is to use asynchronous programming, or multi-threading.\n",
        "\n",
        "In the first several sub-sections of the overview, we provide an introduction that illustrates how to connect to and prompt a model using the basic OneTwo functionality.\n",
        "\n",
        "**OneTwo Standard Library:**\n",
        "\n",
        "In the later sub-sections of the overview, we introduce some of the most widely used components from the OneTwo standard library, including bread-and-butter techniques such as chain-of-thought and self-consistency, as well as more complex agent and tool use strategies.\n",
        "\n",
        "More details on the OneTwo standard library and other advanced features are provided in additional sections after the overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6soYEHIjZci_"
      },
      "source": [
        "## Connecting to a Model\n",
        "\n",
        "The Gemini and OpenAI APIs provide an easy way to connect to a model. Select your preferred API/model in the drop-down and enter the corresponding API key (where relevant) in the text box below.\n",
        "\n",
        "OneTwo also supports connecting to other models such as Gemma models running locally or on a server. See Appendix A for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0HS107hVx3-"
      },
      "source": [
        "### Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akiM6TkoZu5p"
      },
      "outputs": [],
      "source": [
        "# @title {run:\"auto\"}\n",
        "\n",
        "# Select one of the LLM backend options here\n",
        "model_selection = 'Gemini API: gemini-2.5-flash' # @param [ 'Gemini API: gemini-2.5-flash', 'Gemini API: gemini-2.5-pro', 'OpenAI API: gpt-4o-mini', 'OpenAI API: gpt-4o', 'OpenAI API: gpt-3.5-turbo']\n",
        "\n",
        "# Boilerplate for conditional cell execution\n",
        "@IPython.core.magic.register_cell_magic('run_if')\n",
        "def run_if(line, cell):\n",
        "  if eval(line):\n",
        "     IPython.get_ipython().run_cell(cell)\n",
        "  else:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYDn9ZWPzZo4"
      },
      "outputs": [],
      "source": [
        "model_supports_multimodal = (\n",
        "    model_selection.startswith('Gemini API') and (\n",
        "        not model_selection.endswith('gemini-1.0-pro')\n",
        "    )\n",
        ")\n",
        "print(f'model_supports_multimodal: {model_supports_multimodal}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvQeJhkNjALk"
      },
      "outputs": [],
      "source": [
        "model_supports_embedding = model_selection.startswith('Gemini API')\n",
        "print(f'model_supports_embedding: {model_supports_embedding}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_supports_generate_object = model_selection.startswith('Gemini API')\n",
        "print(f'model_supports_generate_object: {model_supports_generate_object}')"
      ],
      "metadata": {
        "id": "o5jAH00KKZ0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJbwvRDfk1JH"
      },
      "cell_type": "code",
      "source": [
        "# For the purposes of this colab, we are disabling thinking by default (where\n",
        "# possible), as it often leads to unintuitive behavior when combined with the\n",
        "# use of the `max_tokens` parameter (i.e., `max_output_tokens` in the\n",
        "# GoogleGenAI API). Specifically, the subtlety is that it gets applied to the\n",
        "# sum of the thinking tokens and actual output tokens, which can often lead to\n",
        "# empty outputs when `max_tokens` is small and thinking is enabled.\n",
        "# For some models, however, it is not possible to disable thinking entirely, in\n",
        "# which case we configure here the smallest allowable thinking budget.\n",
        "thinking_budget = 0\n",
        "if model_selection.startswith('Gemini API'):\n",
        "  # Documentation of allowable thinking budget range for each model:\n",
        "  # https://cloud.google.com/vertex-ai/generative-ai/docs/thinking\n",
        "  # https://ai.google.dev/gemini-api/docs/thinking\n",
        "  if model_selection.endswith('gemini-2.5-pro'):\n",
        "    thinking_budget = 128"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQL3iCpBEU7v"
      },
      "source": [
        "### Caching Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHrn17vNxuzp"
      },
      "outputs": [],
      "source": [
        "# Here we define a location in which to store a cache of requests/replies for\n",
        "# each backend of interest, which we can use for speeding up running of the\n",
        "# colab if we re-run it (or make iterative modifications to it) in the future.\n",
        "# We will use a separate cache file for each backend.\n",
        "OWN_CACHE_DIRECTORY = '/tmp/onetwo_colab_backend_caches/tutorial'\n",
        "\n",
        "# If you would like to share cache files with others in your working group, you\n",
        "# can optionally specify another shared cache directory here. If you specify\n",
        "# this, then we will read from shared cache directory and give precedence to its\n",
        "# contents, while merging in any additional content from OWN_CACHE_DIRECTORY.\n",
        "# When saving the cache, however, we will by default write only to\n",
        "# OWN_CACHE_DIRECTORY to reduce the chance of people clobbering each other's\n",
        "# changes.\n",
        "SHARED_CACHE_DIRECTORY = None\n",
        "\n",
        "# If you want to automatically merge in content from any of your teammates'\n",
        "# cache directories or from a cache that was output by a batch eval run,\n",
        "# you can list the additional directories here.\n",
        "ADDITIONAL_CACHE_DIRECTORIES = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUKMriyEpX9n"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoVm4MZzrfMe"
      },
      "outputs": [],
      "source": [
        "# We encapsulate the logic for loading/saving the caches for a set of backends\n",
        "# in the `CachedBackends` class. Each time we construct a backend, we will add\n",
        "# it to the `cached_backends` mapping so that we can load and save them all at\n",
        "# once.\n",
        "cached_backends = colab_utils.CachedBackends(\n",
        "    own_cache_directory=OWN_CACHE_DIRECTORY,\n",
        "    shared_cache_directory=SHARED_CACHE_DIRECTORY,\n",
        "    additional_cache_directories=ADDITIONAL_CACHE_DIRECTORIES,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-uL_yLBZf2t"
      },
      "source": [
        "### Gemini API\n",
        "\n",
        "OneTwo can connect to publicly-hosted Gemini models via the Gemini API. If you have not used the Gemini API before, you will need to first create an account and API key following the instructions on https://ai.google.dev/. Then either copy-paste your API key into the text box, or store it in the 'GOOGLE_API_KEY' environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbNLcbPvZci_"
      },
      "outputs": [],
      "source": [
        "%%run_if model_selection.startswith('Gemini API')  # Execute cell only for 'Gemini API: *'\n",
        "\n",
        "from onetwo.backends import google_genai_api\n",
        "from google.genai import types as genai_types\n",
        "from onetwo.core import caching\n",
        "\n",
        "# You can specify your API key either here or as an environment variable.\n",
        "google_api_key = \"\"  # @param {type: 'string'}\n",
        "\n",
        "if google_api_key:\n",
        "  print('Using API key specified in the colab.')\n",
        "\n",
        "if not google_api_key and running_in_colab:\n",
        "  try:\n",
        "    google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "    print('Loaded GOOGLE_API_KEY from user secrets.')\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "if not google_api_key and 'GOOGLE_API_KEY' in os.environ:\n",
        "  google_api_key = os.environ['GOOGLE_API_KEY']\n",
        "  print('Loaded GOOGLE_API_KEY from environment.')\n",
        "\n",
        "if not google_api_key:\n",
        "  raise ValueError(\n",
        "      'The GOOGLE_API_KEY must be specified either here or in a user secret or '\n",
        "      'in the environment.')\n",
        "\n",
        "# E.g., 'Gemini API: gemini-1.0-pro' ==\u003e 'gemini-1.0-pro'.\n",
        "model_name = model_selection.split(':')[-1].strip()\n",
        "\n",
        "\n",
        "# We are disabling safety settings for the purposes of this colab to\n",
        "# avoid the chance of getting empty responses in the case of\n",
        "# over-triggering of the safety controls. You can set this to an\n",
        "# appropriate setting for your use case.\n",
        "safety_settings=google_genai_api.SAFETY_DISABLED\n",
        "\n",
        "# For thinking models, you can control the amount of thinking budget\n",
        "# using the `ThinkingConfig` below. For the purposes of this colab,\n",
        "# we are disabling thinking by default, as it tends to not play well\n",
        "# with the use of the `max_tokens` parameter (i.e.,\n",
        "# `max_output_tokens` in the GoogleGenAI API), due to it getting\n",
        "# applied to the sum of the thinking tokens and actual output\n",
        "# tokens, which can often lead to empty outputs.\n",
        "thinking_config=genai_types.ThinkingConfig(\n",
        "    include_thoughts=False,\n",
        "    thinking_budget=thinking_budget,\n",
        ")\n",
        "\n",
        "backend = google_genai_api.GoogleGenAIAPI(\n",
        "    vertexai=False,\n",
        "    api_key=google_api_key,\n",
        "    temperature=0.0,\n",
        "    generate_model_name=f'models/{model_name}',\n",
        "    chat_model_name=f'models/{model_name}',\n",
        "    threadpool_size=100,\n",
        "    max_retries=3,\n",
        "    cache=caching.SimpleFunctionCache(\n",
        "      cache_filename=cached_backends.get_cache_path(model_name),\n",
        "    ),\n",
        "    generate_text_kwargs={\n",
        "        'safety_settings': safety_settings,\n",
        "        'thinking_config': genai_types.ThinkingConfig(\n",
        "            include_thoughts=False,\n",
        "            thinking_budget=thinking_budget,\n",
        "            ),\n",
        "        },\n",
        "    generate_object_kwargs={\n",
        "        'safety_settings': safety_settings,\n",
        "        'thinking_config': genai_types.ThinkingConfig(\n",
        "            include_thoughts=False,\n",
        "            thinking_budget=thinking_budget,\n",
        "            ),\n",
        "        },\n",
        "    chat_kwargs={\n",
        "        'safety_settings': safety_settings,\n",
        "        'thinking_config': genai_types.ThinkingConfig(\n",
        "            include_thoughts=False,\n",
        "            thinking_budget=thinking_budget,\n",
        "            ),\n",
        "        },\n",
        ")\n",
        "\n",
        "cached_backends[model_name] = backend\n",
        "backend.register()\n",
        "\n",
        "print(f'Gemini API backend registered: {model_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrpCvVxJZi9k"
      },
      "source": [
        "### OpenAI API\n",
        "\n",
        "OneTwo can connect to OpenAI models via the OpenAI API. If you have not used the OpenAI API before, you will need to first create an account and API key at https://platform.openai.com/signup. Then copy-paste your API key into the text box below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6o2jzmjHXIA"
      },
      "outputs": [],
      "source": [
        "%%run_if model_selection.startswith('OpenAI API')  # Execute cell only for 'OpenAI API: *'\n",
        "\n",
        "from onetwo.backends import openai_api\n",
        "\n",
        "# You can specify your API key either here or as an environment variable.\n",
        "openai_api_key = \"\"  # @param {type: 'string'}\n",
        "\n",
        "if openai_api_key:\n",
        "  print('Using API key specified in the colab.')\n",
        "\n",
        "if not openai_api_key and running_in_colab:\n",
        "  try:\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    print('Loaded OPENAI_API_KEY from user secrets.')\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "if not openai_api_key and 'OPENAI_API_KEY' in os.environ:\n",
        "  openai_api_key = os.environ['OPENAI_API_KEY']\n",
        "  print('Loaded OPENAI_API_KEY from environment.')\n",
        "\n",
        "if not openai_api_key:\n",
        "  raise ValueError(\n",
        "      'The OPENAI_API_KEY must be specified either here or in a user secret or '\n",
        "      'in the environment.')\n",
        "\n",
        "# E.g., 'OpenAI API: gpt-3.5-turbo' ==\u003e 'gpt-3.5-turbo'.\n",
        "model_name = model_selection.split(':')[-1].strip()\n",
        "\n",
        "# Create and register a connection to the OpenAI backend.\n",
        "backend = openai_api.OpenAIAPI(\n",
        "    api_key=openai_api_key,\n",
        "    model_name=model_name,\n",
        "    temperature=0.0,\n",
        "    max_retries=12,\n",
        "    cache_filename=cached_backends.get_cache_path(model_name),\n",
        ")\n",
        "cached_backends[model_name] = backend\n",
        "backend.register()\n",
        "print('OpenAI API backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNGj_MPN4emn"
      },
      "source": [
        "### Saving and Loading Caches\n",
        "\n",
        "When creating a backend, one can associate a cache file to it, which allows one to store and retrieve previously executed requests. Note that when we created the LLM backend connections above, we already specified a location for loading/saving the cache, using the parameter `cache_filename`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzkK93qG3zPb"
      },
      "source": [
        "**Loading cache:**\n",
        "\n",
        "The following initializes the cache if the file exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd6DOLFS3zPm"
      },
      "outputs": [],
      "source": [
        "cached_backends.load_caches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_m9azV_37lF"
      },
      "source": [
        "**Saving own cache:**\n",
        "\n",
        "As you perform operations on the backend in colab (including eval sweeps, etc.), the in-memory version of the cache will be continually updated. To write the in-memory caches back to file, you can do so at any time by temporarily uncommenting the following line and then manually running it.\n",
        "\n",
        "Since we included your username as part of the cache directory, you can save to your own cache directory at any time without fear of clobbering any one else's work. (If you have multiple instances of this colab open simultaneously, though, then you will still need to be careful to avoid clobbering yourself, as any time you save the cache from one instance of the colab, it will overwrite whatever was there before.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1fjZ9jf37lG"
      },
      "outputs": [],
      "source": [
        "# cached_backends.save_caches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyyWN3yo4EyO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsofAL74Zci_"
      },
      "source": [
        "## LLM Built-ins and Executables\n",
        "\n",
        "OneTwo provides a number of built-in functions representing the basic operations one may want to perform on an LLM.\n",
        "\n",
        "- `llm.generate_text()` - Generate raw text.\n",
        "- `llm.instruct()` - Generate answer to instructions.\n",
        "- `llm.chat()` - Generate text in a multi-turn dialogue.\n",
        "- `llm.embed()` - Calculates embeddings for the given content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGtgda_w0nGd"
      },
      "source": [
        "### `llm.generate_text()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaCIt_uzDQaw"
      },
      "source": [
        "The function `llm.generate_text()` takes a piece of text or multimodal content sequence and asks the model to output text that would be a likely continuation of that sequence. It is mostly commonly used with base models (i.e., models that have not been fine-tuned for instruction following or chat semantics), as sequence completion is the primary task on which these models were trained. Note that for instruction-tuned models, however, it is generally recommended to use the `llm.chat()` or `llm.instruct()` functions instead (see subsequent sections),  as depending on the details of how those models are served, they may be hard-wired to interpret all inputs as instructions or chat messages.\n",
        "\n",
        "If you execute the following code against a base model, the response will likely begin with a list of three not-so-well-known French cities, since this is the most natural completion of the given text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VXQvK2YZci_"
      },
      "outputs": [],
      "source": [
        "# Ask the model to continue the given text.\n",
        "e = llm.generate_text(\n",
        "    'Three not so well known cities in France are',\n",
        "    max_tokens=20,\n",
        ")\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl_Ct1J_AUi5"
      },
      "source": [
        "Note that the LLM built-ins do not directly issue a request to the model. Instead, they return an `Executable` (the variable `e` in the above example), which can then be executed to produce the final result. The benefit of this two-step process is that one can define possibly complex execution flows in a natural pythonic way, and decouple the definition of the flow from the actual backends that are used to execute it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHsoKuiI0xvD"
      },
      "source": [
        "### `llm.chat()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKNZQ9JaD2OV"
      },
      "source": [
        "The function `llm.chat()` asks the model to continue a sequence of messages that are a back-and-forth between a user and the model. As illustrated in the example below, each `Message` consists of a role and some content. The exact behavior of `llm.chat()` is model-specific. If the model supports the `chat` API natively (e.g., like most of the Gemini and OpenAI models), `llm.chat` uses it directly. Otherwise, OneTwo creates a prompt by merging the messages and adding model-specific control tokens and role indicators and then sending the prompt to `llm.generate.text()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSV0tkXwv39u"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.USER,\n",
        "        content=(\n",
        "            'Pretend that you are Albert Einstein in 1911.\\n'\n",
        "            'Hi, my name is Peter. Who are you?'\n",
        "        ),\n",
        "    ),\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.MODEL,\n",
        "        content='Nice to meet you, Peter. My name is Albert.',\n",
        "    ),\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.USER,\n",
        "        content='Tell me more about yourself. Do you have a family and what is your job like?',\n",
        "    ),\n",
        "]\n",
        "e = llm.chat(messages, max_tokens=20)\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZKHMEGC05-E"
      },
      "source": [
        "### `llm.instruct()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCDQb98aD6T3"
      },
      "source": [
        "The function `llm.instruct()` asks the model to follow a certain instruction. This is different from `llm.generate_text()` in that, to the extent supported by the model, the request is formatted such that it is clear that we want the instruction to be followed. The exact behavior of `llm.instruct()` is model-specific. By default OneTwo converts it into an `llm.chat()` request, which works well for most instruction-tuned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDqpQ8WW6vnR"
      },
      "outputs": [],
      "source": [
        "# Issue an instruct() request.\n",
        "e = llm.instruct('What are three well-known cities in France?', max_tokens=50)\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD2pKGvU05-G"
      },
      "outputs": [],
      "source": [
        "# Issue an instruct() request.\n",
        "e = llm.instruct('Write a 4 line poem about the Swiss Alps.', max_tokens=50)\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `llm.generate_object()`: Structured Output"
      ],
      "metadata": {
        "id": "baxeKYJ6IjL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs provide powerful structured output mechanisms, in which the model output can be constrained via Python type specification.\n",
        "\n",
        "*Warning*: While structured output can be effective in enforcing a given response format, it does not necessarily improve the *accuracy* of the response and in some cases may even hurt accuracy. (See in particular the \"Chain-of-Thought\" section below for examples of how a longer and less-constrained response format can potentially lead to higher accuracy.)"
      ],
      "metadata": {
        "id": "nfYys2D0Io0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%run_if model_supports_generate_object\n",
        "\n",
        "# Using generate_object with a specified type ==\u003e returns an int.\n",
        "e = llm.generate_object(prompt='Q: How many meters tall is the Eiffel tower?\\nA:', cls=int)\n",
        "res = ot.run(e)\n",
        "print(res)\n",
        "print(type(res))"
      ],
      "metadata": {
        "id": "5sXiOGDyIiso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%run_if model_supports_generate_object\n",
        "\n",
        "# Using generate_object with a specified type ==\u003e returns a list of strings.\n",
        "e = llm.generate_object(\n",
        "    prompt='Q: Name three common fruits\\nA: ', cls=list[str]\n",
        ")\n",
        "res = ot.run(e)\n",
        "print(res)\n",
        "print(type(res))"
      ],
      "metadata": {
        "id": "MdBxhOCNJcbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard `dataclasses.dataclass` objects need to be wrapped with `pydantic.dataclasses.dataclass` to ensure compatibility with `llm.generate_object`. Types already based on `pydantic.BaseModel` do not require wrapping."
      ],
      "metadata": {
        "id": "Dpa_FBaoJj9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%run_if model_supports_generate_object\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class StandardRecipe:\n",
        "  recipe_name: str\n",
        "  description: str\n",
        "  prep_time_minutes: int\n",
        "  ingredients: list[str]\n",
        "\n",
        "# Wrap the standard dataclass for compatibility with generate_object\n",
        "input_cls = pydantic_dataclass(StandardRecipe)\n",
        "\n",
        "prompt = \"\"\"Give me a simple recipe for banana bread,\n",
        "including a name, a short description, prep time in minutes, and a list of\n",
        "ingredients.\"\"\"\n",
        "e = llm.generate_object(prompt=prompt, cls=input_cls)\n",
        "result = ot.run(e)\n",
        "print(f\"Result: {result}\")\n",
        "print(f\"Recipe Name: {result.recipe_name}\")\n",
        "print(f\"Description: {result.description}\")\n",
        "print(f\"Prep Time: {result.prep_time_minutes} minutes\")\n",
        "print(f\"Ingredients: {result.ingredients}\")"
      ],
      "metadata": {
        "id": "AGPXAKHYJkQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%run_if model_supports_generate_object\n",
        "\n",
        "# Using generate_object with chat history\n",
        "messages = [\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.USER,\n",
        "        content=(\n",
        "            'What is the capital of France?'\n",
        "        ),\n",
        "    ),\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.MODEL,\n",
        "        content='Obviously, it is Paris.',\n",
        "    ),\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.USER,\n",
        "        content='Tell me more about it.',\n",
        "    ),\n",
        "]\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class CityInfo:\n",
        "  city_name: str\n",
        "  country: str\n",
        "  population: int\n",
        "  landmark: str | None = None\n",
        "\n",
        "input_cls = pydantic_dataclass(CityInfo)\n",
        "e = llm.generate_object(prompt=messages, cls=input_cls)\n",
        "result = ot.run(e)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "CDUHlWtoWth-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%run_if model_supports_generate_object\n",
        "\n",
        "class WordCount(BaseModel):\n",
        "  word: str\n",
        "  count: int\n",
        "\n",
        "prompt = (\n",
        "    'Return a breakdown of the words and their frequencies in the following '\n",
        "    'sentence: \"the quick brown fox jumps over the lazy dog dog\". '\n",
        "    'Provide the output as a list of word-count objects.'\n",
        ")\n",
        "e = llm.generate_object(prompt=prompt, cls=list[WordCount])\n",
        "result = ot.run(e)\n",
        "print(f\"Result: {result}\")"
      ],
      "metadata": {
        "id": "Kb0vN0QEJeeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_iAv6afOH3E"
      },
      "source": [
        "### Printing actual formatted prompts sent to the model\n",
        "\n",
        "Often when using builtins like `llm.instruct` and `llm.chat` and debugging your code it may be important to print the arguments used in actual calls to the underlying model API (e.g., final formatted prompts or lists of messages). One simple way to do it is to swap (mock) the `llm.generate_text` (which often happens to be the very last step for many of the builtins) with a fake implementation that returns its input.\n",
        "\n",
        "Below we connect to the `gpt-3.5-turbo-instruct` OpenAI model. This model does not natively support the `chat` API. When we register this backend, `llm.chat` gets configured with the default implementation that formats the prompt and calls `llm.generate_text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33xFxpD1OOgc"
      },
      "outputs": [],
      "source": [
        "%%run_if model_selection == 'OpenAI API'  # Execute cell only for 'OpenAI API'.\n",
        "\n",
        "new_backend = openai_api.OpenAIAPI(\n",
        "    api_key=openai_api_key,\n",
        "    model_name='gpt-3.5-turbo-instruct',  # Does not support `chat` natively.\n",
        "    temperature=0.0,\n",
        ")\n",
        "new_backend.register()\n",
        "\n",
        "messages = [\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.USER,\n",
        "        content=(\n",
        "            'Hello! Who is your favourite writer?'\n",
        "        ),\n",
        "    ),\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.MODEL,\n",
        "        content='I guess one of the American novelists from the 1940s, e.g.',\n",
        "    ),\n",
        "]\n",
        "e = llm.chat(messages, max_tokens=20)\n",
        "\n",
        "# `llm.chat` formats the prompt and calls `llm.generate_text`.\n",
        "print(\n",
        "    f'llm.chat returned:\\n--\u003e|{ot.run(e)}|\u003c--',\n",
        "    end='\\n\\n',\n",
        ")\n",
        "\n",
        "def fake_generate_text(prompt: str | content.ChunkList, **kwargs):\n",
        "    return prompt\n",
        "\n",
        "llm.generate_text.configure(fake_generate_text)\n",
        "\n",
        "# Now `llm.generate_text` simply returns its input.\n",
        "print(f'llm.generate_text was called with prompt:\\n--\u003e|{ot.run(e)}|\u003c--')\n",
        "\n",
        "# Reset the builtins to their original state.\n",
        "backend.register()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk0kxqvU1Shj"
      },
      "source": [
        "## Composing Executables\n",
        "\n",
        "If we want to chain two successive calls, we can perform one after the other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHtezyXqjwL3"
      },
      "outputs": [],
      "source": [
        "result1 = ot.run(\n",
        "    llm.instruct(\n",
        "        'Question: What is the southernmost city in France? (Just the answer.)\\nAnswer:',\n",
        "        max_tokens=20, stop=['\\n']\n",
        "    )\n",
        ")\n",
        "result2 = ot.run(\n",
        "    llm.instruct(\n",
        "        f'Question: Who is the mayor of {result1}? (Just the answer.)\\nAnswer:',\n",
        "        max_tokens=20, stop=['\\n']\n",
        "    )\n",
        ")\n",
        "print(result1)\n",
        "print(result2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXBJUKHj1Shl"
      },
      "source": [
        "But a better way is to create a new Executable that performs the all the desired operations and can then be executed on arbitrary backends. We do this by writing a function with the decorator `@ot.make_executable`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N32s6lcg1Shl"
      },
      "outputs": [],
      "source": [
        "@ot.make_executable\n",
        "async def f() -\u003e str:\n",
        "  result1 = await llm.instruct(\n",
        "      'Question: What is the southernmost city in France? (Just the answer.)\\nAnswer:',\n",
        "      max_tokens=20, stop=['\\n']\n",
        "  )\n",
        "  print('Intermediate result:', result1)\n",
        "  result2 = await llm.instruct(\n",
        "        f'Question: Who is the mayor of {result1}? (Just the answer.)\\nAnswer:',\n",
        "        max_tokens=20, stop=['\\n']\n",
        "  )\n",
        "  return result2\n",
        "\n",
        "result = ot.run(f())\n",
        "print('Final result:', result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSCkzgVk1Shm"
      },
      "source": [
        "If we want to execute in parallel instead of executing serially, we can compose executables with `onetwo.parallel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVMrrvCJ1Shm"
      },
      "outputs": [],
      "source": [
        "e1 = llm.instruct(\n",
        "    'Question: What is the southernmost city in France? (Just the answer.)\\nAnswer:',\n",
        "    max_tokens=20, stop=['\\n']\n",
        ")\n",
        "e2 = llm.instruct(\n",
        "    'Question: What is the southernmost city in Spain? (Just the answer.)\\nAnswer:',\n",
        "    max_tokens=20, stop=['\\n']\n",
        ")\n",
        "e = ot.parallel(e1, e2)\n",
        "result_list = ot.run(e)\n",
        "print(result_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWSlVHY1ZcjA"
      },
      "source": [
        "## Templates and Composables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BKl_h2lZcjA"
      },
      "source": [
        "The advantage of the LLM built-ins is that arbitrarily complex execution flows can be defined directly in the Python language. At the same time, this approach may not always be the best way to visualize the textual structure of the prompt. Therefore, we provide alternative techniques for defining and composing prompts, which tend to be more visual (\"what you see is what you get\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVndXBBZcjA"
      },
      "source": [
        "One technique is to define and execute prompt templates using the Jinja2 syntax. We can create a Jinja2 template using the built-in `composables.j()`. Similarly to the `llm` builtins, this function again does not directly issue a call to a backend. Instead, it returns an executable that can be run on a certain model or can be composed with other executables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTP5uo-ZZcjA"
      },
      "outputs": [],
      "source": [
        "template = composables.j(\"\"\"\\\n",
        "Question: What is the southernmost city in France? (Just the answer.)\n",
        "Answer:{{ generate_text(max_tokens=20, stop=['\\\\n']) }}\n",
        "Question: Who is its mayor?\n",
        "Answer:{{ generate_text(max_tokens=20, stop=['\\\\n']) }}\n",
        "\"\"\")\n",
        "result = ot.run(template)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDu_vKvjZcjA"
      },
      "source": [
        "Another technique is to use `Composables`, which are variants of the LLM built-ins that can be concatenated into the prompt string using `+`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXjoSHb2ZcjA"
      },
      "outputs": [],
      "source": [
        "e = (\n",
        "    'Question: What is the southernmost city in France? (Just the answer.)\\nAnswer:'\n",
        "    + composables.generate_text(max_tokens=20, stop=['\\n'])\n",
        "    + '\\nQuestion: Who is its mayor?\\nAnswer:'\n",
        "    + composables.generate_text(max_tokens=20, stop=['\\n'])\n",
        ")\n",
        "result = ot.run(e)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkh6-IVIjgxV"
      },
      "source": [
        "Composables provide even more flexibility than Jinja2 templates, as they natively support multimodal content and `chat` operations, which Jinja2 templates do not currently support.\n",
        "\n",
        "Below is an an example of the same two step \"conversation\" with the LLM that was shown above, but this time using explicit chat operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3JJINLdmANE"
      },
      "outputs": [],
      "source": [
        "e = (\n",
        "    'Question: What is the southernmost city in France? (Just the answer.)\\nAnswer:'\n",
        "    + composables.chat(max_tokens=20, stop=['\\n'])\n",
        "    + '\\nQuestion: Who is its mayor?\\nAnswer:'\n",
        "    + composables.chat(max_tokens=20, stop=['\\n'])\n",
        ")\n",
        "result = ot.run(e)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JZM8UDFZcjA"
      },
      "source": [
        "## Prompt Variables\n",
        "\n",
        "Another useful technique is to use variables as part of the prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmEp1IdcZcjB"
      },
      "source": [
        "One way to use variables is via Python f-strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37CM70NjZcjB"
      },
      "outputs": [],
      "source": [
        "country = 'France'\n",
        "prompt1 = f'Question: What is the capital of {country}? (Just the answer.)\\nAnswer:'\n",
        "res1 = ot.run(llm.instruct(prompt1, max_tokens=10, stop=['\\n']))\n",
        "print(res1)\n",
        "prompt2 = f'Question: Who is the mayor of {res1}? (Just the answer.)\\nAnswer:'\n",
        "res2 = ot.run(llm.instruct(prompt2, max_tokens=10, stop=['\\n']))\n",
        "print(res2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLaedoVlZcjB"
      },
      "source": [
        "To make this code more reusable, we can compose the two executables into a single function that defines a new executable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf0kMwq9ZcjB"
      },
      "outputs": [],
      "source": [
        "@ot.make_executable\n",
        "async def capital_mayor(country: str) -\u003e str:\n",
        "  prompt = f'Question: What is the capital of {country}? (Just the answer.)\\nAnswer:'\n",
        "  res1 = await llm.instruct(prompt, max_tokens=10, stop=['\\n'])\n",
        "  prompt2 = f'Question: Who is the mayor of {res1}? (Just the answer.)\\nAnswer:'\n",
        "  return await llm.instruct(prompt2, max_tokens=10, stop=['\\n'])\n",
        "\n",
        "print(ot.run(capital_mayor('France')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJJUTmLNZcjB"
      },
      "source": [
        "Another variant is to use variables in the Jinja2 syntax. The following Jinja2 template is parameterized by the input variable `question`. In addition, we issue multiple requests and store their results in variables that can be referenced from within the prompt. The variables can also be retrieved from the resulting executable (`prompt`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgZava8qZcjB"
      },
      "outputs": [],
      "source": [
        "prompt = composables.j(\n",
        "    'Question: What is the capital of {{ question }}? (Just the answer.)\\n'\n",
        "    'Answer: {{ store(\"city\", generate_text(max_tokens=10, stop=[\"\\\\n\"]) | trim) }}\\n'\n",
        "    'Question: Who is the mayor of {{ __vars__.city }}? (Just the answer.)\\n'\n",
        "    'Answer: {{ store(\"mayor\", generate_text(max_tokens=10, stop=[\"\\\\n\"]) | trim) }}'\n",
        ")\n",
        "res = ot.run(prompt(question='France'))\n",
        "print(res)\n",
        "\n",
        "print('\\n(city, mayor) =', (prompt['city'], prompt['mayor']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtpW_iuhZcjB"
      },
      "source": [
        "This above Jinja2 behavior can also be emulated using `composables`. We use `composables.f()` to format a string containing variables and we use `composables.store()` to store the results of LLM requests. As with Jinja2 variables, these variables can be retrieved from the resulting executable (`e`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ6ksMRLZcjB"
      },
      "outputs": [],
      "source": [
        "e = (\n",
        "    composables.f('Question: What is the capital of {question}? (Just the answer.)\\nAnswer:') +\n",
        "    composables.store('city', composables.generate_text(max_tokens=10, stop=['\\n'])) +\n",
        "    composables.f('\\nQuestion: Who is the mayor of {city}? (Just the answer.)\\nAnswer:') +\n",
        "    composables.store('mayor', composables.generate_text(max_tokens=10, stop=['\\n']))\n",
        ")\n",
        "res = ot.run(e(question='France'))\n",
        "print(res)\n",
        "\n",
        "print('\\n(city, mayor) =', (e['city'].strip(), e['mayor'].strip()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-trZgnodt02P"
      },
      "source": [
        "## Sampling\n",
        "\n",
        "By default, the result for a given prompt execution is cached. This means that if you execute the same prompt more than once, a backend request is issued only the first time, and the cached result is returned for all subsquent executions, even if temperature \u003e 0.\n",
        "\n",
        "While this is useful, there are cases where we actually want to obtain different samples. OneTwo provides the function `repeat()` to achieve this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77UZg5X2hhkg"
      },
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    'Please output a completion for the following sentence, without repeating any existing content.\\n'\n",
        "    'Five not-so-well-known cities in Switzerland (just numbered list of city names):\\n'\n",
        "    '1. Kilchberg, 2. Aesch,'\n",
        ")\n",
        "\n",
        "# Make sure we set a non-zero temperature to avoid that the model returns\n",
        "# the same sample for each call.\n",
        "executable = llm.generate_text(prompt=prompt, max_tokens=20, stop=['\\n'], temperature=1.0)\n",
        "\n",
        "# Create a list containing 3 instances of the above executable.\n",
        "repeated_executable = ot.repeat(executable, 3)\n",
        "\n",
        "# Create an executable that executes the executables in the list in parallel.\n",
        "parallel_executable = ot.parallel(*repeated_executable)\n",
        "\n",
        "print(ot.run(parallel_executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nrba0fFzY8k"
      },
      "source": [
        "Note that `repeat()` takes as an argument the `executable` and returns a list (`repeated_executable`) containing 3 instances of it. We then use `parallel` to turn this list into the single executable `parallel_executable`, which executes all elements in parallel.\n",
        "\n",
        "Also note that we set temperature to 1.0 in `generate_text()` to avoid that the model returns the same sample for each call.\n",
        "\n",
        "If we now ask for 6 samples, the first 3 will be drawn from the cache while the next 3 will be generated by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4tkwZuT1q8s"
      },
      "outputs": [],
      "source": [
        "repeated_executable = ot.repeat(executable, 6)\n",
        "parallel_executable = ot.parallel(*repeated_executable)\n",
        "\n",
        "print(ot.run(parallel_executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgz2cId2MpO"
      },
      "source": [
        "If we want to get 2 more samples, we can do this by providing the `start_index` as an additional argument to `repeat()`. In the code below, we use `start_index=6`, which means that we do not want to retrieve the first 6 results of the executable from the cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sCq71bZ2q_Q"
      },
      "outputs": [],
      "source": [
        "repeated_executable = ot.repeat(executable, 2, start_index=6)\n",
        "parallel_executable = ot.parallel(*repeated_executable)\n",
        "\n",
        "print(ot.run(parallel_executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ct5-ISOS00L"
      },
      "source": [
        "## Switching Backends\n",
        "\n",
        "While up till now we just registered a single LLM backend globally so as to send all of the LLM requests to it, it is also possible to specify a backend to use for one specific code branch without affecting the choice of backend used elsewhere.\n",
        "\n",
        "We can do that by wrapping a code block in a `with ot.with_registry(...):` construct and then registering the LLM to use within just that block of code. The basic pattern looks like the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LduEUIPJTupK"
      },
      "outputs": [],
      "source": [
        "# other_backend = ...\n",
        "# with ot.RegistryContext():\n",
        "#   other_backend.register()\n",
        "#   llm.generate_text(prompt=prompt)  # Would use `other_backend`.\n",
        "\n",
        "# llm.generate_text(prompt=prompt)  # Would use the original backend again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obmMPFRbpyvf"
      },
      "source": [
        "You can use a similar pattern for swapping between different variants of the same LLM backend -- e.g., to apply a different default value for a parameter like `temperature` by applying `update` to a built-in function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrUCIR3Kpyvg"
      },
      "outputs": [],
      "source": [
        "# prompt = ...\n",
        "# with ot.RegistryContext():\n",
        "#   # The 'update' method can be used to set default values for arbitrary\n",
        "#   # parameters of any built-in function.\n",
        "#   llm.generate_text.update(temperature=0.5, max_tokens=5)\n",
        "\n",
        "#   # From here on, any calls to `llm.generate_text` will default to the\n",
        "#   # temperature and max_tokens values we set above.\n",
        "#   print('Prompting LLM with temperature 0.5 and max_tokens 5:')\n",
        "#   print(ot.run(llm.generate_text(prompt=prompt, stop=['\\n'])))\n",
        "\n",
        "# # When we exit the `ot.RegistryContext()` block, the registered backends and\n",
        "# # their default values all revert to the values they had previously.\n",
        "# print('\\nPrompting LLM with the default temperature and max_tokens:')\n",
        "# print(ot.run(llm.generate_text(prompt=prompt, stop=['\\n'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xYmXNgVpyvg"
      },
      "source": [
        "One caveat to be aware of is that in some older versions of ipython, there was a bug affecting the use of contexvars in colab or similar environments (see https://github.com/ipython/ipython/issues/11565 for details). If applying the above pattern directly doesn't work for you, it could be due to use of an older version of ipython.\n",
        "\n",
        "Another related approach that works even with the older versions of ipython is to make multiple copies of the registry ahead of time and swap between them. We demonstrate this approach below in the `register_and_get_wrapper` convenience function, which causes the wrapped executable to be executed with the specified backend and with the specified default parameter values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPNU64a0pyvg"
      },
      "outputs": [],
      "source": [
        "def register_and_get_wrapper(\n",
        "    backend: Any,\n",
        "    generate_text_kwargs: dict | None = None,\n",
        "    chat_kwargs: dict | None = None,\n",
        "    ):\n",
        "  \"\"\"Returns a wrapper function to run an executable with the specified backend.\n",
        "\n",
        "  Args:\n",
        "    backend: The backend to use for the wrapped executable.\n",
        "    generate_text_kwargs: Default parameter values to use in calls to\n",
        "      `llm.generate_text` within the wrapped executable.\n",
        "    chat_kwargs: Default parameter values to use in calls to `llm.chat` within\n",
        "      the wrapped executable. (Also applies to `llm.instruct`, which for most\n",
        "      backends calls `llm.chat` internally.)\n",
        "  \"\"\"\n",
        "  with ot.RegistryContext():\n",
        "    backend.register()\n",
        "    if generate_text_kwargs:\n",
        "      llm.generate_text.update(**generate_text_kwargs)\n",
        "    if chat_kwargs:\n",
        "      llm.chat.update(**chat_kwargs)\n",
        "    registry = ot.copy_registry()\n",
        "  return lambda e: ot.with_registry(e, registry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PLhWb2U2GlU"
      },
      "outputs": [],
      "source": [
        "prompt = 'Ten not so well-known cities in Switzerland (on one line) are Kilchberg,'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmrTwneu2Bde"
      },
      "outputs": [],
      "source": [
        "# Now we can use this to create a wrapper function for running an executable\n",
        "# with our customized `temperature` and `max_tokens` values.\n",
        "on_backend_temp_0_5_max_tokens_5 = register_and_get_wrapper(\n",
        "    backend,\n",
        "    generate_text_kwargs={'temperature': 0.5, 'max_tokens': 5},\n",
        "    chat_kwargs={'temperature': 0.5, 'max_tokens': 5},\n",
        ")\n",
        "\n",
        "# Suppose this is the executable that we want to be able to run with the\n",
        "# different backends or parameter defaults.\n",
        "executable = llm.generate_text(prompt=prompt, stop=['\\n'])\n",
        "\n",
        "# Now we can call that same executable as many times as we want, swapping\n",
        "# between backends or parameter defaults using the wrapper function.\n",
        "print('Prompting LLM with temperature 0.5 and max_tokens 5:')\n",
        "print(ot.run(on_backend_temp_0_5_max_tokens_5(executable)))\n",
        "\n",
        "print('\\nPrompting LLM with the default temperature and max_tokens:')\n",
        "print(ot.run(executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0cpbigTZcjC"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "As you tweak your prompt or prompting strategy, it is important to use an evaluation process based on hard numbers and not just anecdotal evidence.\n",
        "OneTwo comes with several example evaluation functions to facilitate this process, which you can either reuse as-is or clone to create your own custom script. We present the most basic and general-purpose script here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQum4Gmkq-Kg"
      },
      "source": [
        "### Programmatic Comparison\n",
        "\n",
        "The `ot.evaluate` function can be used to evaluate a prompting strategy by running it over a dataset and then automatically comparing the answers to the golden data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxYTdfbm2GwJ"
      },
      "outputs": [],
      "source": [
        "# Golden dataset to evaluate our prompting strategy.\n",
        "dataset = [\n",
        "    {'question': 'There are 100 people in a room. 55 are women and 70 are married. If 30 of the women are married, how many unmarried men are there?', 'answer': '5'},\n",
        "    {'question': 'A farmer has 12 sheep and 6 cows. How many more sheep than cows does he have?', 'answer': '6'},\n",
        "    {'question': 'A train travels 240 miles in 3 hours. What is its average speed in miles per hour?', 'answer': '80'},\n",
        "    {'question': 'A rectangular garden is 12 meters long and 8 meters wide. What is its perimeter?', 'answer': '40'},\n",
        "    {'question': 'If x + y = 10 and x - y = 2, what is the value of x?', 'answer': '6'},\n",
        "    {'question': 'A store sells apples for $0.50 each and oranges for $0.75 each. If I buy 5 apples and 3 oranges, how much will I spend?', 'answer': '4.75'},\n",
        "    {'question': 'A circle has a radius of 5 cm. What is its circumference in centimeters?', 'answer': '10*pi'},\n",
        "    {'question': 'A cube has a volume of 27 cubic feet. What is the length of one side of the cube in feet?', 'answer': '3'},\n",
        "    {'question': 'If 2^x = 16, what is the value of x?', 'answer': '4'},\n",
        "    {'question': 'What is the sum of the first 10 positive odd numbers?', 'answer': '100'}\n",
        "]\n",
        "\n",
        "# Define a simple strategy that passes the question directly to the model.\n",
        "@ot.make_executable\n",
        "async def strategy(question, **_):\n",
        "  answer = await llm.generate_text(\n",
        "      prompt=f'Question: {question}\\nFinal answer (formula or number):',\n",
        "      stop=['Question:', '\\n'],\n",
        "  )\n",
        "  return answer.strip()\n",
        "\n",
        "# Define a simple metric function that checks whether the correct answer is part\n",
        "# of the model output.\n",
        "def metric_fn(answer, example):\n",
        "  correct = str(example['answer']) in answer\n",
        "  extra_info = {}\n",
        "  if not correct:\n",
        "    index = hash(example['question'])\n",
        "    extra_info = {index: {\n",
        "        'question': example['question'][0:30] + '...',\n",
        "        'golden': example['answer'],\n",
        "        'answer': answer,\n",
        "    }}\n",
        "  return float(correct), extra_info\n",
        "\n",
        "# We run the evaluation on the dataset.\n",
        "time_elapsed, avg_metric, aggr_info = ot.evaluate(\n",
        "    strategy=strategy,\n",
        "    examples=dataset,\n",
        "    critic=metric_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiL6Z_CrNHxk"
      },
      "outputs": [],
      "source": [
        "# We can look at the cases where the model got a wrong answer.\n",
        "for v in aggr_info.values():\n",
        "  print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hYBlp1VIAJ7"
      },
      "source": [
        "### Using an LLM Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XEF_qrEeEP0"
      },
      "source": [
        "In some cases, programmatic comparison to the Golden is difficult. For instance, consider the example above where the golden answer is `10*pi` while the model may provide an answer such as `31.4 cm` or `10 cm`.\n",
        "\n",
        "The same holds for the dataset below, where the answer to the questions is not a fixed number or string. In such cases, the evaluation script allows one to use an LLM to judge whether or not the provided answer is equivalent to the golden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4A12d1OMz_"
      },
      "outputs": [],
      "source": [
        "# We create another dataset of questions that do not necessarily have a fixed\n",
        "# definite answer.\n",
        "dataset = [\n",
        "    {\n",
        "        'question': 'Who developed the TCP/IP protocol?',\n",
        "        'golden_answer': 'Bob Kahn and Vint Cerf',\n",
        "    },\n",
        "    {\n",
        "        'question': 'What date was the declaration of independence signed (not written)?',\n",
        "        'golden_answer': '8/2/1776'\n",
        "    },\n",
        "    {\n",
        "        'question': 'How big is the area of a triangle with base a and height h',\n",
        "        'golden_answer': '1/2 * ah',\n",
        "    },\n",
        "    {\n",
        "        'question': 'Which countried border Guatemala?',\n",
        "        'golden_answer': 'Mexico, Belize, Honduras, El Salvador',\n",
        "    },\n",
        "    {\n",
        "        'question': 'How do vaccines work?',\n",
        "        'golden_answer': (\n",
        "            'Vaccines contain weakened or inactive pathogens that stimulate the'\n",
        "            ' immune system to produce antibodies, which then protect against'\n",
        "            ' future infections from the same pathogen.'\n",
        "        ),\n",
        "    },\n",
        "]\n",
        "\n",
        "# Define a simple strategy that passes the question directly to the model.\n",
        "@ot.make_executable\n",
        "async def strategy(question, **_):\n",
        "  answer = await llm.generate_text(\n",
        "      prompt=f'Question: {question} ?\\nAnswer (concise):',\n",
        "      stop=['Question:'],\n",
        "      max_tokens=500,\n",
        "  )\n",
        "  return answer.strip()\n",
        "\n",
        "  # We run the evaluation on the dataset using an LLM critic.\n",
        "time_elapsed, total_votes, aggr_info = ot.evaluate(\n",
        "    strategy=strategy,\n",
        "    critic=ot.naive_evaluation_critic,\n",
        "    examples=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nESW4fQ0hnhu"
      },
      "outputs": [],
      "source": [
        "# This prints the critic prompt that was used.\n",
        "critic_prompt = list(aggr_info.values())[0]['critic_prompt']\n",
        "if isinstance(critic_prompt, list):\n",
        "  merged_prompt = ''.join(x.content for x in critic_prompt)\n",
        "  print(merged_prompt)\n",
        "else:\n",
        "  print(critic_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBN4nc8eIfMt"
      },
      "outputs": [],
      "source": [
        "# This prints some debug information about the critic's judgments.\n",
        "for k, v in aggr_info.items():\n",
        "  print('----------')\n",
        "  print(f'Question: {k}')\n",
        "  print(f'   - Golden: {v[\"golden_answer\"]}')\n",
        "  print(f'   - Answer: {v[\"candidate_answer\"]}')\n",
        "  print(f'   - Correct: {v[\"answer_is_correct\"]}')\n",
        "  print(f'   - Reason: {v[\"reason\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph__BdN6YD0Y"
      },
      "source": [
        "## Multimodal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTY4g4WMG_j6"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class ImageSource:\n",
        "  external_url: str = ''\n",
        "  local_path: str = ''\n",
        "  license: str = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF9V7HuZHCJv"
      },
      "outputs": [],
      "source": [
        "%%run_if model_supports_multimodal\n",
        "\n",
        "image_sources = {\n",
        "    'paligemma_fox': ImageSource(\n",
        "        external_url='https://big-vision-paligemma.hf.space/file=/tmp/gradio/4aa2d3fd01a6308961397f68e043b2015bc91493/image.png',\n",
        "        local_path='images/paligemma_fox.png',\n",
        "        license='CC0 by [XiaohuaZhai@](https://sites.google.com/corp/view/xzhai)',\n",
        "    ),\n",
        "    'paligemma_puffin': ImageSource(\n",
        "        external_url='https://big-vision-paligemma.hf.space/file=/tmp/gradio/78f93b49088f8d72ee546d656387403d647b413f/image.png',\n",
        "        local_path='images/paligemma_puffin.png',\n",
        "        license='CC0 by [XiaohuaZhai@](https://sites.google.com/corp/view/xzhai)',\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN5xKKRvHI22"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tFjAG4ZuKr4"
      },
      "outputs": [],
      "source": [
        "%%run_if model_supports_multimodal\n",
        "\n",
        "# Now, let's load a few images for use in prompting the LLM.\n",
        "\n",
        "images = {}\n",
        "for i, (image_name, image_source) in enumerate(image_sources.items()):\n",
        "  image_bytes = None\n",
        "  if image_source.local_path:\n",
        "    with open(image_source.local_path, 'rb') as f:\n",
        "      image_bytes = f.read()\n",
        "  elif image_source.external_url:\n",
        "    url = image_source.external_url\n",
        "    try:\n",
        "      response = requests.get(url, stream=True)\n",
        "      response.raise_for_status()\n",
        "      image_bytes = response.content\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print(f\"Error loading image from {url}: {e}\")\n",
        "  if not image_bytes:\n",
        "    raise ValueError(f'Unable to download image {image_name}: {image_source}')\n",
        "  images[image_name] = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "for image_name, image in images.items():\n",
        "  if isinstance(image, Image.Image):\n",
        "    image_np = np.array(image)\n",
        "  else:\n",
        "    image_np = np.array(Image.open(io.BytesIO(image)))\n",
        "  print(f'Image: {image_name}')\n",
        "  print(f'Image license: {image_sources[image_name].license}')\n",
        "  plt.imshow(image_np)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIudaGSTuKr7"
      },
      "outputs": [],
      "source": [
        "%%run_if model_supports_multimodal\n",
        "\n",
        "# We can now use the \"composables\" mechanism to construct a multimodal prompt\n",
        "# and ask the multimodal model to answer questions about those images.\n",
        "for image_name, image in images.items():\n",
        "  composable = (\n",
        "      composables.c(image)\n",
        "      + 'Question: What is on this picture? Please explain how you reach the'\n",
        "      ' answer.\\nAnswer: This is '\n",
        "      + composables.generate_text()\n",
        "  )\n",
        "  print(f'Image: {image_name}')\n",
        "  print(ot.run(composable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uvNOrPyCpjQ"
      },
      "source": [
        "## Few-shot prompting\n",
        "\n",
        "A common technique for controlling an LLM's behavior is to include in the prompt several examples of the expected inputs and outputs, which are referred to as \"few-shot exemplars\". This has traditionally been one of the most effective techniques for controlling the behavior of base language models that have not been specifically trained for instruction-following.\n",
        "\n",
        "If you send the following prompt to a reasonably powerful non-instruction-tuned model, for example, the model will likely output a concise etymology for the word \"sauerkraut\", based on the two preceding exemplars, even though we do not include any explicit instructions about the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NooT-tRVsVKy"
      },
      "outputs": [],
      "source": [
        "# Example of few-shot prompting of a non-instruction-tuned LLM.\n",
        "\n",
        "prompt_prefix = \"\"\"\\\n",
        "Word: machine\n",
        "Etymology: \"machine\" (Modern English) \u003c- \"machina\" (Latin: machine, siege engine) \u003c- \"makhana\" (Greek: contrivance, machine, engine)\n",
        "\n",
        "Word: doorknob\n",
        "Etymology: \"doorknob\" (Modern English) \u003c- \"door\" (Modern English) + \"knob\" (Modern English) \u003c- \"duru\" (Old English: door) + \"knobbe\" (Middle Low German: knob, knot)\n",
        "\n",
        "Word: sauerkraut\n",
        "Etymology: \"\"\"\n",
        "\n",
        "e = llm.generate_text(prompt_prefix, max_tokens=100, stop=['\\n'])\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGwJ_6YKf2n3"
      },
      "source": [
        "When using an instruction-tuned model, the use of explicit instructions becomes more important, although few-shot exemplars are still often used as well, as a way of more precisely conveying the desired output format.\n",
        "\n",
        "In the case where the instruction-tuned model has been trained on chat semantics (as is the case with most of the Gemini and OpenAI models), a recipe that tends to work well is to represent the prompt as a sequence of chat messages, beginning with explicit instructions about the task, and the following that the few-shot exemplars in the form of alternating `USER` and `MODEL` messages (`USER` to represent the prompt we send to the LLM, and `MODEL` to represent its expected response)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RxQYAmZvblV"
      },
      "outputs": [],
      "source": [
        "# Example of few-shot prompting of an instruction-tuned LLM in chat format.\n",
        "messages = [\n",
        "    # We start with explicit instructions describing the task.\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.USER,\n",
        "        content=(\n",
        "            'Please summarize the most likely etymology for the given word in a single line separated by arrows.\\n\\n'\n",
        "        ),\n",
        "    ),\n",
        "    # We then optionally include one or more \"few-shot\" exemplars in the form of\n",
        "    # alternating `USER` and `MODEL` messages (`USER` to represent the prompt we\n",
        "    # send to the LLM, and `MODEL` to represent its expected response).\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.USER,\n",
        "        content=(\n",
        "            'Word: machine\\nEtymology: '\n",
        "        ),\n",
        "    ),\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.MODEL,\n",
        "        content='\"machine\" (Modern English) \u003c- \"machina\" (Latin: machine, siege engine) \u003c- \"makhana\" (Greek: contrivance, machine, engine)\\n\\n',\n",
        "    ),\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.USER,\n",
        "        content='Word: doorknob\\nEtymology: ',\n",
        "    ),\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.MODEL,\n",
        "        content='\"doorknob\" (Modern English) \u003c- \"door\" (Modern English) + \"knob\" (Modern English) \u003c- \"duru\" (Old English: door) + \"knobbe\" (Middle Low German: knob, knot)\\n\\n',\n",
        "    ),\n",
        "    content_lib.Message(\n",
        "        role=content_lib.PredefinedRole.USER,\n",
        "        content='Word: sauerkraut\\nEtymology: ',\n",
        "    ),\n",
        "]\n",
        "\n",
        "e = llm.chat(\n",
        "    messages,\n",
        "    max_tokens=100,\n",
        "    stop=['\\n'],\n",
        ")\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_d8cxyA4Ovn"
      },
      "source": [
        "## Chain-of-Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVv17afNUIFZ"
      },
      "source": [
        "Although in the QA examples up till now, we mostly prompted the model to directly output an answer, for problems that could benefit from reasoning or from some sort of multi-step solution, one can often achieve best results by using \"chain-of-thought prompting\" [[Wei, et al., 2023]](https://arxiv.org/pdf/2201.11903).\n",
        "\n",
        "In its most basic form, chain-of-thought is simply a style of how to\n",
        "write a prompt, where one prompts the LLM to output a reasoning chain followed\n",
        "by a final answer, rather than just the final answer, with the improved accuracy coming from the fact that the individual steps are easier to do \"off the top of one's head\" than the full problem, and the model is able to attend to its solutions to the intermediate steps when decoding the final answer.\n",
        "\n",
        "Chain-of-thought is easy to incorporate into your prompting strategies and, in its simplest form, does not require any dedicated components beyond what have already been introduced above. For example, as shown in [Kojima, et al., 2022](https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf), chain-of-thought reasoning can be elicited in many models in a zero-shot manner, by simply prefixing the answer with a phrase like \"Let's think step by step.\". For instruction-tuned models, chain-of-thought can typically be similarly induced by straightforwardly instructing the model to first output its reasoning before answering (or depending on how the model was trained, it might even default to outputting responses in chain-of-thought format for certain types of questions).\n",
        "\n",
        "As you can see below, implementing a zero-shot chain-of-thought strategy is just as easy as implementing a zero-shot direct answer strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjLRi-2-wNfl"
      },
      "outputs": [],
      "source": [
        "# Simple direct-answer strategy (similar to the one illustrated earlier).\n",
        "@ot.make_executable\n",
        "async def direct_answer_strategy(question, **_):\n",
        "  answer = await llm.generate_text(\n",
        "      prompt=f\"\"\"\\\n",
        "Please answer the question directly.\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\",\n",
        "      max_tokens=500,\n",
        "      stop=['\\nQuestion:'],\n",
        "  )\n",
        "  return answer.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GnPbcxF7NEM"
      },
      "outputs": [],
      "source": [
        "# Equally simple zero-shot chain-of-thought strategy.\n",
        "@ot.make_executable\n",
        "async def zero_shot_cot_strategy(question, **_):\n",
        "  answer = await llm.generate_text(\n",
        "      prompt=f\"\"\"\\\n",
        "Please answer the question, starting with your step-by-step reasoning, \\\n",
        "followed by your final answer (all on a single line).\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer: Let's think step by step.\"\"\",\n",
        "      max_tokens=500,\n",
        "      stop=['\\nQuestion:'],\n",
        "  )\n",
        "  return answer.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID_9YkF_8CFK"
      },
      "source": [
        "Now let's try evaluating each of these strategies on one of the questions from our earlier dataset that many models get wrong when prompted for the answer directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXN0kgAL5KhK"
      },
      "outputs": [],
      "source": [
        "# The correct answer is '5'.\n",
        "question = 'There are 100 people in a room. 55 are women and 70 are married. If 30 of the women are married, how many unmarried men are there?'\n",
        "direct_answer = ot.run(direct_answer_strategy(question))\n",
        "print(direct_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qezoj9XCwizS"
      },
      "outputs": [],
      "source": [
        "cot_answer = ot.run(zero_shot_cot_strategy(question))\n",
        "print(cot_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ-putxb8TTL"
      },
      "source": [
        "As you can see, the chain-of-thought answer now contains a detailed reasoning chain leading up to a final answer.\n",
        "\n",
        "When using chain-of-thought in practice, however, a number of additional pieces typically come into play, in particular:\n",
        "* **Answer extraction:** Even if the long-form chain-of-thought response contains the correct final answer, to make use of the answer programmatically, you will typically need to extract the final answer as a stand-alone string, for comparison to the golden answer or for consumption by the caller.\n",
        "* **Few-shot prompting:** Providing few-shot exemplars illustrating a particular style of chain-of-thought output can further improve accuracy on many tasks and/or make it easier to parse the final answer from the LLM reply.\n",
        "\n",
        "OneTwo's `chain_of_thought` library provides a number of off-the-shelf components that illustrate typical approaches to solving the above problems, which you can either re-use as-is or imitate in your own solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEtGrheAMsKz"
      },
      "source": [
        "Specifically, one approach to extracting the final answer is to prompt the LLM in two steps, first for the reasoning, and then for the final answer. This approach is illustrated in `QACoTPromptChat`, which encapsulates the two-step prompt in a simple prompting solution based on chat operations. The result is returned as a `CoTReply` data structure, which contains separate fields for the `reasoning` and the `answer`.\n",
        "\n",
        "This approach of using a follow-up prompt to elicit a final answer is essentially equivalent to the one described in [Zhou, et al., 2023](https://arxiv.org/pdf/2205.10625), who used it when applying a more structured chain-of-thought variant called \"least-to-most prompting\" to math reasoning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z1xshFUGlqB"
      },
      "outputs": [],
      "source": [
        "two_step_cot_strategy = chain_of_thought.QACoTPromptChat()\n",
        "cot_reply, trace_cot = ot.run(two_step_cot_strategy(question), enable_tracing=True)\n",
        "print(f'==========\\nReasoning:\\n==========\\n{cot_reply.reasoning}')\n",
        "print(f'\\n==========\\nAnswer:\\n==========\\n{cot_reply.answer}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1VMxeIgNZXp"
      },
      "source": [
        "You can see the exact prompts that were sent to the LLM by inspecting the trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIFkIAyS6z2k"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace_cot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70HPQeHyNf-_"
      },
      "source": [
        "The other approach to answer extraction is to apply few-shot prompting to guide the LLM to output an answer in a specific format (e.g., by separating the reasoning chain from the final answer with some fixed phrase like \"The answer is\") and then programmatically parse the LLM reply to separate the reasoning from the answer. This approach is illustrated in `QACoTPromptWithAnswerParserChat`, which makes just a single call to the LLM, followed by a call to an answer parser. The result is returned as a `CoTReply` data structure, the same as above.\n",
        "\n",
        "The below example emulates the same prompt and roughly the same parsing algorithm used in [Wei, et al., 2023](https://arxiv.org/pdf/2201.11903) in their original evaluation of chain-of-thought on math reasoning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM7BWB52J6Vw"
      },
      "outputs": [],
      "source": [
        "few_shot_cot_strategy = chain_of_thought.QACoTPromptWithAnswerParserChat(\n",
        "    exemplars=chain_of_thought.QA_COT_EXEMPLARS_ORIGINAL_MATH_WORD_PROBLEMS\n",
        ")\n",
        "cot_reply, trace_cot = ot.run(few_shot_cot_strategy(question), enable_tracing=True)\n",
        "print(f'==========\\nReasoning:\\n==========\\n{cot_reply.reasoning}')\n",
        "print(f'\\n==========\\nAnswer:\\n==========\\n{cot_reply.answer}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1cTmRbmRnf0"
      },
      "source": [
        "You can again see the expanded prompt and raw LLM reply by inspecting the trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXpFdceVB6JY"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace_cot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB4Edjwv94A3"
      },
      "source": [
        "## Self-Consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75xgMIrx94A3"
      },
      "source": [
        "One way of thinking about chain-of-thought is as a general-purpose strategy for trading higher computational cost (in the form of a longer decoded output) for higher accuracy on a complex, multi-step task.\n",
        "\n",
        "Another widely applicable strategy that trades higher computational cost for higher accuracy on complex tasks is that of \"self-consistency\" [[Wang, et al., 2023]](https://arxiv.org/pdf/2203.11171). Rather than higher computational cost through computation of longer sequences, however, self-consistency invests additional computational cost in the form of parallel processing.\n",
        "\n",
        "Specifically, the basic idea of self-consistency is to generate multiple samples using chain-of-thought or some other strategy involving diverse reasoning paths; extract a final answer from each sample; and then do majority voting over the answers. This has the effect of estimating a probability distribution over final answers, while marginalizing over the many possible reasoning paths that could potentially lead to the same answer. Empirically, this has been found to lead to double-digit improvements in accuracy on many tasks.\n",
        "\n",
        "Using techniques like those illustrated in the \"Sampling\" section above, you already have all the building blocks you need to implement your own versions of self-consistency, by combining things like `ot.repeat` and `ot.parallel` with some underlying strategy and appropriate post-processing. As a convenience, however, OneTwo also provides a generic implementation of self-consistency in the form of the `SelfConsistency` strategy, which can be used to wrap an arbitrary underlying strategy. Below we show how use this to wrap an underlying chain-of-thought strategy, like in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVP_CfdF0JoU"
      },
      "outputs": [],
      "source": [
        "# For this example we will use a model with temperature \u003e 0, so that we can\n",
        "# sample multiple distinct candidates.\n",
        "on_backend_temp_0_7 = register_and_get_wrapper(\n",
        "    backend,\n",
        "    generate_text_kwargs={'temperature': 0.7},\n",
        "    chat_kwargs={'temperature': 0.7},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTaiWY9F94A4"
      },
      "outputs": [],
      "source": [
        "# Wrapping a few-shot chain-of-thought strategy in self-consistency is as\n",
        "# simple as this.\n",
        "cot_sc = self_consistency.SelfConsistency(\n",
        "    sampler=sampling.Repeated(few_shot_cot_strategy),\n",
        "    bucketizer=lambda x: x.answer,  # Use the final answer as the bucket.\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4xGRWWE94A4"
      },
      "outputs": [],
      "source": [
        "# Now let's run it on the same question as before and inspect the results.\n",
        "answer_distribution_cot_sc, trace_cot_sc = ot.run(on_backend_temp_0_7(\n",
        "    cot_sc(question)), enable_tracing=True)\n",
        "\n",
        "pprint.pprint(answer_distribution_cot_sc, width=160)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdDk501_XrPd"
      },
      "source": [
        "As you can see, rather than returning just a single `CoTReply`, the strategy now returns a probability distribution over possible `CoTReply` candidates, with votes aggregated by bucketizing all of the candidate replies that contained the same final answer.\n",
        "\n",
        "Depending on the model, you may see, for example, a distribution in which the highest probability is assigned to the correct answer ('5'), with some probability scattered over several other incorrect candidates. For each of the candidate answers, we also receive one representative reasoning path that led to that answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sORLpQsxZAmz"
      },
      "source": [
        "If all we care about is extracting the consensus answer, we can do so by wrapping the `SelfConsistency` strategy with `ExtractConsensus`. This gives us a strategy with exactly the same function signature as the original underlying strategy (in this case, of `few_shot_cot_strategy`), so that it can be used as a drop-in replacement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7RwFS5SZMml"
      },
      "outputs": [],
      "source": [
        "cot_sc_concensus = self_consistency.ExtractConsensus(cot_sc)\n",
        "cot_sc_consensus_reply, trace_cot_sc_consensus = ot.run(\n",
        "    on_backend_temp_0_7(cot_sc_concensus(question)), enable_tracing=True)\n",
        "cot_sc_consensus_reply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u2T_C3_ZrCD"
      },
      "source": [
        "If we inspect the trace, we can see the full series of parallel calls to the LLM, along with the post-processing step that led to the consensus answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_5Hc69W94A4"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace_cot_sc_consensus))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J71bgSVypamm"
      },
      "source": [
        "## Agents and Tool Use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0bM7ZuAU2gh"
      },
      "source": [
        "In the above sections, we illustrated the low-level primitives of OneTwo and demonstrated how they can be used to construct multi-call prompting strategies from the ground up.\n",
        "\n",
        "In addition to defining one-off experiments, these same low-level primitives can also be used to encapsulate generic higher-level strategies into reusable building blocks, which can in turn be composed to build more complex custom solutions.\n",
        "\n",
        "In this section we will illustrate two higher-level strategies that are available as off-the-shelf components in OneTwo, both targeting multi-step tool use:\n",
        "\n",
        "\n",
        "1.   `ReActAgent`\n",
        "2.   `PythonPlanningAgent`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuF-dxzpgZi1"
      },
      "source": [
        "Both of these strategies take the form of an \"agent\", which we define as a strategy that converts inputs to outputs by way of a series of repeated updates to an internal state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3pW2gYu2zIU"
      },
      "source": [
        "### ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdCZIRuNeymM"
      },
      "source": [
        "The `ReActAgent` is based on the \"ReAct\" strategy presented in https://arxiv.org/abs/2210.03629.\n",
        "\n",
        "In this strategy, we present the LLM with a list of tool descriptions with invocation examples, and then iteratively prompt the LLM to output a sequence of steps, each of which consists of a \"thought\", an \"action\" and an \"observation\". The \"thought\" and \"action\" are output by the LLM directly. At each step, we programmatically parse the LLM-generated \"action\" string, perform the corresponding tool call, and then use the result of that tool call as the \"observation\" that is included in the LLM prompt in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgH--0of3J2n"
      },
      "source": [
        "#### Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss57Hu7NgPgP"
      },
      "source": [
        "As a first step, we will prepare a list of tools that we want to make available to the LLM.\n",
        "\n",
        "In this case, we will use two tools:\n",
        "\n",
        "*   **Python:** Tool for executing a program in a Python sandbox (e.g., for performing calculations).\n",
        "*   **Search:** Tool for retrieving snippets from Google Search.\n",
        "\n",
        "In addition, we provide one more \"tool\", which is actually just a means for the LLM to indicate when it is ready to return the final answer:\n",
        "\n",
        "*   **Finish:** Simple \"tool\" that the LLM uses to indicate when it is ready to return the final answer.\n",
        "\n",
        "For each tool, we provide a tool name, description, and usage example. These will all be included in the prompt that is shown to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j9Hwvs_TFO_"
      },
      "outputs": [],
      "source": [
        "python_sandbox_class = python_execution_safe_subset.PythonSandboxSafeSubset\n",
        "python_sandbox_factory = python_execution_safe_subset.PythonSandboxSafeSubsetFactory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JczX5xa3nm7"
      },
      "outputs": [],
      "source": [
        "# Python tool for executing a program in a Python sandbox.\n",
        "PYTHON_EXAMPLE = textwrap.dedent(\"\"\"\\\n",
        "  tool_code(\"1 + 1\") returns \"2\".\n",
        "  We can also run multiple lines of code like this:\n",
        "  ```tool_code\n",
        "  a = []\n",
        "  a.append(1)\n",
        "  a.append(2)\n",
        "  a\n",
        "  ```\n",
        "  returns [1, 2].\"\"\")\n",
        "\n",
        "# Here we show the simplest case of a stateless Python tool. If we don't need\n",
        "# to carry variable state over from one call to another, we can just create a\n",
        "# fresh sandbox on each invocation of the Python tool.\n",
        "async def run_stateless_python(request: str) -\u003e str:\n",
        "  temporary_sandbox = python_sandbox_class()\n",
        "  async with temporary_sandbox.start() as temporary_sandbox:\n",
        "    result = await temporary_sandbox.run(request)\n",
        "    return str(result)\n",
        "\n",
        "python_tool = llm_tool_use.Tool(\n",
        "    name='tool_code',\n",
        "    function=run_stateless_python,\n",
        "    description='Python interpreter. Can be used as a calculator or to execute any Python code. Returns the result of execution.',\n",
        "    example=PYTHON_EXAMPLE,\n",
        "    color='plum',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the open source environment, there are a number of commercial services available for accessing web search. If you do not have web search connectivity set up yet, you can start exploring the OneTwo agent strategies using the following simple mock Search tool that returns hard-coded responses. When using the agent for real, you can replace this with a function that calls a real search engine, or that retrieves relevant passages from an indexed corpus."
      ],
      "metadata": {
        "id": "TvlgOEYvXsb5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zlnAB9I3oe_"
      },
      "outputs": [],
      "source": [
        "def mock_search(query: str) -\u003e str:\n",
        "  response_by_query = {\n",
        "      'capital of France': 'Paris',\n",
        "      'population of Tuebingen': 'Tubingen 91,877 Population [2021]',\n",
        "      'population of Tubingen': 'Tubingen 91,877 Population [2021]',\n",
        "      'population Tuebingen': 'Tbingen 91,877 Population [2021]',\n",
        "      'population Tubingen': 'Tbingen 91,877 Population [2021]',\n",
        "      'population of Tuebingen 2023': 'Tubingen 91,877 Population [2021]',\n",
        "      'population of Zuerich': '402,762 (2017)',\n",
        "      'population of Zurich': '402,762 (2017)',\n",
        "      'population of Zrich': '402,762 (2017)',\n",
        "      'population Zuerich': '402,762 (2017)',\n",
        "      'population Zurich': '402,762 (2017)',\n",
        "      'population Zrich': '402,762 (2017)',\n",
        "      'population of Zuerich 2023': '402,762 (2017)',\n",
        "      'first president of the United States': 'George Washington',\n",
        "      'who was the first president of the United States?': 'George Washington',\n",
        "      'wife of George Washington': 'Martha Washington',\n",
        "      'who was the wife of George Washington?': 'Martha Washington',\n",
        "      'Frozen box office': '$1.280 billion',\n",
        "      'Frozen box office earnings': '$1.280 billion',\n",
        "      'Frozen box office gross': '$1.280 billion',\n",
        "      'Frozen movie box office': '$1.280 billion',\n",
        "      'Frozen movie box office earnings': '$1.280 billion',\n",
        "      'box office Frozen': '$1.280 billion',\n",
        "      'box office for Frozen': '$1.280 billion',\n",
        "      'box office of Frozen': '$1.280 billion',\n",
        "      'box office earnings of Frozen': '$1.280 billion',\n",
        "      'box office revenue Frozen': '$1.280 billion',\n",
        "      'how much did Frozen make at the box office?': '$1.280 billion',\n",
        "      'Lion King box office': '1.663 billion USD',\n",
        "      'Lion King box office earnings': '1.663 billion USD',\n",
        "      'Lion King box office gross': '1.663 billion USD',\n",
        "      'Lion King movie box office': '1.663 billion USD',\n",
        "      'Lion King movie box office earnings': '1.663 billion USD',\n",
        "      'The Lion King box office earnings': '1.663 billion USD',\n",
        "      'The Lion King 1994 box office earnings': '1.663 billion USD',\n",
        "      'box office Lion King': '1.663 billion USD',\n",
        "      'box office for Lion King': '1.663 billion USD',\n",
        "      'box office of Lion King': '1.663 billion USD',\n",
        "      'box office earnings of Lion King': '1.663 billion USD',\n",
        "      'box office revenue Lion King': '1.663 billion USD',\n",
        "      'how much did Lion King make at the box office?': '1.663 billion USD',\n",
        "      'Titanic box office': 'worldwide theatrical total = $2.264 billion',\n",
        "      'Titanic box office earnings': 'worldwide theatrical total = $2.264 billion',\n",
        "      'Titanic box office gross': 'worldwide theatrical total = $2.264 billion',\n",
        "      'Titanic movie box office': 'worldwide theatrical total = $2.264 billion',\n",
        "      'Titanic movie box office earnings': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office for Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office of Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office earnings of Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office revenue Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'how much did Titanic make at the box office?': 'worldwide theatrical total = $2.264 billion',\n",
        "      'what is the third largest city in Switzerland?': 'Basel',\n",
        "      'third largest city in Switzerland': 'Basel',\n",
        "      'what are the three largest cities in Switzerland?': 'Zurich, Geneva, Basel',\n",
        "      'largest cities in Switzerland': 'List of cities in Switzerland: Zrich  Geneva  Basel  Lausanne  Bern',\n",
        "      'how many puffins are left in the world?': 'Between them, there are over 14 million puffins in the world.',\n",
        "      'how many puffins are left in the world': 'Between them, there are over 14 million puffins in the world.',\n",
        "      'puffin population': 'Between them, there are over 14 million puffins in the world.',\n",
        "      'puffin population worldwide': 'Between them, there are over 14 million puffins in the world.',\n",
        "      'puffin population numbers': 'Between them, there are over 14 million puffins in the world.',\n",
        "      'puffin population estimate': 'Between them, there are over 14 million puffins in the world.',\n",
        "      'puffin population size': 'Between them, there are over 14 million puffins in the world.',\n",
        "      'puffin population 2023': 'Between them, there are over 14 million puffins in the world.',\n",
        "      'Atlantic puffin population': 'Europe, which holds more than 90% of the global population, is home to 4,770,0005,780,000 pairs (equalling 9,550,00011,600,000 adults).',\n",
        "      'How many Atlantic puffins are left in the world?': 'Europe, which holds more than 90% of the global population, is home to 4,770,0005,780,000 pairs (equalling 9,550,00011,600,000 adults).',\n",
        "      'bird with black and white body, orange beak and feet': 'puffin',\n",
        "      'bird with black back, white belly, colorful beak and orange feet': 'puffin',\n",
        "      'name of bird with black back, white belly, colorful beak and orange feet': 'puffin',\n",
        "      'name of sea bird with black back, white belly, colorful beak and orange feet': 'puffin',\n",
        "      'puffin bird with black back, white belly, colorful beak and orange feet': 'puffin',\n",
        "  }\n",
        "  # Normalize capitalization.\n",
        "  response_by_query = {k.lower(): v for k, v in response_by_query.items()}\n",
        "  query = query.lower()\n",
        "  return response_by_query.get(query, 'No results.')\n",
        "\n",
        "search_tool = llm_tool_use.Tool(\n",
        "    name='search',\n",
        "    function=mock_search,\n",
        "    description='Search engine. Returns a relevant snippet or answer to query.',\n",
        "    example=textwrap.dedent(\"search('capital of France')  # returns 'Paris'\"),\n",
        "    color='darkseagreen',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One example of integrating `Search` tools from other frameworks is leveraging the tool-use capabilities of the [Google Generative AI SDK](https://github.com/googleapis/python-genai).\n",
        "\n",
        "The `genai_types.Tool` with `google_search=genai_types.GoogleSearch()` configures the SDK's built-in Google Search tool. In this setup, the search capability is provided to an LLM call. OneTwo's `llm_tool_use.Tool` then wraps this entire LLM interaction, meaning the \"tool\" from the OneTwo agent's perspective is the LLM itself, which has been instructed to use the GenAI SDK's search functionality to answer the query."
      ],
      "metadata": {
        "id": "U_GN62XwXecx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%run_if model_selection.startswith('Gemini API')\n",
        "\n",
        "genai_search_tool = genai_types.Tool(google_search=genai_types.GoogleSearch())\n",
        "max_tokens = thinking_budget or 128\n",
        "chat_config_kwargs = {\n",
        "    'system_instruction': 'You are an expert in Google Search. Use the Search tool for every query the user provides.',\n",
        "    'tools': [genai_search_tool],\n",
        "    'max_tokens': max_tokens,\n",
        "}\n",
        "\n",
        "search_tool = llm_tool_use.Tool(\n",
        "    name='search',\n",
        "    function=lambda x: llm.instruct(x, **chat_config_kwargs),\n",
        "    description='Search engine. Returns a relevant snippet or answer to query.',\n",
        ")\n",
        "\n",
        "# Example\n",
        "print(ot.run(search_tool('capital of France')))"
      ],
      "metadata": {
        "id": "0KhFm1g1Xez9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QWTDoRa5q0X"
      },
      "outputs": [],
      "source": [
        "# The \"Finish\" function provides the LLM with a way of indicating when it is\n",
        "# ready to return a final answer. E.g., \"Finish('USA')\" returns 'USA'.\n",
        "finish_tool = llm_tool_use.Tool(\n",
        "    name='finish',\n",
        "    function=lambda x: x,\n",
        "    description='Function for returning the final answer.',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD03FWpki1dT"
      },
      "outputs": [],
      "source": [
        "react_tools = [python_tool, search_tool, finish_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjaoKmFdRRof"
      },
      "source": [
        "Finally, we create a couple of few-shot examples to demonstrate the use of these tools:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXxBnraASacp"
      },
      "outputs": [],
      "source": [
        "react_exemplars= react.default_react_exemplars(\n",
        "    python_tool_name='tool_code',\n",
        "    search_tool_name='search',\n",
        "    finish_tool_name='finish',\n",
        ")\n",
        "pprint.pprint(react_exemplars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa-Jv6Sh3Gpm"
      },
      "source": [
        "#### Invoke ReActAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNioHChyireA"
      },
      "source": [
        "Once we've set up the tool handler, constructing a `ReActAgent` and executing it on a question is just a matter of a few lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9HlNTj3YkEz"
      },
      "outputs": [],
      "source": [
        "react_agent = react.ReActAgent(\n",
        "    prompt=react.ReActPromptComposable(),\n",
        "    exemplars=react_exemplars,\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        tools=react_tools,\n",
        "    ),\n",
        "    max_steps=10,\n",
        "    stop_prefix='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "852PwsNyjSGE"
      },
      "source": [
        "In the simplest usage, we can treat the `ReActAgent` as a black box -- i.e., as just a function that takes a question as input and then returns the answer. We can do this quite literally, as the `Agent` class qualifies as a `Callable`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZZ3iBXpYkE0"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer = ot.run(react_agent(inputs=question))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um2O6qsY3RJX"
      },
      "source": [
        "#### Inspect ReActAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkoM9zRn6Y6Q"
      },
      "source": [
        "**Inspect steps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6o9rkU_jVL-"
      },
      "source": [
        "If we want to see what is going on under the hood, there are multiple ways to do this.\n",
        "\n",
        "One simple way is to specify `return_final_state=True` when calling the agent. When we do this, we now receive a `final_state` object as return value, alongside the `answer` itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BemcJB9r581N"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, final_state = ot.run(react_agent(inputs=question, return_final_state=True))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZqWwYatkMJc"
      },
      "source": [
        "If we print the final state, we can see the series of steps that the agent took in determining the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOeqdbWKuBSZ"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx-iD6oC6dPs"
      },
      "source": [
        "**Inspect detailed trace**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj4Qvz0XkYrL"
      },
      "source": [
        "For even more details, we can specify `enable_tracing=True` in the call to `ot.run` to receive a detailed execution trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf3TIhce6lYR"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, trace = ot.run(react_agent(inputs=question), enable_tracing=True)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9xtKbVBL9tG"
      },
      "source": [
        "If we render the execution trace, we can explore the full hierarchy of the prompting strategy, down to the exact series of requests that were sent to the LLM, along with the LLM's replies. (Try clicking on the stage names to expand/collapse.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soi784rGL9tH"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ9E2GIlZEEM"
      },
      "source": [
        "#### Multimodal ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs7x2IwgZJU_"
      },
      "source": [
        "You may have noticed that when constructing the ReAct agent, we specified `prompt=react.ReActPromptComposable()`. There are actually multiple ways of defining a ReAct prompt, and we provide a couple of different variants out-of-the-box, one defined using Jinja2 and another defined using composables. For most purposes, we recommend using the composables-based implementation, which provides the most flexibility in taking advantage of the full features of modern LLMs.\n",
        "\n",
        "One advantage in particular of using a composables-based prompt is that this allows the ReAct agent to natively accept multimodal inputs (provided we are using a compatible backend, as shown in the Multimodal section).\n",
        "\n",
        "Here is an example of applying the ReAct agent to a visual QA task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2sYkLxpZGSQ"
      },
      "outputs": [],
      "source": [
        "%%run_if model_supports_multimodal\n",
        "\n",
        "# This part is just for previewing the image in colab.\n",
        "image_name = 'paligemma_puffin'\n",
        "image_bytes = images[image_name]\n",
        "image_np = np.array(Image.open(io.BytesIO(image_bytes)))\n",
        "print(f'Image: {image_name}')\n",
        "print(f'Image license: {image_sources[image_name].license}')\n",
        "plt.imshow(image_np)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXE-8y5B6Idf"
      },
      "outputs": [],
      "source": [
        "%%run_if model_supports_multimodal\n",
        "\n",
        "question = content_lib.ChunkList([\n",
        "    content_lib.Chunk(images['paligemma_puffin']),\n",
        "    content_lib.Chunk('How many of these are there left in the world?'),\n",
        "])\n",
        "(answer, final_state), trace = ot.run(react_agent(inputs=question, return_final_state=True), enable_tracing=True)\n",
        "pprint.pprint(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0-5LXoElcbu"
      },
      "outputs": [],
      "source": [
        "%%run_if model_supports_multimodal\n",
        "\n",
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiwEK9VQlGE-"
      },
      "outputs": [],
      "source": [
        "%%run_if model_supports_multimodal\n",
        "\n",
        "IPython.display.HTML(ot.HTMLRenderer().render(trace))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIfWTPWR20ov"
      },
      "source": [
        "### Python Planning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUjphg0rpFth"
      },
      "source": [
        "The `PythonPlanningAgent` is inspired by various various research in Python-based tool orchestration, such as ViperGPT (https://arxiv.org/pdf/2303.08128.pdf) and AdaPlanner (https://arxiv.org/pdf/2305.16653.pdf).\n",
        "\n",
        "In this strategy, we present the LLM with a list of tool descriptions with invocation examples, and then iteratively prompt the LLM to output a sequence of steps, each of which consists of a Python code block, with \"thoughts\", where relevant, in the form of code comments. At each step, we execute the LLM-generated code in a Python sandbox that provides access to the relevant tools via predefined functions. We then take everything that the code writes to stdout, and we include that in the LLM prompt in the next step, similarly to how we did with the \"observation\" in the ReAct strategy.\n",
        "\n",
        "While the `ReActAgent` performs exactly one tool call in each step, the `PythonPlanningAgent` can potentially make multiple tool calls from a single code block, and can include other control structures like loops and if-statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poM2x5ZZ3X0u"
      },
      "source": [
        "#### Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ennQjA3iV-Ff"
      },
      "source": [
        "Similarly to what we did for ReAct, we will again start by configuring the tools that we want to make available to the `PythonPlanningAgent`.\n",
        "\n",
        "The way we register the tools is very similar to before. One thing you might have noticed in the syntax for the `ReActAgent` was that we specified the list of tools as part of a `PythonToolUseEnvironmentConfig`. While for `ReActAgent`, we were treating the `PythonToolUseEnvironment` as basically just a class that manages a set of tools and provides a uniform way to call them, the `PythonToolUseEnvironment` actually contains quite a bit more functionality than that, including functionality to allow the tools to be called from within a Python sandbox, and to create and manage Python sandboxes on demand. In the `PythonPlanningAgent`, we will use this full range of functionality, as we orchestrate the tool use via execution of blocks of Python code.\n",
        "\n",
        "Note that for security reasons, it is important to always use a well-protected sandbox when automatically executing code that was generated by an LLM, similarly to how you would avoid any unprotected automatic execution of code that was provided by an untrusted user. The main idea of the sandbox is that we don't want to allow the LLM-generated program to directly read/write files or directly perform RPCs or import arbitrary libraries. Instead, we will provide an explicit allow-list of libraries to be imported and functions that can be called, which will include all of the tools from the tool handler.\n",
        "\n",
        "In this case, we will register two tools:\n",
        "\n",
        "*   **search:** Tool for retrieving snippets from Google Search (same as in the ReAct example, and already defined above).\n",
        "*   **firstnumber:** Simple function for extracting the first number from a block of text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m96l9fj4Hu_"
      },
      "outputs": [],
      "source": [
        "# Define a tool called 'firstnumber' which is just a simple Python function that\n",
        "# we define here, for extracting the first number from a block of text.\n",
        "def firstnumber(x):\n",
        "  matches = re.match(r'[^\\d]*([\\d\\.,]+).*', str(x).replace(',', ''))\n",
        "  if matches:\n",
        "    try:\n",
        "      return float(matches.group(1))\n",
        "    except Exception as e:\n",
        "      return f'Error: could not parse {x} as a number ({e})'\n",
        "  else:\n",
        "    return f'Error: could not parse {x} as a number'\n",
        "\n",
        "first_number_tool = llm_tool_use.Tool(\n",
        "    name='firstnumber',\n",
        "    function=firstnumber,\n",
        "    description='Extracts the first number in a string.',\n",
        "    example=\"firstnumber('it is 1,203m high')  # return 1203.0 as a float\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDTeM0-vWGRX"
      },
      "outputs": [],
      "source": [
        "python_planning_tools = [search_tool, first_number_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh5LmoZq3cgT"
      },
      "source": [
        "#### Invoke PythonPlanningAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4hB6ztC8acK"
      },
      "source": [
        "Once we've set up the tool handler, constructing a `PythonPlanningAgent` and executing it on a question is again just a matter of a few lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSZlzLw2yFE5"
      },
      "outputs": [],
      "source": [
        "python_agent = python_planning.PythonPlanningAgent(\n",
        "    prompt=python_planning.PythonPlanningPromptComposable(),\n",
        "    exemplars=python_planning.DEFAULT_PYTHON_PLANNING_EXEMPLARS,\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        sandbox_factory=python_sandbox_factory,\n",
        "        tools=python_planning_tools,\n",
        "    ),\n",
        "    max_steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Nscop98kIB"
      },
      "source": [
        "In the simplest usage, we can again simply treat the `PythonPlanningAgent` as just a function that takes a question as input and then returns the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P_ilWpByFE6"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Titanic?'\n",
        "answer = ot.run(python_agent(inputs=question))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7tW0FNH3gGo"
      },
      "source": [
        "#### Inspect PythonPlanningAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kOqBQib8XIN"
      },
      "source": [
        "**Inspect steps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRjw-w2S8vvU"
      },
      "source": [
        "The same options that we saw for inspecting the intermediate steps of a `ReActAgent` are available for `PythonPlanningAgent` as well.\n",
        "\n",
        "In particular, if we  specify `return_final_state=True` when calling the agent, we receive a `final_state` object as return value, alongside the `answer` itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX6e8sgZ8fGP"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Titanic?'\n",
        "answer, final_state = ot.run(python_agent(inputs=question, return_final_state=True))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMlIWt7d9JWr"
      },
      "source": [
        "If we print the final state, we can see the series of steps that the agent took in determining the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GsjxWX1861z"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3ABClO78ZNE"
      },
      "source": [
        "**Inspect detailed trace**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHYyWcIQ9S-o"
      },
      "source": [
        "For even more details, we can again specify `enable_tracing=True` in the call to `ot.run` to receive a detailed execution trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iONYreoFDvF"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Titanic?'\n",
        "answer, trace = ot.run(python_agent(inputs=question), enable_tracing=True)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yk9oaT_9dbZ"
      },
      "source": [
        "If we render the execution trace, we can again see the hierarchy of the prompting strategy, including the exact series of requests that were sent to the LLM, along with the LLM's replies. Each of the tool calls also appears in the execution trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrRktccEBxlX"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer(levels_to_expand=1).render(trace))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Urbtlm84uMO"
      },
      "source": [
        "#### Multimodal PythonPlanning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldnmfYX544XD"
      },
      "outputs": [],
      "source": [
        "%%run_if model_supports_multimodal\n",
        "\n",
        "question = content_lib.ChunkList([\n",
        "    content_lib.Chunk(images['paligemma_puffin']),\n",
        "    content_lib.Chunk('How many of these are there left in the world?'),\n",
        "])\n",
        "(answer, final_state), trace = ot.run(python_agent(inputs=question, return_final_state=True), enable_tracing=True)\n",
        "pprint.pprint(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7AU2i1Z44XE"
      },
      "outputs": [],
      "source": [
        "%%run_if model_supports_multimodal\n",
        "\n",
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEJbmOM444XF"
      },
      "outputs": [],
      "source": [
        "%%run_if model_supports_multimodal\n",
        "\n",
        "IPython.display.HTML(ot.HTMLRenderer().render(trace))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J4sKi0COHm0"
      },
      "source": [
        "# Additional Agent Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFiLfkuYRzhb"
      },
      "source": [
        "One thing you may have noticed in the above examples is how similar the syntax is for interacting with `ReActAgent` and `PythonPlanningAgent`. That actually is not a coincidence! Both of these strategies have been implemented as subclasses of a generic `Agent` interface.\n",
        "\n",
        "Strategies that are implemented in this way support a number of additional operations out-of-the-box, in addition to what you saw above.\n",
        "\n",
        "We will illustrate some of these operations using the `ReActAgent` example from above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nISq_RQOKZH"
      },
      "source": [
        "## Stream states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3ISgVqrRFZt"
      },
      "source": [
        "For a long-running agent, rather than running monolothically, we may alternatively choose to stream the sequence of agent states, so that we can potentially save progress as we go along or apply our own dynamic criteria for stopping.\n",
        "\n",
        "We can do this using `Agent.stream_states`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlRBDVYsOedL"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "\n",
        "state_iterator = react_agent.start_environment_and_stream_states(\n",
        "    initial_state=state0)\n",
        "\n",
        "state_trajectory = []\n",
        "with ot.safe_stream(state_iterator) as state_stream:\n",
        "  for state in state_stream:\n",
        "    print('State arrived!')\n",
        "    state_trajectory.append(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN5z58UMSrWK"
      },
      "source": [
        "We end up receiving one state for each step that the agent performed -- in this case, 4 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tOLl8LnO095"
      },
      "outputs": [],
      "source": [
        "len(state_trajectory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIZU-gO9S71B"
      },
      "source": [
        "If we inspect the first state, we can see that it is of the same form as the final state that we saw earlier, except that the `updates` list contains only the first step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haJgNI-sOwPD"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(state_trajectory[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjnmoRwNTH4U"
      },
      "source": [
        "If we inspect the second state, we can see that the `updates` list now contains two steps. And so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h_g_8OSO7q_"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(state_trajectory[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxbPwShLQTSk"
      },
      "source": [
        "## Invoke prompt template directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_0UNyYETRHn"
      },
      "source": [
        "If you ever want to debug the behavior of the prompt template that is used by the agent internally (e.g., if you customize the prompt and are iterating on debugging), you can also execute the prompt template standalone, which can be done as follows, using any of the intermediate states from the state trajectory retrieved above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAvZdKlLzSzN"
      },
      "outputs": [],
      "source": [
        "type(react_agent.environment_config.tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvWPnl_3yoF9"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(react_agent.environment_config.tools, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CIzMzcQYT0"
      },
      "outputs": [],
      "source": [
        "prompt_outputs, trace = ot.run(react_agent.prompt(\n",
        "        # These arguments are just imitating what is done in the `ReActAgent`\n",
        "        # implementation (more or less copy-pasted from `react.py`).\n",
        "        tools=react_agent.environment_config.tools,\n",
        "        exemplars=react_agent.exemplars,\n",
        "        stop_prefix=react_agent.stop_prefix,\n",
        "        stop_sequences=react_agent._get_stop_sequences(),\n",
        "        force_finish=False,\n",
        "        # Here we can manually specify any of the intermediate states from the\n",
        "        # state trajectory above to reproduce the behavior of the prompt\n",
        "        # template at that step.\n",
        "        state=state_trajectory[1],\n",
        "    ),\n",
        "    enable_tracing=True)\n",
        "pprint.pprint(prompt_outputs, width=160)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS1UfsT1UqJQ"
      },
      "source": [
        "If we want to see the precise prompt that was sent to the LLM, we can render the detailed execution trace of the prompt template, in the same way we did earlier for the agent strategy as a whole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OeX9DTLjsiB"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer(levels_to_expand=1).render(trace))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzNqGKcxOOF8"
      },
      "source": [
        "## Stream updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBmv7bMcXpWD"
      },
      "source": [
        "Similarly to how we produced a stream of agent states using `Agent.stream_states`, we can alternatively produce a stream of state updates using `Agent.stream_updates`. The idea is very similar, except that each agent update contains just the new information that needs to be added to the previous state to create the new state. In the case of `ReActAgent`, the state update is represented as a `ReActStep` (the same data structure that we saw inside of the `final_state` earlier, for representing the individual steps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yj5CPggWRFl"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "\n",
        "# Request a stream of updates.\n",
        "update_iterator = react_agent.start_environment_and_stream_updates(\n",
        "    initial_state=state0)\n",
        "\n",
        "# Two things we could do with these updates:\n",
        "# (A) Gather them in a list / work with them directly.\n",
        "# (B) Add them to a previous state to create the next state.\n",
        "updates = []\n",
        "current_state = copy.deepcopy(state0)\n",
        "with ot.safe_stream(update_iterator) as state_stream:\n",
        "  for update in state_stream:\n",
        "    print('Update arrived!')\n",
        "    updates.append(update)\n",
        "    # Agent states can be updated using `+=` with a state update.\n",
        "    current_state += update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE44WxG2Yl8P"
      },
      "source": [
        "If we look at the list of updates, we can see that each update is one `ReActStep`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_khaEA8WwPz"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(updates, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txPC-CrsYu3j"
      },
      "source": [
        "By incrementally updating an initial state with each of the updates using `+=`, we can also reproduce the current state at any given step. Now that we have processed the full update stream, we can see that `current_state` is exactly the same as the `final_state` that we observed earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiOrlNS_W7go"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(current_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcCA2iTOOQ6a"
      },
      "source": [
        "## Stop / edit / resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNE0AmYFZS-C"
      },
      "source": [
        "Since the behavior of the agent at each step is fully determined by the contents of the agent state, we are free to directly manipulate any of these state objects and pass them back into the agent to see what the agent would have done in that scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJqUuGdRZ-go"
      },
      "source": [
        "As an example, let's use `stream_states` for just 2 steps and temporarily stop execution there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTQ6Ey6GOlOy"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "state_iterator = react_agent.start_environment_and_stream_states(\n",
        "    initial_state=state0,\n",
        "    max_steps=2)\n",
        "with ot.safe_stream(state_iterator) as state_stream:\n",
        "  state_trajectory = list(state_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sr151AxaKsF"
      },
      "source": [
        "At this point, we can see that the agent had just finished calling the `Search` tool to get the `population of Zuerich`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbT57_VNU6q9"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(state_trajectory[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM70KMHHaY5Z"
      },
      "source": [
        "Let's try modifying the `observation` from the last step to see what would have happened if Google Search had returned a different snippet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwMfNI90VCoE"
      },
      "outputs": [],
      "source": [
        "state_trajectory[1].updates[-1].observation = 'Population of Zurich: 5 million people and growing!'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68xAaVaJas6M"
      },
      "source": [
        "Now let's resume execution from this modified state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvEVO925VhC2"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, final_state = ot.run(react_agent(\n",
        "    inputs=question,\n",
        "    initial_state=state_trajectory[1],\n",
        "    return_final_state=True))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJgXKk8Vaybz"
      },
      "source": [
        "If we look at the final trajectory, we can see the modified internal state, as well as the new sequence of steps the agent would have taken after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm3G7wlkV9vT"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na0I3zpcrnlQ"
      },
      "source": [
        "## Customize exemplars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo5JIVYJrqLh"
      },
      "source": [
        "One thing you might have noticed when we were instantiating the `ReActAgent` and `PythonPlanningAgent` was that we needed to provide a list of exemplars.\n",
        "\n",
        "```python\n",
        "react_agent = react.ReActAgent(\n",
        "    prompt=react.ReActPromptComposable(),\n",
        "    exemplars=react_exemplars,          # \u003c== Exemplars\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        tools=react_tools,\n",
        "    ),\n",
        "    max_steps=10,\n",
        "    stop_prefix='')\n",
        "```\n",
        "\n",
        "```python\n",
        "python_agent = python_planning.PythonPlanningAgent(\n",
        "    prompt=python_planning.PythonPlanningPromptComposable(),\n",
        "    exemplars=python_planning.DEFAULT_PYTHON_PLANNING_EXEMPLARS,         # \u003c== Exemplars\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        sandbox_factory=python_sandbox_factory,\n",
        "        tools=python_planning_tools,\n",
        "    ),\n",
        "    max_steps=10)\n",
        "```\n",
        "\n",
        "So far we have just used a predefined default list of exemplars, which were compatible with the set of tools that we had configured. In practice, though, you may want to customize the list of exemplars, or even select them dynamically from some kind of larger exemplar pool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZp7JTfhs4EH"
      },
      "source": [
        "Let's take a look at the exemplars that we have been using so far for `ReActAgent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbxaLlb5s-Oo"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(react_agent.exemplars, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtgyUcNutMnW"
      },
      "source": [
        "One thing you might have noticed is that the format of these exemplars looks very similar to the `final_state` that is output when we execute the agent with `return_final_state = True`.\n",
        "\n",
        "This is again not a coincidence, and you can indeed directly reuse any state object output from a past run of the agent strategy as an exemplar in future runs, or you can construct them fully manually, or semi-automatically via the \"stop / edit / resume\" workflow shown earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36klZfYouRPq"
      },
      "source": [
        "Let's try harvesting a couple of trajectories output by our current `react_agent` and use those as exemplars in a new agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrtMoeL4ucdb"
      },
      "outputs": [],
      "source": [
        "q1 = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, final_state_q1 = ot.run(react_agent(inputs=q1, return_final_state=True))\n",
        "answer\n",
        "# pprint.pprint(final_state_q1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAlRgUvLw5MS"
      },
      "outputs": [],
      "source": [
        "q2 = 'Who was the wife of the first president of the United States?'\n",
        "answer, final_state_q2 = ot.run(react_agent(inputs=q2, return_final_state=True))\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLJXbk-OwU4t"
      },
      "outputs": [],
      "source": [
        "react_agent2 = react.ReActAgent(\n",
        "    prompt=react.ReActPromptComposable(),\n",
        "    exemplars=[final_state_q1, final_state_q2],\n",
        "    environment_config=react_agent.environment_config,\n",
        "    max_steps=10,\n",
        "    stop_prefix='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEJ_2H8BzAPr"
      },
      "source": [
        "If we look at the exemplars of the new agent, we can see that these indeed correspond to the trajectories from the two questions that we presented to the first agent above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veSmLJtnuxU5"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(react_agent2.exemplars, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUAW8TYhzOso"
      },
      "source": [
        "Now let's try the new agent on a new question and see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQEF42dLudJ6"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Lion King?'\n",
        "(answer, final_state), trace = ot.run(react_agent2(inputs=question, return_final_state=True), enable_tracing=True)\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdmFmd1DxnCj"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq_3bIt-zSgb"
      },
      "source": [
        "If we look at the prompt that `react_agent2` sent to the LLM, we can see that the exemplars were now the ones that we specified, which were bootstrapped from the original `react_agent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CZVMOa-mmLW"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace.get_leaf_results()[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAqxD4A7dWT8"
      },
      "source": [
        "## Run step-by-step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WB-yqzWm0L6"
      },
      "source": [
        "As an alternative to iterating through a stream of updates, we can also run individual steps of the agent interactively using `Agent.sample_next_step`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx6arXo8diEU"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "env = python_tool_use.PythonToolUseEnvironment(config=config)\n",
        "\n",
        "# Since we will perform multiple operations on the same environment\n",
        "# actively, we start the environment manually here, rather than wrapping\n",
        "# everything in a context manager. (We will call `stop` manually later.)\n",
        "env = ot.run(env.start_unsafe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s4qStvGdsGb"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "\n",
        "# Sample a single candidate for step 1).\n",
        "next_step_candidates = ot.run(\n",
        "    react_agent.sample_next_step(\n",
        "        state=state0, num_candidates=1, environment=env\n",
        "    )\n",
        ")\n",
        "pprint.pprint(next_step_candidates, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2e_XUHdfd24"
      },
      "outputs": [],
      "source": [
        "update1 = next_step_candidates[0]\n",
        "state1 = state0 + next_step_candidates[0]\n",
        "\n",
        "# Sample a single candidate for step 2.\n",
        "next_step_candidates = ot.run(\n",
        "    react_agent.sample_next_step(\n",
        "        state=state1, num_candidates=1, environment=env\n",
        "    )\n",
        ")\n",
        "pprint.pprint(next_step_candidates, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWA_aVVMwNHA"
      },
      "outputs": [],
      "source": [
        "# We need to stop the environment manually, since we started it manually\n",
        "# earlier.\n",
        "env.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dhXcl0GiM23"
      },
      "source": [
        "## Sample multiple candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAG7K28Iv_ll"
      },
      "source": [
        "You may have noticed that when we called `sample_next_step` above, rather than returning just a single state update, it returned a list of state updates (in this case, a list of length 1).\n",
        "\n",
        "If we specify a value of `num_candidates` greater than 1, we can generate multiple candidate next steps.\n",
        "\n",
        "This is how you would interact with the agent, for example, if you wanted to implement a higher-level strategy on top of it such as beam search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCWhBBlZiUNf"
      },
      "outputs": [],
      "source": [
        "ot.run(env.start_unsafe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz7KXabKiWVB"
      },
      "outputs": [],
      "source": [
        "question = 'What is the population of the third largest city in Switzerland?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdEnbZO83MZ1"
      },
      "outputs": [],
      "source": [
        "# Since `register_and_get_wrapper` makes a full copy of the current\n",
        "# function registry, we make a new wrapper now for `_backend_temp_0_7` to\n",
        "# include the `GSearchEngine` that was registered in the previous section.\n",
        "on_backend_temp_0_7_with_search = register_and_get_wrapper(\n",
        "    backend,\n",
        "    generate_text_kwargs={'temperature': 0.7},\n",
        "    chat_kwargs={'temperature': 0.7},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z08Z_URYhTtX"
      },
      "outputs": [],
      "source": [
        "# Sample three candidates for step 1.\n",
        "# Note that we use a model with temperature \u003e 0, so that we can sample multiple\n",
        "# distinct candidates.\n",
        "next_step_candidates = ot.run(\n",
        "    on_backend_temp_0_7_with_search(\n",
        "        react_agent.sample_next_step(\n",
        "            state=state0, num_candidates=3, environment=env\n",
        "        )\n",
        "    )\n",
        ")\n",
        "pprint.pprint(next_step_candidates, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJWWtFtvdnKL"
      },
      "outputs": [],
      "source": [
        "env.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bo3VABFxCtD"
      },
      "source": [
        "Notice that `next_step_candidates` this time was a list containing three different candidate `ReActStep` objects, all being candidates for the same step (Step 1 in this case)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtfBYHvf02dc"
      },
      "source": [
        "# PythonToolUseEnvironment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx5SCBFB1JBX"
      },
      "source": [
        "Although agents in OneTwo don't necessarily need to involve tool use, the two agents shown so far (`ReActAgent` and `PythonPlanningAgent`) both did take advantage of tool use and/or code execution capabilities, which were provided by a shared `PythonToolUseEnvironment` class.\n",
        "\n",
        "In this section, we will illustrate some additional operations that can be performed using the `PythonToolUseEnvironment`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz6t09Z0ri4v"
      },
      "source": [
        "## Invoke tools directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90u8RwzlrnH0"
      },
      "source": [
        "If you ever want to debug the behavior of the tools that are called by the agent internally, you can execute the tools standalone as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03moJ_f3tgan"
      },
      "source": [
        "First let's try executing the `Search` tool that we registered earlier with the `ReActAgent`. We can do this using `PythonToolUseEnvironment.run_tool`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3p2FjuOsS_N"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_tool(tool_name='search', tool_args=['capital of France'], tool_kwargs={})\n",
        "  )\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3nCdvRctnk9"
      },
      "source": [
        "Now let's try executing the `Python` tool from the same `ReActAgent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTgvg459s6iR"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_tool(tool_name='tool_code', tool_args=['8849 - 8611'], tool_kwargs={})\n",
        "  )\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mEQx5Gmu2bb"
      },
      "source": [
        "Of course, we can similarly execute the tools that we registered for use with the `PythonPlanningAgent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdabyPpPuKrD"
      },
      "outputs": [],
      "source": [
        "config = python_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_tool(\n",
        "          tool_name='firstnumber',\n",
        "          tool_args=['Tubingen 91,877 Population [2021]  Estimate 108.1 km2'],\n",
        "          tool_kwargs={},\n",
        "      )\n",
        "  )\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO_oNVzw0Guc"
      },
      "source": [
        "## Run code directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHoecBVVxF6N"
      },
      "source": [
        "To spawn a Python sandbox and run a block of code in it (potentially including calls to tools) the way that is done in `PythonPlanningAgent`, we can use `PythonToolUseEnvironment.run_code`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYP2hmTNu-7P"
      },
      "outputs": [],
      "source": [
        "config = python_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_code(\n",
        "          sandbox_state=tuple(),\n",
        "          code=textwrap.dedent(\"\"\"\\\n",
        "          search_result = search('population of Tubingen')\n",
        "          print('Search result: %s' % search_result)\n",
        "          population = firstnumber(search_result)\n",
        "          population\n",
        "          \"\"\"),\n",
        "      )\n",
        "  )\n",
        "pprint.pprint(result, width=160)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42gDq_8CxpB9"
      },
      "source": [
        "Note that `PythonToolUseEnvironment.run_code` is somewhat similar to running the \"Python tool\" from `ReActAgent`, but is more powerful, since the executed Python code can include calls to any of the tools registered in the `PythonToolUseEnvironment` (for example, the \"search\" and \"firstnumber\" tools invoked in the code snippet above). Note also that while the \"Python tool\" returns just a single string as its output, `run_code` returns a much more detailed `RunCodeResult` object, which contains, among other things, both the value of the final expression (in this case, the value of `population`) and all content that was written to `stdout`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc800ur92Dz2"
      },
      "source": [
        "One thing you may have noticed above is the `sandbox_state` parameter, which we set simply to an empty `tuple()`. The tuple provided in `sandbox_state` represents the sequence of code blocks that we expect to have been executed in the Python sandbox so far. In this case, we requested that the code be run in a fresh sandbox (i.e., one in which no code has been executed yet). For achieving the effect of a stateful sandbox, however, we can also specify a non-empty sequence of code blocks as the `sandbox_state`, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEizEIft0Uqa"
      },
      "outputs": [],
      "source": [
        "config = python_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_code(\n",
        "          sandbox_state=(\n",
        "              \"search_result = 'Tubingen 91,877'\",\n",
        "          ),\n",
        "          code='firstnumber(search_result)',\n",
        "      )\n",
        "  )\n",
        "result.sandbox_result.final_expression_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcNohQjG3Vph"
      },
      "source": [
        "Notice that the code succeeded in running even though it included a reference to a variable `search_result` that was not directly defined in the given code block, since it was defined in one of the code blocks from the provided `sandbox_state`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrgHBKs64fTg"
      },
      "source": [
        "Notice also that the above `run_code` request succeeded even though we had never actually previously run the precise code block `\"search_result = 'Tubingen 91,877'\"` on this `PythonToolUseEnvironment` instance up till now. In cases like this, `PythonToolUseEnvironment` will automatically reconstruct Python sandbox instances on demand that match the requested state. This is convenient, for example, if you have a long-running `PythonPlanningAgent` strategy for which you would like to serialize the intermediate state, and then later deserialize the state and continue execution in a separate process, or if you were to apply a branching strategy like beam search over top of the `PythonPlanningAgent` trajectories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlY6w7Vf7GN_"
      },
      "source": [
        "In the simple case where you want to just execute a series of code blocks in a single stateful Python sandbox in a single process, you can simply pass in the list of code blocks executed so far in each call to `PythonToolUseEnvironment.run_code`, and the environment will automatically reuse the same sandbox for the whole series of calls, to avoid any unnecessary duplicate code execution.\n",
        "\n",
        "Here is an example of what that could look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjhmJUyY7JFK"
      },
      "outputs": [],
      "source": [
        "config = python_agent.environment_config\n",
        "code_blocks_to_execute = [\n",
        "    \"search_result = 'Tbingen 91,877'\",\n",
        "    'firstnumber(search_result)',\n",
        "]\n",
        "code_blocks_executed_so_far = []\n",
        "result_list = []\n",
        "\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  for code in code_blocks_to_execute:\n",
        "    result = ot.run(\n",
        "        env.run_code(\n",
        "            sandbox_state=tuple(code_blocks_executed_so_far),\n",
        "            code=code,\n",
        "        )\n",
        "    )\n",
        "    result_list.append(result)\n",
        "    code_blocks_executed_so_far.append(code)\n",
        "\n",
        "pprint.pprint(result_list, width=160)"
      ]
    },
    {
      "metadata": {
        "id": "g86LyZG11lhB"
      },
      "cell_type": "markdown",
      "source": [
        "## Select a sandbox"
      ]
    },
    {
      "metadata": {
        "id": "ez72CG121oIw"
      },
      "cell_type": "markdown",
      "source": [
        "In the `PythonToolUseEnvironmentConfig`, the `sandbox_factory` attribute controls which type of `PythonSandbox` to construct when handling `run_code` requests.\n",
        "\n",
        "There are two different implementations of the PythonSandbox interface in OneTwo.\n",
        "\n",
        "* **PythonSandboxSafeSubset:** Simplest implementation, which runs in entirely within the same thread and process as the calling code. Rather than isolating the Python code execution in a separate sandboxed process, this implementation ensures safety by limiting the subset of Python programs that can be evaluated, and by evaluating the code via a custom implementation, rather than via the native `eval` function. Unlike the other two sandbox implementations below, this `PythonSandboxSafeSubset` only supports a limited subset of the Python language. The subset that it supports is sufficient for many purposes, however, and due to the lack of external dependencies, this is the most portable implementation.\n",
        "* **PythonSandboxSafeSubsetMultiProcess:** This implementation wraps `PythonSandboxSafeSubset` and runs the code execution in a separate, persistent process using Python's standard `multiprocessing` library. This approach provides a more robustly isolated environment, crucial for scenarios involving parallel execution. This multi-process approach offers significant benefits, including true parallelism by bypassing GIL limitations for CPU-bound tasks within the sandboxed code when running multiple sandbox instances. More importantly, it provides improved isolation, protecting the main process from crashes, hangs, or resource exhaustion within any single sandbox instance. This isolation is essential for safety when executing LLM-generated code, especially in parallel. Note that starting a new process within a Colab environment takes time and make `PythonSandboxSafeSubsetMultiProcess` feel slower than `PythonSandboxSafeSubset` for single, interactive runs.\n",
        "\n",
        "**When to Use Which Sandbox:**\n",
        "\n",
        "*   **PythonSandboxSafeSubsetMultiProcess:** You should **always** use `PythonSandboxSafeSubsetMultiProcess` whenever the sandbox might be used in a strategy running in parallel across multiple examples in a dataset, or which may internally involve some kind of fan-out. This ensures true isolation and prevents state leakage or crashes in one execution from affecting others.\n",
        "\n",
        "*   **PythonSandboxSafeSubset:** You can optionally use `PythonSandboxSafeSubset` directly *only* when running interactively in Colab on just a single example at a time. It can be faster for these single interactive cases due to lower startup overhead.\n"
      ]
    },
    {
      "metadata": {
        "id": "FkvgCrYB3uoU"
      },
      "cell_type": "markdown",
      "source": [
        "Here is an example of running code in `PythonToolUseEnvironment` using each of the two implementations."
      ]
    },
    {
      "metadata": {
        "id": "fx9sgYrY1m5Z"
      },
      "cell_type": "code",
      "source": [
        "for sandbox_factory in [\n",
        "    python_execution_safe_subset.PythonSandboxSafeSubsetFactory,\n",
        "    python_execution_safe_subset.PythonSandboxSafeSubsetMultiProcessFactory\n",
        "]:\n",
        "  config = python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "      sandbox_factory=sandbox_factory(),\n",
        "      tools=python_planning_tools,\n",
        "  )\n",
        "  with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "    result = ot.run(\n",
        "        env.run_code(\n",
        "            sandbox_state=(\n",
        "                \"search_result = 'Tubingen 91,877'\",\n",
        "            ),\n",
        "            code='firstnumber(search_result)',\n",
        "        )\n",
        "    )\n",
        "  print(sandbox_factory.__name__, \"-\u003e\", result.sandbox_result.final_expression_value)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTaLMNXSp0Nw"
      },
      "source": [
        "## Start environment manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IEDAPslshrj"
      },
      "source": [
        "When using a `PythonToolUseEnvironment`, we need to always \"start\" it first and then \"stop\" it onces we are done, so as to ensure that any background threads or other resources get cleaned up.\n",
        "\n",
        "As you may have noticed in the examples above, there are two different syntaxes available for starting and stopping the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVZozTtEtFG5"
      },
      "source": [
        "When implementing a prompting strategy for real, the recommended approach is to start the environment via a context manager, as shown below. In this approach, the environment is started when it is enters the context and then is automatically stopped when exiting the context. This is the \"safest\" syntax to use, as it ensures that the environment is always cleaned up at the end, even in cases where execution is interrupted due to an exception."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCVavsOqrEd0"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "# The environment will start when entering the context and stop when exiting.\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(env.run_code(sandbox_state=tuple(), code='x = 1'))\n",
        "  result = ot.run(env.run_code(sandbox_state=('x = 1',), code='x * 2'))\n",
        "result.sandbox_result.final_expression_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brtFxXM9nbMJ"
      },
      "source": [
        "In cases where we will be performing a series of interactive operations on the same `PythonToolUseEnvironment` instance, though, it can be convenient to simply start the environment once at the beginning, and then just leaving it running indefinitely until we choose to manually stop it later, as shown below.\n",
        "\n",
        "Note that this approach is \"unsafe\" in the sense that if we forget to call `env.stop()` (or if that line fails to run due to an exception raised in an earlier part of the code), we could be left with some threads left running in the background, which would be undesirable in a long-running process. For the purposes of experimenting manually in colab, though, this approach is fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoRf89zxsJX6"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "env = python_tool_use.PythonToolUseEnvironment(config=config)\n",
        "\n",
        "# Here we start the environment manually.\n",
        "ot.run(env.start_unsafe())\n",
        "\n",
        "result = ot.run(env.run_code(sandbox_state=tuple(), code='x = 1'))\n",
        "result = ot.run(env.run_code(sandbox_state=('x = 1',), code='x * 2'))\n",
        "\n",
        "# Here we stop the environment manually.\n",
        "# (This could also be done in a separate cell.)\n",
        "env.stop()\n",
        "\n",
        "result.sandbox_result.final_expression_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-VJtIUNS0PC"
      },
      "source": [
        "# Agent Evals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk7w_Qzikw2v"
      },
      "source": [
        "For evaluating an agent strategy over a dataset, one option is to follow the same approach illustrated earlier in the \"Evals\" section, where the general-purpose `evaluation.evaluate` script provides a loose structure for the eval run, but where you define for yourself the format in which to output the evaluation results.\n",
        "\n",
        "As an alternative, though, we also provide an `agent_evaluation.evaluate` script, which is designed to be easy to use with agent strategies, and which automatically generates various detailed debug information out-of-the-box in a standardized format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCCWbuanm6He"
      },
      "source": [
        "In both cases, we would start by constructing a dataset consisting of a list or stream of examples, where each example is represented as a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtjACM5kTX_g"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    {'question': 'What is the total population of Tuebingen and Zuerich?',\n",
        "     'answer': 512392},\n",
        "    {'question': 'Which movie had the larger box office: Frozen or Titanic?',\n",
        "     'answer': 'Titanic'},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om74PtcSnUw0"
      },
      "source": [
        "When using the `agent_evaluation` script, we define our metric functions following the function signature shown below, where the function takes as input a target and prediction and returns a float value.\n",
        "\n",
        "In this example, we are providing just a simple, ordinary function, but it could also be an `async` function, e.g., for a metric based on an AI rater."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrS6UW9ZmIG"
      },
      "outputs": [],
      "source": [
        "def number_aware_accuracy(target: str | float | int, prediction: str) -\u003e float:\n",
        "  if isinstance(target, float) or isinstance(target, int):\n",
        "    # Numerical comparison: Ignore thousands separators, handle rounding error.\n",
        "    try:\n",
        "      prediction_as_float = float(prediction.replace(',', ''))\n",
        "      correct = (round(target - prediction_as_float, 8) == 0.0)\n",
        "    except ValueError:\n",
        "      correct = False\n",
        "  else:\n",
        "    # String comparison.\n",
        "    correct = (target == prediction)\n",
        "  return float(correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rah0K6TRodf-"
      },
      "source": [
        "You can then run the eval as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4swk9FMUJNF"
      },
      "outputs": [],
      "source": [
        "react_summary = agent_evaluation.evaluate(\n",
        "    strategy=react_agent,\n",
        "    examples=dataset,\n",
        "    metric_functions={'accuracy': number_aware_accuracy},\n",
        "    output_results=True,\n",
        "    output_results_debug=True,\n",
        "    output_final_states=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga51ECD0piD6"
      },
      "outputs": [],
      "source": [
        "print(f'\\nTime elapsed (seconds): {react_summary.timing.time_elapsed.total_seconds()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXImio46ptFe"
      },
      "source": [
        "The returned object is an `EvaluationSummary` object, which summarizes the aggregated eval results (metrics, counters, timing), along with various optional details for debugging purposes:\n",
        "* **results:** Brief summary of each example: Inputs, outputs, metric, etc.\n",
        "* **results_debug:** Detailed trace of each example.\n",
        "* **final_states:** Final state of the agent for each example (only relevant when the strategy is an agent).\n",
        "\n",
        "Try clicking on the various elements in the hierarchies to expand/collapse!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_JccsYZUUTW"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(react_summary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "081MOFNp3cpZ"
      },
      "source": [
        "# Self-Consistency Variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYyYOF753jZU"
      },
      "source": [
        "Here we illustrate additional variants of the self-consistency strategy that appeared in the original self-consistency paper ([Wang, et al., 2023](https://arxiv.org/pdf/2203.11171)) or in follow-up research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW_sog79pDaG"
      },
      "outputs": [],
      "source": [
        "# For purposes of comparison, we will evaluate each of the variants on the\n",
        "# following question (same one used in the earlier ReAct section).\n",
        "question = 'What is the total population of Tuebingen and Zuerich?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mOnz3qT5Hj9"
      },
      "source": [
        "## SC with CoT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3bBUkanKu8A"
      },
      "source": [
        "Self-consistency with chain-of-thought is the most basic/traditional usage of self-consistency, as described in [Wang, et al., 2023](https://arxiv.org/pdf/2203.11171) and in the presentation of self-consistency in the Overview section.\n",
        "\n",
        "As a reminder, this is what it looks like to initialize this strategy, using the zero-shot two-step variants of chain-of-thought."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq8vhJGxrerO"
      },
      "outputs": [],
      "source": [
        "# Optional instruction to encourage the model to actually attempt to answer the\n",
        "# question, rather than giving a cautious response like \"Cannot determine\" or\n",
        "# \"I do not have access to real-time information\", etc., as some models tend to\n",
        "# do when presented with a question that requires up-to-date knowledge.\n",
        "cot_qa_instruction = \"\"\"\\\n",
        "Answer the question. Give a correct answer if you know it, or otherwise just \\\n",
        "guess and make up some plausible-sounding reasoning.\"\"\"\n",
        "\n",
        "cot_sc = self_consistency.SelfConsistency(\n",
        "    sampler=sampling.Repeated(chain_of_thought.QACoTPromptChat(\n",
        "        instruction=cot_qa_instruction,\n",
        "    )),\n",
        "    bucketizer=lambda x: x.answer,  # Use the final answer as the bucket.\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjH6g60KLd53"
      },
      "source": [
        "And here is the output of running this strategy on our question of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43px78K19wjB"
      },
      "outputs": [],
      "source": [
        "answer_distribution_cot_sc, trace_cot_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_sc(question)), enable_tracing=True)\n",
        "\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_cot_sc):\n",
        "  cot_reply = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': round(probability, 2),\n",
        "      'Answer': cot_reply.answer,\n",
        "      'Representative Reasoning': cot_reply.reasoning,\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN-Q5yIc9wjC"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer().render(trace_cot_sc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJwk1Gba5UFJ"
      },
      "source": [
        "## SC with ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQp2vwYiMIho"
      },
      "source": [
        "Aside from chain-of-thought, we can just as easily apply self-consistency to arbitrarily complex prompting strategies, such as `ReActAgent`, as long as there is some notion of \"intermediate steps\" to marginalize over, and some notion of \"final answer\" to vote on, and where many different sequences of intermediate steps could potentially lead to the same final answer. In the case of `ReActAgent`, the natural choice is to use the agent's final state (i.e., the trajectory of thoughts/actions that were taken by the agent) as the intermediate steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BouhMwsngKSL"
      },
      "outputs": [],
      "source": [
        "react_agent = react.ReActAgent(\n",
        "    prompt=react.ReActPromptComposable(),\n",
        "    exemplars=react_exemplars,\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        tools=react_tools,\n",
        "    ),\n",
        "    max_steps=5,\n",
        "    stop_prefix='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J30Bt4sr5-J3"
      },
      "outputs": [],
      "source": [
        "react_sc = self_consistency.SelfConsistency(\n",
        "    # We use `return_final_state=True` to make it easy to inspect/evaluate a\n",
        "    # representative reasoning trajectory along with the final answer, the same\n",
        "    # way we did for chain-of-thought.\n",
        "    sampler=sampling.Repeated(\n",
        "        functools.partial(react_agent, return_final_state=True)),\n",
        "    bucketizer=lambda x: x[0],  # Use the final answer as the bucket.\n",
        "    num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF6LDOZB6USI"
      },
      "outputs": [],
      "source": [
        "answer_distribution_react_sc, trace_react_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(react_sc(question)), enable_tracing=True)\n",
        "\n",
        "# Let's summarize the results in a table in the same format as before.\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_react_sc):\n",
        "  answer, final_state = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': round(probability, 2),\n",
        "      'Answer': answer,\n",
        "      'Representative Reasoning': final_state,\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "\n",
        "answer_distribution_summary_data_df\n",
        "\n",
        "# IPython.display.HTML(ot.HTMLRenderer(levels_to_expand=2).render(trace_react_sc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKNqAHt01vSp"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer(levels_to_expand=2).render(trace_react_sc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypeo2BX55atr"
      },
      "source": [
        "## SC over diverse exemplars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPLVJc_zOU6H"
      },
      "source": [
        "Self-consistency is typically thought of as a type of \"rationale-augmented ensemble\" [[Wang, et al., 2022]](https://arxiv.org/pdf/2207.00747). As an ensembling method, its effectiveness depends crucially on the diversity and independence of the samples that are being aggregated over. The less diversity in the samples, the more quickly the consensus accuracy will tend to plateau.\n",
        "\n",
        "One approach that is sometimes taken for increasing the diversity of the samples is to draw the samples from multiple different variations of the underlying strategy. In particular, one approach that was adopted in [[Li, et al., 2023]](https://arxiv.org/pdf/2206.02336) is to use the same basic underlying prompting strategy (e.g., the same basic prompt template), but with different choices of few-shot exemplars.\n",
        "\n",
        "We can do this easily using the `RoundRobin` sampler, which takes a list of other samplers as input and then does round-robin over them.\n",
        "\n",
        "The resulting answer distribution is in the same format as before, but with more diversity of reasoning paths (and in this case, of answers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzQ_P2EWCppX"
      },
      "outputs": [],
      "source": [
        "cot_exemplars = chain_of_thought.QA_COT_EXEMPLARS_ORIGINAL_MATH_WORD_PROBLEMS\n",
        "cot_over_diverse_exemplars_sampler = sampling.RoundRobin([\n",
        "    sampling.Repeated(chain_of_thought.QACoTPromptChat(\n",
        "        instruction=cot_qa_instruction)),\n",
        "    sampling.Repeated(chain_of_thought.QACoTPromptChat(\n",
        "        instruction=cot_qa_instruction, exemplars=cot_exemplars[0:4])),\n",
        "    sampling.Repeated(chain_of_thought.QACoTPromptChat(\n",
        "        instruction=cot_qa_instruction, exemplars=cot_exemplars[4:8])),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7RD9wKPCtXH"
      },
      "outputs": [],
      "source": [
        "cot_diverse_exemplars_sc = self_consistency.SelfConsistency(\n",
        "    sampler=cot_over_diverse_exemplars_sampler,\n",
        "    bucketizer=lambda x: x.answer,  # Use the final answer as the bucket.\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui0japsKCtXQ"
      },
      "outputs": [],
      "source": [
        "answer_distribution_cot_div_ex_sc, trace_cot_div_ex_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_diverse_exemplars_sc(question)),\n",
        "    enable_tracing=True)\n",
        "\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_cot_div_ex_sc):\n",
        "  cot_reply = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': round(probability, 2),\n",
        "      'Answer': cot_reply.answer,\n",
        "      'Representative Reasoning': cot_reply.reasoning,\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6IZP8q6CtXQ"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer().render(trace_cot_div_ex_sc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVyyAgrE5m8-"
      },
      "source": [
        "## SC over diverse strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6jQVGFHRHJF"
      },
      "source": [
        "The same principle can be applied even more radically by sampling round-robin over fundamentally different prompting strategies. E.g., in the example below, we generate some of the samples using chain-of-thought and others using ReAct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuc9o4OVDT7E"
      },
      "outputs": [],
      "source": [
        "cot_over_diverse_strategies_sampler = sampling.RoundRobin([\n",
        "    sampling.Repeated(chain_of_thought.QACoTPromptChat(\n",
        "        instruction=cot_qa_instruction)),\n",
        "    sampling.Repeated(functools.partial(react_agent, return_final_state=True)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0L3m2LODySk"
      },
      "outputs": [],
      "source": [
        "# Since the outputs of the different strategies are in slightly different\n",
        "# formats, we define a couple of helper functions here for extracting the\n",
        "# \"answer\" and \"reasoning\" in a way that would work with either format.\n",
        "\n",
        "def extract_answer_cot_or_react(\n",
        "    x: chain_of_thought.CoTReply | tuple[str, str]) -\u003e str:\n",
        "  if isinstance(x, chain_of_thought.CoTReply):\n",
        "    # Chain-of-Thought\n",
        "    return x.answer\n",
        "  elif isinstance(x, tuple):\n",
        "    # ReAct: tuple(answer, reasoning)\n",
        "    return x[0]\n",
        "  else:\n",
        "    raise ValueError(f'Unexpected type: {type(x)}')\n",
        "\n",
        "def extract_reasoning_cot_or_react(\n",
        "    x: chain_of_thought.CoTReply | tuple[str, str]) -\u003e str:\n",
        "  if isinstance(x, chain_of_thought.CoTReply):\n",
        "    # Chain-of-Thought\n",
        "    return x.reasoning\n",
        "  elif isinstance(x, tuple):\n",
        "    # ReAct: tuple(answer, reasoning)\n",
        "    return x[1]\n",
        "  else:\n",
        "    raise ValueError(f'Unexpected type: {type(x)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb8telOsDT7P"
      },
      "outputs": [],
      "source": [
        "cot_diverse_strategies_sc = self_consistency.SelfConsistency(\n",
        "    sampler=cot_over_diverse_strategies_sampler,\n",
        "    bucketizer=extract_answer_cot_or_react,\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA0Cw4HADT7P"
      },
      "outputs": [],
      "source": [
        "answer_distribution_div_strat_sc, trace_div_strat_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_diverse_strategies_sc(question)),\n",
        "    enable_tracing=True)\n",
        "\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_div_strat_sc):\n",
        "  reply = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': round(probability, 2),\n",
        "      'Answer': extract_answer_cot_or_react(reply),\n",
        "      'Representative Reasoning': extract_reasoning_cot_or_react(reply),\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b2ukx2UFksw"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer().render(trace_div_strat_sc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs9_E12dwvUo"
      },
      "source": [
        "## SC with normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pj15IkYR4IE"
      },
      "source": [
        "One of the things you might have noticed in the above answer distributions is that we sometimes have multiple different competing answers that actually mean the same thing, but are just formatted differently (e.g., \"490,000\" vs. \"490000\"). If we want to ensure that these are placed in the same bucket for voting purposes, we can do so by providing a custom `bucketizer` function that performs some appropriate normalization of the answer format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZeemtt1B5sl"
      },
      "outputs": [],
      "source": [
        "def get_numeric_answer(prediction: str) -\u003e str:\n",
        "  \"\"\"Returns a normalized number string, where possible, otherwise the original.\n",
        "\n",
        "  Args:\n",
        "    prediction: The LLM reply to normalize.\n",
        "  \"\"\"\n",
        "  # Remove any trailing periods.\n",
        "  normalized = prediction\n",
        "  while normalized and normalized.endswith('.'):\n",
        "    normalized = normalized[:-1]\n",
        "\n",
        "  # If the prediction is in the form of an equation (e.g., '2 + 3 = 5'), then\n",
        "  # we take just the answer ('5').\n",
        "  normalized = normalized.split('=')[-1]\n",
        "\n",
        "  # Ignore any thousands separators.\n",
        "  normalized = normalized.replace(',', '')\n",
        "\n",
        "  try:\n",
        "    _ = float(normalized)\n",
        "    return normalized\n",
        "  except ValueError:\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BZwVdtIwvU-"
      },
      "outputs": [],
      "source": [
        "get_numeric_answer('490,000.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp6slg8TElq3"
      },
      "outputs": [],
      "source": [
        "cot_sc_normalized = self_consistency.SelfConsistency(\n",
        "    sampler=sampling.Repeated(chain_of_thought.QACoTPromptChat(\n",
        "        instruction=cot_qa_instruction)),\n",
        "    bucketizer=lambda x: get_numeric_answer(x.answer),\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogquWZWyElq3"
      },
      "outputs": [],
      "source": [
        "answer_distribution_cot_norm_sc, trace_cot_norm_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_sc_normalized(question)),\n",
        "    enable_tracing=True)\n",
        "\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_cot_norm_sc):\n",
        "  cot_reply = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': round(probability, 2),\n",
        "      'Answer': cot_reply.answer,\n",
        "      'Representative Reasoning': cot_reply.reasoning,\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pnt5mnj5ttX"
      },
      "source": [
        "## SC with weighted votes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dXzAH55StGr"
      },
      "source": [
        "In cases where we have access to some additional signal for judging the plausibility of an answer, we can pass this to the `SelfConsistency` strategy in the form of a custom `scorer` function, so as to weight each vote by its score.\n",
        "\n",
        "This approach is described in [[Li, et al., 2023]](https://arxiv.org/pdf/2206.02336) as a \"voting verifier\".\n",
        "\n",
        "Here we show an example of this approach, where the \"verifier\" is implemented via prompting of the same LLM that was used to generate the response (while eliciting different behavior, and thus hopefully additional signal, by using a materially different prompt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-ksdpxOdZzf"
      },
      "outputs": [],
      "source": [
        "# Type representing a strategy's input.\n",
        "_I = TypeVar('_I')\n",
        "\n",
        "# Type representing a strategy's output.\n",
        "_O = TypeVar('_O')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLAOxpH8GVkP"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class VerifierReply:\n",
        "  \"\"\"Verifier reply.\"\"\"\n",
        "  rating: float | None = None\n",
        "  rating_string: str = ''\n",
        "  reasoning: str = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl5q2LkBy1xt"
      },
      "outputs": [],
      "source": [
        "VERIFIER_INSTRUCTION = \"\"\"\\\n",
        "Give a numerical rating for plausibility of the proposed answer, ranging from 0.0 (clearly wrong or not actually answering the question) to 1.0 (very plausible), along with the justification for the rating. When rating the plausibility of the answer, be humble (i.e., don't simply assume that you yourself know the correct answer), but consider factors such as the following:\n",
        "* whether the answer is of the correct type (e.g., the question asked for a place, but they answered with a time)\n",
        "* whether the answer cites its sources, includes the detailed reasoning that led up to the final answer\n",
        "* whether the reasoning is sound\n",
        "* whether there was any sign of system error (e.g., the answer was blank, or included an error message, or repeated the same sentence many times, or got cut off)\n",
        "* whether the answer is of a plausible order of magnitude (in case of a numeric answer)\n",
        "\n",
        "Rough guidelines:\n",
        "* If the answer does not actually answer the question (e.g., says 'Unable to answer', 'Insufficient context', etc., or indicates a system error), then the rating should be 0.0.\n",
        "* If the answer is clearly of the wrong type, then the rating should be 0.0.\n",
        "* If the answer is of the correct type, but based on your own background knowledge or common sense you highly doubt it to be correct, then the rating should be around 0.5.\n",
        "* If the answer is plausible but non-obvious, and if no sources are cited and no reasoning is provided, then the rating should be around 0.9.\n",
        "* If the answer is plausible and sources are cited and/or plausible reasoning is provided, then the rating should be 1.0.\n",
        "\n",
        "First output your justification, then on a separate line, after 'Rating (0.0 to 1.0): ', output your numerical rating (number only).\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmXmJH-pDzYs"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class AnswerPlausibilityVerifier(Generic[_I, _O]):\n",
        "  \"\"\"Basic zero-shot verifier of answer plausibility.\n",
        "\n",
        "  Attributes:\n",
        "    instruction: The instruction to use for the prompt. Does not need to begin\n",
        "      or end with a newline, as these will be handled automatically.\n",
        "  \"\"\"\n",
        "  instruction: str = VERIFIER_INSTRUCTION\n",
        "\n",
        "  @ot.make_executable(copy_self=False)\n",
        "  @tracing.trace(name=utils.FROM_INSTANCE_CLASS_NAME)\n",
        "  async def __call__(\n",
        "      self,\n",
        "      inputs: _I,\n",
        "      prediction: _O,\n",
        "  ) -\u003e VerifierReply:\n",
        "    \"\"\"Returns a rating of the plausibility of the prediction, with reasoning.\n",
        "\n",
        "    Args:\n",
        "      inputs: The question.\n",
        "      prediction: The predicted answer.\n",
        "    \"\"\"\n",
        "    # Prompt the LLM for the justification.\n",
        "    messages = [\n",
        "        content_lib.Message(\n",
        "            content_lib.PredefinedRole.USER,\n",
        "            self.instruction + '\\n'),\n",
        "        content_lib.Message(content_lib.PredefinedRole.USER, '\\nQuestion: '),\n",
        "        content_lib.Message(content_lib.PredefinedRole.USER, inputs),\n",
        "        content_lib.Message(\n",
        "            content_lib.PredefinedRole.USER,\n",
        "            f'\\nProposed answer: {prediction}\\nJustification for rating: '),\n",
        "    ]\n",
        "    reasoning = await llm.chat(messages=messages, stop=['\\nRating'])\n",
        "    reasoning = reasoning.strip()\n",
        "\n",
        "    # Prompt the LLM for the numerical rating.\n",
        "    messages.extend([\n",
        "        content_lib.Message(content_lib.PredefinedRole.MODEL, reasoning),\n",
        "        content_lib.Message(\n",
        "            content_lib.PredefinedRole.USER,\n",
        "            'Rating (0.0 to 1.0): '),\n",
        "    ])\n",
        "    rating_string = await llm.chat(messages=messages, stop=['\\nQuestion:'])\n",
        "\n",
        "    # Parse the rating string.\n",
        "    # Although we ask the LLM to output just a single number, we make a best\n",
        "    # effort to accept variations in the output format, such as when the LLM\n",
        "    # unnecessarily repeats a portion of the prompt, or appends extra lines.\n",
        "    rating_string = rating_string.split('Rating (0.0 to 1.0):')[-1].strip()\n",
        "    rating_string = rating_string.split('Rating:')[-1].strip()\n",
        "    rating_string = rating_string.split('\\n')[0].strip()\n",
        "    try:\n",
        "      rating = float(rating_string)\n",
        "    except (TypeError, ValueError, AttributeError):\n",
        "      rating = None\n",
        "\n",
        "    return VerifierReply(\n",
        "        rating=rating,\n",
        "        rating_string=rating_string,\n",
        "        reasoning=reasoning,\n",
        "    )\n",
        "\n",
        "@tracing.trace\n",
        "async def cot_verifier(\n",
        "    args: Sequence[Any],\n",
        "    kwargs: Mapping[str, Any],\n",
        "    output: chain_of_thought.CoTReply) -\u003e float:\n",
        "  \"\"\"Returns a rating of the plausibility of the CoT reply for the question.\"\"\"\n",
        "  if len(args) == 1 and not kwargs:\n",
        "    # Question provided as positional arg.\n",
        "    question = args[0]\n",
        "  elif not args and len(kwargs) == 1:\n",
        "    # Question provided as keyword arg.\n",
        "    question = list(kwargs.values())[0]\n",
        "  else:\n",
        "    # Multiple args/kwargs provided -- squeeze them into a string to use as the\n",
        "    # \"question\".\n",
        "    question = repr({'args': args, 'kwargs': kwargs})\n",
        "  critic = AnswerPlausibilityVerifier()\n",
        "  prediction = (\n",
        "      f'{output.reasoning.strip()} The answer is {output.answer}.')\n",
        "  verifier_reply = await critic(inputs=question, prediction=prediction)\n",
        "  return verifier_reply.rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na4jixuE7Wws"
      },
      "outputs": [],
      "source": [
        "# Question provided as positional arg.\n",
        "verifier_result, verifier_trace = ot.run(cot_verifier(\n",
        "    args=('What is the total population of Tuebingen and Zuerich?',),\n",
        "    kwargs={},\n",
        "    output=chain_of_thought.CoTReply(\n",
        "        answer='490,000',\n",
        "        reasoning= '\\nPopulation of Tuebingen = 90,000\\nPopulation of Zuerich = 400,000\\nTotal population = 90,000 + 400,000 = 490,000')\n",
        "), enable_tracing=True)\n",
        "verifier_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePZSVdoUX0N7"
      },
      "outputs": [],
      "source": [
        "# Question provided as keyword arg.\n",
        "verifier_result, verifier_trace = ot.run(cot_verifier(\n",
        "    args=tuple(),\n",
        "    kwargs={'question': 'What is the total population of Tuebingen and Zuerich?'},\n",
        "    output=chain_of_thought.CoTReply(\n",
        "        answer='490,000',\n",
        "        reasoning= '\\nPopulation of Tuebingen = 90,000\\nPopulation of Zuerich = 400,000\\nTotal population = 90,000 + 400,000 = 490,000')\n",
        "), enable_tracing=True)\n",
        "verifier_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7ebXDvb79q9"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(verifier_trace))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5TtWkyhGcTJ"
      },
      "outputs": [],
      "source": [
        "cot_sc_score_weighted = self_consistency.SelfConsistency(\n",
        "    sampler=cot_over_diverse_exemplars_sampler,\n",
        "    bucketizer=lambda x: get_numeric_answer(x.answer),\n",
        "    scorer=cot_verifier,\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUk9dy2e5ttY"
      },
      "outputs": [],
      "source": [
        "answer_distribution_cot_score_sc, trace_cot_score_sc = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_sc_score_weighted(question)),\n",
        "    enable_tracing=True)\n",
        "\n",
        "answer_distribution_summary_data = []\n",
        "for i, (output, probability) in enumerate(answer_distribution_cot_score_sc):\n",
        "  cot_reply = output\n",
        "  answer_distribution_summary_data.append({\n",
        "      'Probability': round(probability, 2),\n",
        "      'Answer': cot_reply.answer,\n",
        "      'Representative Reasoning': cot_reply.reasoning,\n",
        "  })\n",
        "answer_distribution_summary_data_df = pd.DataFrame(answer_distribution_summary_data)\n",
        "answer_distribution_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5va7d6vQIJQw"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer().render(trace_cot_score_sc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT13bumQXDav"
      },
      "source": [
        "## SC with consensus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_aJAXzxUq5q"
      },
      "source": [
        "As described in the overview section, if we simply we want to use a self-consistency-enhanced strategy as a drop-in replacement for its underlying strategy, we can do so by wrapping it in `ExtractConsensus`, so as to return just the single consensus answer (i.e., highest probability answer), rather than the full answer distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWNQCOdjVpeM"
      },
      "outputs": [],
      "source": [
        "cot_sc_concensus = self_consistency.ExtractConsensus(cot_sc)\n",
        "cot_sc_consensus_reply, trace_cot_sc_consensus = ot.run(\n",
        "    on_backend_temp_0_7_with_search(cot_sc_concensus(question)),\n",
        "    enable_tracing=True)\n",
        "pprint.pprint(cot_sc_consensus_reply, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKVR0glaWm2K"
      },
      "outputs": [],
      "source": [
        "# IPython.display.HTML(ot.HTMLRenderer().render(trace_cot_sc_consensus))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj0l4UOf8sGb"
      },
      "source": [
        "## Eval with distribution metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjdGv_5UVQez"
      },
      "source": [
        "Another approach to evaluation, however, is to use the `SelfConsistency` strategy as-is, while providing an evaluation metric that takes an answer distribution as its \"prediction\".\n",
        "\n",
        "An advantage of this approach is that it allows outputting of metrics like \"accuracy@k\" (i.e., the fraction of time that the correct answer is among the top K candidates), which would not be possible to calculate based on the consensus answer alone.\n",
        "\n",
        "Note that \"accuracy@k\" fully subsumes its underlying ordinary accuracy metric, which simply corresponds to the case of `k=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VONGeBBS8YgS"
      },
      "outputs": [],
      "source": [
        "cot_sc = self_consistency.SelfConsistency(\n",
        "    sampler=sampling.Repeated(chain_of_thought.QACoTPromptChat(\n",
        "        instruction=cot_qa_instruction)),\n",
        "    bucketizer=lambda x: x.answer,  # Use the final answer as the bucket.\n",
        "    num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UBp7IMX8Rv-"
      },
      "outputs": [],
      "source": [
        "# We define a custom accuracy metric, since CoT predictions are represented as\n",
        "# ChainOfThoughtReply objects (containing 'answer' and 'reasoning'), whereas the\n",
        "# target is simply the final answer.\n",
        "cot_exact_match = lambda t, p: 1.0 * (str(t) == p.answer)\n",
        "cot_answer_included = lambda t, p: 1.0 * (str(t) in p.answer)\n",
        "\n",
        "AccuracyAtK = distribution_metrics.AccuracyAtK\n",
        "\n",
        "with ot.RegistryContext():\n",
        "  llm.generate_text.update(temperature=0.7)\n",
        "  cot_sc_summary = agent_evaluation.evaluate(\n",
        "      strategy=cot_sc,\n",
        "      examples=dataset,\n",
        "      metric_functions={\n",
        "          'exact_match': AccuracyAtK(k=1, base_metric=cot_exact_match),\n",
        "          'exact_match@3': AccuracyAtK(k=3, base_metric=cot_exact_match),\n",
        "          'answer_included': AccuracyAtK(k=1, base_metric=cot_answer_included),\n",
        "          'answer_included@3': AccuracyAtK(k=3, base_metric=cot_answer_included),\n",
        "      },\n",
        "      output_results=True,\n",
        "      output_results_debug=True,\n",
        "      output_final_states=True,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPa4WphCANCS"
      },
      "outputs": [],
      "source": [
        "# Here are the metric values aggregated over the whole dataset.\n",
        "for metric, value in cot_sc_summary.metrics.items():\n",
        "  print(f'{metric}: {value:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyWagmbNBu08"
      },
      "outputs": [],
      "source": [
        "# Here is a table showing the results broken down by question.\n",
        "eval_summary_data = []\n",
        "for i, result in cot_sc_summary.results.items():\n",
        "  predicted_distribution = result.outputs['output']\n",
        "  eval_summary_data.append({\n",
        "      'Question': result.inputs['args'][0],\n",
        "      'Target': result.targets['target'],\n",
        "      'EM@1': result.metrics['exact_match'],\n",
        "      'EM@3': result.metrics['exact_match@3'],\n",
        "      'Answer Included@1': result.metrics['answer_included'],\n",
        "      'Answer Included@3': result.metrics['answer_included@3'],\n",
        "      'Answer 1': predicted_distribution[0][0].answer if len(predicted_distribution) \u003e= 1 else '--',\n",
        "      'Answer 2': predicted_distribution[1][0].answer if len(predicted_distribution) \u003e= 2 else '--',\n",
        "      'Answer 3': predicted_distribution[2][0].answer if len(predicted_distribution) \u003e= 3 else '--',\n",
        "  })\n",
        "eval_summary_data_df = pd.DataFrame(eval_summary_data)\n",
        "eval_summary_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2Yd6fc3w1yE"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(cot_sc_summary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HWP6DhTw7Rc"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT052IG0ckbI"
      },
      "source": [
        "## Appendix A: Other Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFbbBl9Pcs3k"
      },
      "source": [
        "As of the initial release, we support connections to the following models (more will be added soon):\n",
        "- Gemini API\n",
        "- OpenAI API\n",
        "- Gemma running locally\n",
        "- Gemma running on a OneTwo model server\n",
        "\n",
        "In the Overview section, we illustrated how to connect to the Gemini and OpenAI APIs. Below, we illustrate how to setup and connect to Gemma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sesaZZtsglUb"
      },
      "source": [
        "### Gemma (running locally)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOxjsEVxx8FR"
      },
      "source": [
        "The [Gemma](https://ai.google.dev/gemma) family of open weights models ([GitHub](https://github.com/google-deepmind/gemma)) can be obtained from the following repositories:\n",
        "- Vertex Model Garden: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335\n",
        "- Kaggle: https://www.kaggle.com/models/google/gemma/\n",
        "- HuggingFace: https://huggingface.co/docs/transformers/en/model_doc/gemma\n",
        "\n",
        "As an example, below are the instructions for downloading a Gemma model from Kaggle. If you have not used Kaggle before, you will need to first create a Kaggle account and API key (a.k.a. API token) following the instructions on https://www.kaggle.com/docs/api. Then copy-paste your username and API key into the placeholders below.\n",
        "\n",
        "If you have not used Gemma on Kaggle before, you will also need to go to https://www.kaggle.com/models/google/gemma/ and click \"Request access\" to complete the consent form before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptIrpyV3_t7R"
      },
      "outputs": [],
      "source": [
        "# If you want to run this section, just change the below to `True`.\n",
        "# (We keep it disabled by default due to the kaggle dependency.)\n",
        "enable_gemma_local = False  # @param [\"True\", \"False\"] {type:\"raw\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLfR3pFzhDMN"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "# Install kaggle\n",
        "! pip install kaggle\n",
        "! pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWepgri9gptd"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "import kagglehub\n",
        "from kagglehub import auth\n",
        "\n",
        "auth.set_kaggle_credentials(\n",
        "    username='YOUR_KAGGLE_USERNAME', api_key='YOUR_KAGGLE_API_KEY'\n",
        ")\n",
        "kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHylR7QshOHB"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "VARIANT = '2b-it'  # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\n",
        "weights_dir = kagglehub.model_download(f'google/gemma/Flax/{VARIANT}')\n",
        "\n",
        "checkpoint_path = os.path.join(weights_dir, VARIANT)\n",
        "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD7WNFp_yzFb"
      },
      "source": [
        "Once you have downloaded a copy of the model, you can create a OneTwo backend using this model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vwlxrmDfNc8"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "from onetwo.backends import gemma_local\n",
        "\n",
        "backend = gemma_local.Gemma(\n",
        "    checkpoint_path=checkpoint_path, vocab_path=vocab_path\n",
        ")\n",
        "backend.register()\n",
        "print('Gemma local backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv8giCScg8k5"
      },
      "source": [
        "### Gemma (running on a separate server)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QvJLDTIzAcJ"
      },
      "source": [
        "While experimenting, you may not want to have to reload the model every time you change something in your code, so it can be convenient to set up the Gemma model to run in a separate process or even on a separate machine.\n",
        "\n",
        "We provide a model server script for that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzw2TkcIhja9"
      },
      "source": [
        "#### Server setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLYm4DaItG1J"
      },
      "source": [
        "In order to set up a server, you can use the `run_model_server.py` script. For example you can create a server that serves a Gemma model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3w18fFWhxSy"
      },
      "source": [
        "```shell\n",
        "CHECKPOINT_PATH=\"PATH_TO_THE_CHECKPOINT_DIR\"\n",
        "VOCAB_PATH=\"PATH_TO_THE_VOCAB_FILE\"\n",
        "\n",
        "python run_model_server.py \\\n",
        "  --backend_module=\"onetwo.backends.gemma_local\" \\\n",
        "  --backend_class=\"Gemma\" \\\n",
        "  --backend_params=\"{\\\"checkpoint_path\\\": \\\"$CHECKPOINT_PATH\\\", \\\"vocab_path\\\": \\\"$VOCAB_PATH\\\"}\" \\\n",
        "  --port=8888\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12trBlN7tUIb"
      },
      "source": [
        "You can also simply create your own python code to load a model locally or connect to a remote model and run as a simple web server using code that looks like:\n",
        "\n",
        "```python\n",
        "import uvicorn\n",
        "\n",
        "def main(args):\n",
        "  # Code to create the backend connection (local or remote).\n",
        "  backend = ...\n",
        "  backend.register()\n",
        "\n",
        "  # Starting a simple server piping requests to the registered backend.\n",
        "  uvicorn.run(\n",
        "      'onetwo.backends.model_server:ModelServer',\n",
        "      host='0.0.0.0',\n",
        "      port=8888,\n",
        "      factory=True,\n",
        "  )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m26cftOxhlYs"
      },
      "source": [
        "#### Client connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9EOqVN6uGiC"
      },
      "source": [
        "Once your server is set up, on the client side, you can connect to it using the onetwo_api module.\n",
        "\n",
        "If you have set up a server as described above, you can uncomment the code below and copy-paste your server hostname into the placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tedsV2vmhnzP"
      },
      "outputs": [],
      "source": [
        "# from onetwo.backends import onetwo_api\n",
        "\n",
        "# backend = onetwo_api.OneTwoAPI(endpoint='http://SERVER_HOST_NAME:8888')\n",
        "# backend.register()\n",
        "# print('Gemma server backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-a8NwgirOKy"
      },
      "source": [
        "## Appendix B: Diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We8MLiOQsXfa"
      },
      "outputs": [],
      "source": [
        "cached_backends.print_cache_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OATefA9QqbNQ"
      },
      "source": [
        "**Short explanation:**\n",
        "If the above diagnostics show a non-zero `calls_in_progress` for any of the backends, you can clear them by temporarily uncommenting and executing the line below.\n",
        "\n",
        "**Longer explanation:**\n",
        "Sometimes if you cancel a call to `ot.run` or related function in the middle of its execution via repeatedly clicking \"Interrupt Execution\" or through repeated keyboard interrupts, you can end up with orphaned pending entries left over in the cache. These pending entries are stored internally in a `calls_in_progress` field of the cache handler, which serves an important role during execution, but can lead to problems if orphaned entries are carried over from one run to another. Normally you should not need to worry about this, as the `calls_in_progress` field is normally cleared automatically at the end of each run, both in the case where the run succeeded and in the case where it was interrupted due to an exception or keyboard interrupt. If you press interrupt repeatedly, however, it may end up interrupting the logic that cleans up things like `calls_in_progress`, in which case you may see a non-zero value for `calls_in_progress` in the diagnostics above. In this case, you can forcibly clear the `calls_in_progress` by temporarily uncommenting and executing the line below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXWBv6mWIyDk"
      },
      "outputs": [],
      "source": [
        "cached_backends.clear_all_calls_in_progress()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q--1ZcDKEALU"
      },
      "source": [
        "If you would like to automatically save the cache at the end of the colab run, you can uncomment the following line. (You should avoid doing this, though, if you are running multiple instances of the same colab, or of other colabs that write to the same cache.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vICdxYEiELPy"
      },
      "outputs": [],
      "source": [
        "# cached_backends.save_caches()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/brain/research/system12/onetwo/colabs:onetwo_colab",
        "kind": "private"
      },
      "name": "OneTwo - Tutorial",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
